# æ¨èç®—æ³•æ ¸å¿ƒå®ç°

## ğŸ§® ç®—æ³•ä½“ç³»æ¦‚è§ˆ

æœ¬æ¨èç³»ç»Ÿå®ç°äº†å››å¤§æ ¸å¿ƒç®—æ³•å¼•æ“ï¼Œé‡‡ç”¨TDDæ–¹æ³•è®ºç¡®ä¿ç®—æ³•çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼š

```
æ¨èç®—æ³•ä½“ç³»æ¶æ„ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                ğŸ¯ æ··åˆæ¨èå¼•æ“                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ååŒè¿‡æ»¤å¼•æ“  â”‚å†…å®¹æ¨èå¼•æ“  â”‚ç¤¾äº¤æ¨èå¼•æ“  â”‚           â”‚
â”‚  â”‚  â€¢ ç”¨æˆ·ç›¸ä¼¼  â”‚  â€¢ TF-IDF   â”‚  â€¢ å½±å“åŠ›   â”‚           â”‚
â”‚  â”‚  â€¢ ç‰©å“ç›¸ä¼¼  â”‚  â€¢ ç‰¹å¾æå–  â”‚  â€¢ ä¿¡ä»»ä¼ æ’­  â”‚           â”‚
â”‚  â”‚  â€¢ çŸ©é˜µåˆ†è§£  â”‚  â€¢ ç›¸ä¼¼åº¦è®¡ç®—â”‚  â€¢ ç¤¾äº¤è·¯å¾„  â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¤ ååŒè¿‡æ»¤ç®—æ³• (Collaborative Filtering)

### ç®—æ³•åŸç†

ååŒè¿‡æ»¤åŸºäº"ç‰©ä»¥ç±»èšï¼Œäººä»¥ç¾¤åˆ†"çš„æ€æƒ³ï¼Œé€šè¿‡åˆ†æç”¨æˆ·çš„å†å²è¡Œä¸ºå’Œåå¥½ï¼Œæ‰¾åˆ°ç›¸ä¼¼çš„ç”¨æˆ·æˆ–ç‰©å“ï¼Œç„¶ååŸºäºç›¸ä¼¼æ€§è¿›è¡Œæ¨èã€‚

### æ ¸å¿ƒå®ç°

#### 1. ç”¨æˆ·ç›¸ä¼¼åº¦è®¡ç®—

```python
def calculate_user_similarity(self, user_a: str, user_b: str,
                             method: str = "cosine") -> float:
    """
    è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦

    Args:
        user_a, user_b: ç”¨æˆ·ID
        method: ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³• (cosine, pearson, jaccard)

    Returns:
        float: ç›¸ä¼¼åº¦åˆ†æ•° [0, 1]
    """
    # è·å–ä¸¤ä¸ªç”¨æˆ·çš„å…±åŒç‰©å“
    common_items = self.get_common_items(user_a, user_b)
    if not common_items:
        return 0.0

    if method == "cosine":
        return self._cosine_similarity(user_a, user_b, common_items)
    elif method == "pearson":
        return self._pearson_correlation(user_a, user_b, common_items)
    elif method == "jaccard":
        return self._jaccard_similarity(user_a, user_b)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•: {method}")
```

#### 2. ä½™å¼¦ç›¸ä¼¼åº¦å®ç°

```python
def _cosine_similarity(self, user_a: str, user_b: str,
                      common_items: Set[str]) -> float:
    """ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—"""

    # æ„å»ºå‘é‡
    vector_a = np.array([self.matrix[user_a][item] for item in common_items])
    vector_b = np.array([self.matrix[user_b][item] for item in common_items])

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    dot_product = np.dot(vector_a, vector_b)
    norm_a = np.linalg.norm(vector_a)
    norm_b = np.linalg.norm(vector_b)

    if norm_a == 0 or norm_b == 0:
        return 0.0

    return dot_product / (norm_a * norm_b)
```

#### 3. çš®å°”é€Šç›¸å…³ç³»æ•°

```python
def _pearson_correlation(self, user_a: str, user_b: str,
                        common_items: Set[str]) -> float:
    """çš®å°”é€Šç›¸å…³ç³»æ•°è®¡ç®—"""

    ratings_a = [self.matrix[user_a][item] for item in common_items]
    ratings_b = [self.matrix[user_b][item] for item in common_items]

    mean_a = np.mean(ratings_a)
    mean_b = np.mean(ratings_b)

    numerator = sum((ra - mean_a) * (rb - mean_b)
                   for ra, rb in zip(ratings_a, ratings_b))

    sum_sq_a = sum((ra - mean_a) ** 2 for ra in ratings_a)
    sum_sq_b = sum((rb - mean_b) ** 2 for rb in ratings_b)

    denominator = np.sqrt(sum_sq_a * sum_sq_b)

    if denominator == 0:
        return 0.0

    return numerator / denominator
```

#### 4. åŸºäºç”¨æˆ·çš„æ¨èç”Ÿæˆ

```python
def user_based_recommend(self, user_id: str, k: int = 10) -> List[RecommendationItem]:
    """
    åŸºäºç”¨æˆ·çš„ååŒè¿‡æ»¤æ¨è

    ç®—æ³•æ­¥éª¤ï¼š
    1. æ‰¾åˆ°ä¸ç›®æ ‡ç”¨æˆ·æœ€ç›¸ä¼¼çš„Nä¸ªç”¨æˆ·
    2. è·å–è¿™äº›ç›¸ä¼¼ç”¨æˆ·è¯„åˆ†é«˜ä½†ç›®æ ‡ç”¨æˆ·æœªè¯„åˆ†çš„ç‰©å“
    3. æ ¹æ®ç›¸ä¼¼åº¦å’Œè¯„åˆ†è®¡ç®—æ¨èåˆ†æ•°
    """

    # 1. è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦
    similarities = []
    for other_user in self.matrix.users:
        if other_user != user_id:
            sim = self.calculate_user_similarity(user_id, other_user)
            if sim > 0:
                similarities.append((other_user, sim))

    # 2. æ’åºå¹¶é€‰æ‹©TopNç›¸ä¼¼ç”¨æˆ·
    similarities.sort(key=lambda x: x[1], reverse=True)
    top_similar_users = similarities[:self.n_similar_users]

    # 3. ç”Ÿæˆæ¨è
    recommendations = {}
    user_items = set(self.matrix[user_id].keys())

    for similar_user, similarity in top_similar_users:
        for item, rating in self.matrix[similar_user].items():
            if item not in user_items and rating > 0:
                if item not in recommendations:
                    recommendations[item] = 0
                recommendations[item] += similarity * rating

    # 4. å½’ä¸€åŒ–å¹¶æ’åº
    normalized_recommendations = []
    for item, score in recommendations.items():
        normalized_score = min(score / len(top_similar_users), 1.0)
        normalized_recommendations.append(
            RecommendationItem(item, normalized_score)
        )

    normalized_recommendations.sort(key=lambda x: x.score, reverse=True)
    return normalized_recommendations[:k]
```

### ç®—æ³•å¤æ‚åº¦åˆ†æ

| æ“ä½œ | æ—¶é—´å¤æ‚åº¦ | ç©ºé—´å¤æ‚åº¦ | ä¼˜åŒ–ç­–ç•¥ |
|------|------------|------------|----------|
| ç”¨æˆ·ç›¸ä¼¼åº¦è®¡ç®— | O(mÂ²n) | O(nÂ²) | ç¨€ç–çŸ©é˜µã€è¿‘ä¼¼è®¡ç®— |
| æ¨èç”Ÿæˆ | O(mn) | O(k) | é¢„è®¡ç®—ã€ç¼“å­˜ |
| ç›¸ä¼¼ç”¨æˆ·æŸ¥æ‰¾ | O(m log m) | O(m) | KDæ ‘ã€LSH |

### ç®—æ³•ä¼˜åŒ–

#### ç¨€ç–çŸ©é˜µä¼˜åŒ–
```python
class SparseUserItemMatrix:
    """ç¨€ç–ç”¨æˆ·-ç‰©å“çŸ©é˜µ"""

    def __init__(self):
        self.user_to_items = defaultdict(dict)  # ç”¨æˆ·åˆ°ç‰©å“çš„æ˜ å°„
        self.item_to_users = defaultdict(dict)  # ç‰©å“åˆ°ç”¨æˆ·çš„æ˜ å°„
        self.user_averages = {}                 # ç”¨æˆ·å¹³å‡è¯„åˆ†

    def get_user_vector(self, user_id: str) -> dict:
        """è·å–ç”¨æˆ·è¯„åˆ†å‘é‡ï¼ˆç¨€ç–è¡¨ç¤ºï¼‰"""
        return self.user_to_items.get(user_id, {})

    def get_common_items(self, user_a: str, user_b: str) -> Set[str]:
        """è·å–ä¸¤ä¸ªç”¨æˆ·çš„å…±åŒç‰©å“"""
        items_a = set(self.user_to_items[user_a].keys())
        items_b = set(self.user_to_items[user_b].keys())
        return items_a & items_b
```

#### è¿‘ä¼¼æœ€è¿‘é‚»ä¼˜åŒ–
```python
class ApproximateNearestNeighbors:
    """è¿‘ä¼¼æœ€è¿‘é‚»ç®—æ³•"""

    def __init__(self, n_trees: int = 10):
        self.n_trees = n_trees
        self.trees = []

    def build_index(self, users: List[str], embeddings: np.ndarray):
        """æ„å»ºéšæœºæ£®æ—ç´¢å¼•"""
        for _ in range(self.n_trees):
            tree = self._build_random_tree(users, embeddings)
            self.trees.append(tree)

    def find_similar_users(self, query_user: str, k: int = 10) -> List[Tuple[str, float]]:
        """æŸ¥æ‰¾ç›¸ä¼¼ç”¨æˆ·"""
        candidates = set()
        for tree in self.trees:
            candidates.update(tree.search(query_user, k * 2))

        # ç²¾ç¡®è®¡ç®—å€™é€‰ç”¨æˆ·çš„ç›¸ä¼¼åº¦
        similarities = []
        for candidate in candidates:
            sim = self._exact_similarity(query_user, candidate)
            similarities.append((candidate, sim))

        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:k]
```

## ğŸ“š å†…å®¹æ¨èç®—æ³• (Content-Based Recommendation)

### ç®—æ³•åŸç†

å†…å®¹æ¨èåŸºäºç‰©å“çš„å†…å®¹ç‰¹å¾å’Œç”¨æˆ·çš„å†å²åå¥½ï¼Œé€šè¿‡ç‰¹å¾åŒ¹é…è¿›è¡Œæ¨èã€‚æ ¸å¿ƒæ€æƒ³æ˜¯æ¨èä¸ç”¨æˆ·å†å²å–œå¥½çš„ç‰©å“ç›¸ä¼¼çš„å†…å®¹ã€‚

### æ ¸å¿ƒå®ç°

#### 1. ç‰¹å¾æå–å™¨

```python
class FeatureExtractor:
    """ç‰¹å¾æå–å™¨"""

    def __init__(self):
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=5000,
            stop_words='english',
            ngram_range=(1, 2)
        )
        self.feature_cache = {}

    def extract_text_features(self, text: str) -> np.ndarray:
        """æå–æ–‡æœ¬ç‰¹å¾"""
        # æ¸…ç†æ–‡æœ¬
        cleaned_text = self._clean_text(text)

        # TF-IDFç‰¹å¾
        tfidf_features = self.tfidf_vectorizer.fit_transform([cleaned_text])

        return tfidf_features.toarray()[0]

    def extract_structured_features(self, item_metadata: dict) -> np.ndarray:
        """æå–ç»“æ„åŒ–ç‰¹å¾"""
        features = []

        # ç±»åˆ«ç‰¹å¾ï¼ˆOne-Hotç¼–ç ï¼‰
        categories = item_metadata.get('categories', [])
        category_features = self._encode_categories(categories)
        features.extend(category_features)

        # æ•°å€¼ç‰¹å¾
        numeric_features = [
            item_metadata.get('price', 0),
            item_metadata.get('popularity', 0),
            item_metadata.get('rating', 0)
        ]
        features.extend(numeric_features)

        # æ—¶é—´ç‰¹å¾
        timestamp = item_metadata.get('timestamp')
        if timestamp:
            time_features = self._extract_time_features(timestamp)
            features.extend(time_features)

        return np.array(features)
```

#### 2. ç”¨æˆ·ç”»åƒæ„å»º

```python
class UserProfileBuilder:
    """ç”¨æˆ·ç”»åƒæ„å»ºå™¨"""

    def __init__(self, feature_extractor: FeatureExtractor):
        self.feature_extractor = feature_extractor
        self.user_profiles = {}

    def build_profile(self, user_id: str,
                     interaction_history: List[Interaction]) -> UserProfile:
        """æ„å»ºç”¨æˆ·ç”»åƒ"""

        # æ”¶é›†ç”¨æˆ·äº¤äº’è¿‡çš„ç‰©å“ç‰¹å¾
        positive_features = []
        negative_features = []

        for interaction in interaction_history:
            item_features = self.feature_extractor.extract_features(
                interaction.item_id
            )

            weight = self._calculate_interaction_weight(interaction)
            weighted_features = item_features * weight

            if interaction.rating >= 4.0:  # æ­£é¢è¯„ä»·
                positive_features.append(weighted_features)
            else:  # è´Ÿé¢è¯„ä»·
                negative_features.append(weighted_features)

        # è®¡ç®—åå¥½å‘é‡
        preference_vector = self._calculate_preference_vector(
            positive_features, negative_features
        )

        # æ„å»ºç”¨æˆ·ç”»åƒ
        profile = UserProfile(
            user_id=user_id,
            preference_vector=preference_vector,
            favorite_categories=self._extract_favorite_categories(interaction_history),
            interaction_patterns=self._analyze_interaction_patterns(interaction_history)
        )

        self.user_profiles[user_id] = profile
        return profile
```

#### 3. ç›¸ä¼¼åº¦è®¡ç®—

```python
def calculate_content_similarity(self, user_profile: UserProfile,
                               item_features: np.ndarray) -> float:
    """
    è®¡ç®—ç”¨æˆ·ç”»åƒä¸ç‰©å“çš„å†…å®¹ç›¸ä¼¼åº¦

    æ”¯æŒå¤šç§ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ï¼š
    - ä½™å¼¦ç›¸ä¼¼åº¦
    - æ¬§æ°è·ç¦»
    - æ›¼å“ˆé¡¿è·ç¦»
    """

    user_vector = user_profile.preference_vector

    # ä½™å¼¦ç›¸ä¼¼åº¦
    cosine_sim = np.dot(user_vector, item_features) / (
        np.linalg.norm(user_vector) * np.linalg.norm(item_features)
    )

    # åŠ æƒç›¸ä¼¼åº¦ï¼ˆè€ƒè™‘ç±»åˆ«åå¥½ï¼‰
    category_weight = self._calculate_category_weight(
        user_profile.favorite_categories, item_features
    )

    # ç»¼åˆç›¸ä¼¼åº¦
    final_similarity = 0.7 * cosine_sim + 0.3 * category_weight

    return max(0, min(1, final_similarity))
```

#### 4. æ¨èç”Ÿæˆ

```python
def generate_content_recommendations(self, user_id: str,
                                   k: int = 10) -> RecommendationResult:
    """åŸºäºå†…å®¹çš„æ¨èç”Ÿæˆ"""

    # 1. è·å–ç”¨æˆ·ç”»åƒ
    user_profile = self.get_user_profile(user_id)
    if not user_profile:
        return RecommendationResult(user_id, "content_based", [])

    # 2. è·å–å€™é€‰ç‰©å“
    candidate_items = self.get_candidate_items(user_id)

    # 3. è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
    recommendations = []
    user_seen_items = set(self.get_user_seen_items(user_id))

    for item_id in candidate_items:
        if item_id not in user_seen_items:
            # æå–ç‰©å“ç‰¹å¾
            item_features = self.feature_extractor.extract_features(item_id)

            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = self.calculate_content_similarity(
                user_profile, item_features
            )

            if similarity > 0.1:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                recommendations.append(
                    RecommendationItem(item_id, similarity)
                )

    # 4. å¤šæ ·æ€§ä¼˜åŒ–
    diversified_recommendations = self._diversify_recommendations(
        recommendations, k
    )

    # 5. æ’åºå¹¶è¿”å›
    diversified_recommendations.sort(key=lambda x: x.score, reverse=True)

    return RecommendationResult(
        user_id,
        "content_based",
        diversified_recommendations[:k]
    )
```

### ç‰¹å¾å·¥ç¨‹æŠ€å·§

#### 1. TF-IDFä¼˜åŒ–
```python
class OptimizedTFIDF:
    """ä¼˜åŒ–çš„TF-IDFå®ç°"""

    def __init__(self, max_features: int = 10000):
        self.max_features = max_features
        self.word_to_idx = {}
        self.idx_to_word = {}
        self.document_frequency = defaultdict(int)
        self.total_documents = 0

    def fit_transform(self, documents: List[str]) -> np.ndarray:
        """è®­ç»ƒå¹¶è½¬æ¢æ–‡æ¡£"""
        self._build_vocabulary(documents)
        return self._transform(documents)

    def _build_vocabulary(self, documents: List[str]):
        """æ„å»ºè¯æ±‡è¡¨"""
        word_freq = defaultdict(int)

        for doc in documents:
            words = set(self._tokenize(doc))
            for word in words:
                word_freq[word] += 1
                self.document_frequency[word] += 1

        # é€‰æ‹©é«˜é¢‘è¯
        sorted_words = sorted(word_freq.items(),
                            key=lambda x: x[1], reverse=True)
        self.word_to_idx = {word: idx for idx, (word, _)
                           in enumerate(sorted_words[:self.max_features])}
        self.idx_to_word = {idx: word for word, idx
                           in self.word_to_idx.items()}

        self.total_documents = len(documents)
```

#### 2. ç‰¹å¾é™ç»´
```python
class FeatureDimensionalityReduction:
    """ç‰¹å¾é™ç»´å¤„ç†"""

    def __init__(self, method: str = "pca", n_components: int = 100):
        self.method = method
        self.n_components = n_components
        self.reducer = None

    def fit_transform(self, features: np.ndarray) -> np.ndarray:
        """è®­ç»ƒå¹¶è½¬æ¢ç‰¹å¾"""
        if self.method == "pca":
            self.reducer = PCA(n_components=self.n_components)
        elif self.method == "svd":
            self.reducer = TruncatedSVD(n_components=self.n_components)
        elif self.method == "nmf":
            self.reducer = NMF(n_components=self.n_components)

        return self.reducer.fit_transform(features)

    def transform(self, features: np.ndarray) -> np.ndarray:
        """è½¬æ¢æ–°ç‰¹å¾"""
        if self.reducer is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fit_transform")
        return self.reducer.transform(features)
```

## ğŸŒ ç¤¾äº¤æ¨èç®—æ³• (Social Recommendation)

### ç®—æ³•åŸç†

ç¤¾äº¤æ¨èåŸºäºç”¨æˆ·ä¹‹é—´çš„ç¤¾äº¤å…³ç³»ï¼Œåˆ©ç”¨ç¤¾äº¤ç½‘ç»œä¸­çš„ä¿¡ä»»ä¼ æ’­å’Œå½±å“åŠ›æ‰©æ•£è¿›è¡Œæ¨èã€‚æ ¸å¿ƒå‡è®¾æ˜¯æœ‹å‹æˆ–ä¿¡ä»»çš„ç”¨æˆ·ä¼šå–œæ¬¢ç›¸ä¼¼çš„ç‰©å“ã€‚

### æ ¸å¿ƒå®ç°

#### 1. ç¤¾äº¤ç½‘ç»œå»ºæ¨¡

```python
class SocialNetworkModel:
    """ç¤¾äº¤ç½‘ç»œæ¨¡å‹"""

    def __init__(self):
        self.social_graph = defaultdict(dict)  # ç¤¾äº¤å›¾
        self.user_influence = {}               # ç”¨æˆ·å½±å“åŠ›
        self.trust_scores = {}                 # ä¿¡ä»»åˆ†æ•°

    def add_social_connection(self, user_a: str, user_b: str,
                            strength: float):
        """æ·»åŠ ç¤¾äº¤è¿æ¥"""
        # éªŒè¯è¿æ¥å¼ºåº¦
        if not 0 <= strength <= 1:
            raise ValueError("è¿æ¥å¼ºåº¦å¿…é¡»åœ¨0-1ä¹‹é—´")

        # å»ºç«‹åŒå‘è¿æ¥
        self.social_graph[user_a][user_b] = strength
        self.social_graph[user_b][user_a] = strength

        # æ¸…ç©ºç›¸å…³ç¼“å­˜
        self._clear_trust_cache(user_a, user_b)
```

#### 2. å½±å“åŠ›è®¡ç®—

```python
def calculate_social_influence(self, source_user: str,
                             target_user: str, max_depth: int = 3) -> float:
    """
    è®¡ç®—ç¤¾äº¤å½±å“åŠ›

    ä½¿ç”¨BFSç®—æ³•åœ¨ç¤¾äº¤ç½‘ç»œä¸­ä¼ æ’­å½±å“åŠ›
    """
    if source_user == target_user:
        return 1.0

    if source_user not in self.social_graph:
        return 0.0

    # BFSæœç´¢ç¤¾äº¤è·¯å¾„
    visited = set()
    queue = deque([(source_user, 1.0, 0)])  # (ç”¨æˆ·, å½±å“åŠ›, æ·±åº¦)

    while queue:
        current_user, current_influence, depth = queue.popleft()

        if current_user in visited or depth >= max_depth:
            continue

        visited.add(current_user)

        if current_user == target_user:
            return current_influence

        # ä¼ æ’­å½±å“åŠ›åˆ°æœ‹å‹
        if current_user in self.social_graph:
            source_influence = self.user_influence.get(current_user, 0.5)

            for friend, connection_strength in self.social_graph[current_user].items():
                if friend not in visited:
                    # å½±å“åŠ›è¡°å‡è®¡ç®—
                    decay_factor = 0.8 ** depth
                    new_influence = (current_influence *
                                  connection_strength *
                                  source_influence *
                                  decay_factor)
                    queue.append((friend, new_influence, depth + 1))

    return 0.0
```

#### 3. ä¿¡ä»»åº¦è®¡ç®—

```python
def calculate_trust_score(self, user_a: str, user_b: str,
                         max_depth: int = 4) -> float:
    """
    è®¡ç®—ä¿¡ä»»åˆ†æ•°

    ä¿¡ä»»åº¦åŸºäºï¼š
    1. ç›´æ¥è¿æ¥å¼ºåº¦
    2. é—´æ¥è·¯å¾„è¡°å‡
    3. å…±åŒæœ‹å‹æ•°é‡
    """
    if user_a == user_b:
        return 1.0

    # æ£€æŸ¥ç¼“å­˜
    cache_key = tuple(sorted([user_a, user_b]))
    if cache_key in self.trust_scores:
        return self.trust_scores[cache_key]

    # ç›´æ¥è¿æ¥
    if user_b in self.social_graph.get(user_a, {}):
        direct_trust = self.social_graph[user_a][user_b]
        self.trust_scores[cache_key] = direct_trust
        return direct_trust

    # é—´æ¥è¿æ¥ä¿¡ä»»åº¦è®¡ç®—
    trust_score = 0.0
    visited = set()
    queue = deque([(user_a, 1.0, 0)])  # (ç”¨æˆ·, ä¿¡ä»»åº¦, æ·±åº¦)

    while queue:
        current_user, current_trust, depth = queue.popleft()

        if current_user in visited or depth >= max_depth:
            continue

        visited.add(current_user)

        if current_user == user_b:
            trust_score = max(trust_score, current_trust)
            break

        if current_user in self.social_graph:
            for friend, connection_strength in self.social_graph[current_user].items():
                if friend not in visited:
                    # ä¿¡ä»»åº¦è¡°å‡
                    trust_decay = 0.7 ** depth
                    new_trust = current_trust * connection_strength * trust_decay
                    queue.append((friend, new_trust, depth + 1))

    self.trust_scores[cache_key] = trust_score
    return trust_score
```

#### 4. ç¤¾äº¤æ¨èç”Ÿæˆ

```python
def generate_social_recommendations(self, user_id: str,
                                 user_activities: Dict[str, Dict[str, float]],
                                 k: int = 10) -> RecommendationResult:
    """ç”Ÿæˆç¤¾äº¤æ¨è"""

    # 1. è·å–æœ‹å‹æ¨è
    friends_recommendations = self._get_friends_recommendations(
        user_id, user_activities, k * 2
    )

    # 2. è·å–å½±å“åŠ›æ¨è
    influence_recommendations = self._get_influence_based_recommendations(
        user_id, user_activities, k * 2
    )

    # 3. åˆå¹¶æ¨èç»“æœ
    combined_scores = defaultdict(float)
    user_items = set(user_activities.get(user_id, {}).keys())

    # æœ‹å‹æ¨èæƒé‡ï¼š0.6
    for item_id, score in friends_recommendations:
        if item_id not in user_items:
            combined_scores[item_id] += score * 0.6

    # å½±å“åŠ›æ¨èæƒé‡ï¼š0.4
    for item_id, score in influence_recommendations:
        if item_id not in user_items:
            combined_scores[item_id] += score * 0.4

    # 4. ç”Ÿæˆæœ€ç»ˆæ¨è
    recommendations = []
    for item_id, score in combined_scores.items():
        if score > 0:
            normalized_score = min(score / 5.0, 1.0)
            recommendations.append(
                RecommendationItem(item_id, normalized_score)
            )

    recommendations.sort(key=lambda x: x.score, reverse=True)

    return RecommendationResult(user_id, "social_based", recommendations[:k])
```

### ç¤¾äº¤è·¯å¾„åˆ†æ

```python
def find_social_paths(self, source_user: str, target_user: str,
                     max_depth: int = 4) -> List[List[str]]:
    """
    æŸ¥æ‰¾ä¸¤ä¸ªç”¨æˆ·ä¹‹é—´çš„ç¤¾äº¤è·¯å¾„

    ç”¨äºï¼š
    1. ä¿¡ä»»ä¼ æ’­è·¯å¾„åˆ†æ
    2. å½±å“åŠ›æ‰©æ•£è·¯å¾„
    3. ç¤¾äº¤ç½‘ç»œå¯è§†åŒ–
    """
    if source_user == target_user:
        return [[]]

    if source_user not in self.social_graph:
        return []

    paths = []
    visited_global = set()
    queue = deque([(source_user, [source_user])])  # (å½“å‰ç”¨æˆ·, è·¯å¾„)

    while queue:
        current_user, path = queue.popleft()

        if current_user in visited_global or len(path) > max_depth + 1:
            continue

        # ä¸ºæ¯ä¸ªè·¯å¾„ç»´æŠ¤ç‹¬ç«‹çš„visitedé›†åˆ
        visited_path = set(path)

        if current_user == target_user:
            paths.append(path[1:])  # æ’é™¤æºç”¨æˆ·
            continue

        visited_global.add(current_user)

        if current_user in self.social_graph:
            for friend in self.social_graph[current_user]:
                if friend not in visited_path:  # é¿å…å¾ªç¯
                    queue.append((friend, path + [friend]))

    return paths
```

## ğŸ¯ æ··åˆæ¨èç®—æ³• (Hybrid Recommendation)

### ç®—æ³•åŸç†

æ··åˆæ¨èç»“åˆå¤šç§æ¨èç®—æ³•çš„ä¼˜åŠ¿ï¼Œé€šè¿‡åŠ æƒèåˆã€çº§è”æ··åˆæˆ–åˆ‡æ¢ç­–ç•¥æ¥æé«˜æ¨èè´¨é‡ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿé‡‡ç”¨åŠ æƒèåˆç­–ç•¥ï¼ŒåŠ¨æ€è°ƒæ•´å„ç®—æ³•çš„æƒé‡ã€‚

### æ ¸å¿ƒå®ç°

#### 1. åŠ æƒèåˆç­–ç•¥

```python
class WeightedHybridStrategy:
    """åŠ æƒèåˆç­–ç•¥"""

    def __init__(self):
        self.weights = {
            "collaborative": 0.5,
            "content": 0.3,
            "social": 0.2
        }
        self.personalized_weights = {}

    def calculate_hybrid_score(self, cf_score: Optional[float],
                             content_score: Optional[float],
                             social_score: Optional[float]) -> float:
        """
        è®¡ç®—æ··åˆæ¨èåˆ†æ•°

        å¤„ç†ç­–ç•¥ï¼š
        1. æœ‰åˆ†æ•°çš„å¼•æ“å‚ä¸è®¡ç®—
        2. æ— åˆ†æ•°çš„å¼•æ“æƒé‡è¢«é‡æ–°åˆ†é…
        3. ç¡®ä¿åˆ†æ•°å½’ä¸€åŒ–åˆ°[0,1]
        """
        scores = []
        weights = []

        # æ”¶é›†æœ‰æ•ˆåˆ†æ•°å’Œå¯¹åº”æƒé‡
        if cf_score is not None:
            scores.append(cf_score)
            weights.append(self.weights["collaborative"])

        if content_score is not None:
            scores.append(content_score)
            weights.append(self.weights["content"])

        if social_score is not None:
            scores.append(social_score)
            weights.append(self.weights["social"])

        if not scores:
            return 0.0

        # é‡æ–°åˆ†é…æƒé‡
        total_weight = sum(weights)
        if total_weight > 0:
            normalized_weights = [w / total_weight for w in weights]
            hybrid_score = sum(score * weight
                             for score, weight in zip(scores, normalized_weights))
            return hybrid_score

        return 0.0
```

#### 2. è‡ªé€‚åº”æƒé‡è°ƒæ•´

```python
class AdaptiveWeightAdjustment:
    """è‡ªé€‚åº”æƒé‡è°ƒæ•´"""

    def __init__(self):
        self.performance_history = defaultdict(list)
        self.adjustment_rate = 0.1

    def update_weights(self, user_id: str,
                      engine_performance: Dict[str, float]):
        """
        åŸºäºæ€§èƒ½æŒ‡æ ‡è°ƒæ•´æƒé‡

        æ€§èƒ½æŒ‡æ ‡åŒ…æ‹¬ï¼š
        1. ç‚¹å‡»ç‡ (CTR)
        2. è½¬åŒ–ç‡
        3. ç”¨æˆ·æ»¡æ„åº¦
        4. æ¨èå¤šæ ·æ€§
        """

        # è®°å½•æ€§èƒ½å†å²
        for engine, performance in engine_performance.items():
            self.performance_history[engine].append(performance)

        # è®¡ç®—å¹³å‡æ€§èƒ½
        avg_performance = {}
        for engine in self.weights.keys():
            history = self.performance_history[engine]
            if history:
                avg_performance[engine] = np.mean(history[-10:])  # æœ€è¿‘10æ¬¡
            else:
                avg_performance[engine] = 0.5  # é»˜è®¤å€¼

        # è°ƒæ•´æƒé‡
        self._adjust_weight_based_on_performance(avg_performance)

    def _adjust_weight_based_on_performance(self, performance: Dict[str, float]):
        """åŸºäºæ€§èƒ½è°ƒæ•´æƒé‡"""

        # è®¡ç®—æ€§èƒ½åˆ†æ•°æ€»å’Œ
        total_performance = sum(performance.values())

        if total_performance > 0:
            # è®¡ç®—æ–°æƒé‡
            new_weights = {}
            for engine, base_weight in self.weights.items():
                perf_score = performance.get(engine, 0.5)
                # ç»“åˆåŸºç¡€æƒé‡å’Œæ€§èƒ½åˆ†æ•°
                new_weight = (base_weight * 0.7 + perf_score * 0.3)
                new_weights[engine] = new_weight

            # å½’ä¸€åŒ–æƒé‡
            total_new_weight = sum(new_weights.values())
            if total_new_weight > 0:
                for engine in new_weights:
                    new_weights[engine] /= total_new_weight

                # å¹³æ»‘è°ƒæ•´ï¼ˆé¿å…çªå˜ï¼‰
                for engine in self.weights:
                    self.weights[engine] = (
                        self.weights[engine] * (1 - self.adjustment_rate) +
                        new_weights[engine] * self.adjustment_rate
                    )
```

#### 3. å¤šæ ·æ€§å¢å¼º

```python
class DiversityEnhancer:
    """æ¨èå¤šæ ·æ€§å¢å¼º"""

    def __init__(self):
        self.category_weights = defaultdict(float)
        self.recommendation_history = defaultdict(list)

    def enhance_diversity(self, recommendations: List[RecommendationItem],
                         k: int) -> List[RecommendationItem]:
        """
        å¢å¼ºæ¨èå¤šæ ·æ€§

        ç­–ç•¥ï¼š
        1. ç±»åˆ«å¹³è¡¡
        2. æ—¶é—´åˆ†æ•£
        3. å…´è¶£æ¢ç´¢
        """
        if len(recommendations) <= k:
            return recommendations

        diverse_recommendations = []
        used_categories = set()
        category_count = defaultdict(int)
        max_per_category = max(1, k // 3)  # æ¯ä¸ªç±»åˆ«æœ€å¤šå 1/3

        # æŒ‰åˆ†æ•°æ’åº
        sorted_recs = sorted(recommendations, key=lambda x: x.score, reverse=True)

        # ç¬¬ä¸€è½®ï¼šç±»åˆ«å¹³è¡¡é€‰æ‹©
        for rec in sorted_recs:
            if len(diverse_recommendations) >= k:
                break

            category = self._extract_category(rec.item_id)

            # ç±»åˆ«å¤šæ ·æ€§æ§åˆ¶
            if (category not in used_categories or
                category_count[category] < max_per_category):
                diverse_recommendations.append(rec)
                used_categories.add(category)
                category_count[category] += 1

        # ç¬¬äºŒè½®ï¼šå¡«å……å‰©ä½™ä½ç½®
        remaining_items = [rec for rec in sorted_recs
                          if rec not in diverse_recommendations]
        diverse_recommendations.extend(
            remaining_items[:k - len(diverse_recommendations)]
        )

        return diverse_recommendations
```

#### 4. æ¨èè§£é‡Šç”Ÿæˆ

```python
def generate_recommendation_explanation(self, user_id: str,
                                      item_id: str) -> Dict[str, Any]:
    """
    ç”Ÿæˆæ¨èè§£é‡Š

    è§£é‡ŠåŒ…æ‹¬ï¼š
    1. å„å¼•æ“è´¡çŒ®åˆ†æ•°
    2. æ¨èåŸå› åˆ†æ
    3. ç›¸ä¼¼ç”¨æˆ·/ç‰©å“ä¿¡æ¯
    """

    explanation = {
        "item_id": item_id,
        "explanations": [],
        "confidence_scores": {},
        "similar_users": [],
        "similar_items": []
    }

    # ååŒè¿‡æ»¤è§£é‡Š
    cf_score = self._get_cf_score(user_id, item_id)
    if cf_score > 0:
        similar_users = self._get_similar_users_for_item(user_id, item_id)
        explanation["explanations"].append({
            "engine": "collaborative",
            "reason": f"ä¸æ‚¨ç›¸ä¼¼çš„ç”¨æˆ·ä¹Ÿå–œæ¬¢{item_id}",
            "score": cf_score,
            "support": len(similar_users)
        })
        explanation["similar_users"] = similar_users[:3]

    # å†…å®¹æ¨èè§£é‡Š
    content_score = self._get_content_score(user_id, item_id)
    if content_score > 0:
        similar_items = self._get_similar_items(user_id, item_id)
        explanation["explanations"].append({
            "engine": "content",
            "reason": f"åŸºäºæ‚¨çš„å†å²åå¥½æ¨è{item_id}",
            "score": content_score,
            "support": len(similar_items)
        })
        explanation["similar_items"] = similar_items[:3]

    # ç¤¾äº¤æ¨èè§£é‡Š
    social_score = self._get_social_score(user_id, item_id)
    if social_score > 0:
        friends_liked = self._get_friends_who_liked(user_id, item_id)
        explanation["explanations"].append({
            "engine": "social",
            "reason": f"æ‚¨çš„æœ‹å‹å–œæ¬¢{item_id}",
            "score": social_score,
            "support": len(friends_liked)
        })

    # è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦
    explanation["overall_confidence"] = self._calculate_overall_confidence(
        explanation["explanations"]
    )

    return explanation
```

## ğŸ“Š ç®—æ³•è¯„ä¼°æŒ‡æ ‡

### å‡†ç¡®æ€§æŒ‡æ ‡

```python
class RecommendationMetrics:
    """æ¨èç®—æ³•è¯„ä¼°æŒ‡æ ‡"""

    @staticmethod
    def precision_at_k(recommended: List[str], relevant: List[str], k: int) -> float:
        """Precision@K"""
        if k == 0:
            return 0.0

        recommended_k = recommended[:k]
        relevant_set = set(relevant)

        hits = sum(1 for item in recommended_k if item in relevant_set)
        return hits / k

    @staticmethod
    def recall_at_k(recommended: List[str], relevant: List[str], k: int) -> float:
        """Recall@K"""
        if not relevant:
            return 0.0

        recommended_k = recommended[:k]
        relevant_set = set(relevant)

        hits = sum(1 for item in recommended_k if item in relevant_set)
        return hits / len(relevant)

    @staticmethod
    def ndcg_at_k(recommended: List[str], relevant: List[str],
                  relevance_scores: List[float], k: int) -> float:
        """NDCG@K"""
        def dcg_at_k(relevances: List[float], k: int) -> float:
            relevances_k = relevances[:k]
            return sum(rel / np.log2(i + 2)
                      for i, rel in enumerate(relevances_k))

        # å®é™…DCG
        actual_relevances = []
        for item in recommended[:k]:
            if item in relevant:
                idx = relevant.index(item)
                actual_relevances.append(relevance_scores[idx])
            else:
                actual_relevances.append(0.0)

        actual_dcg = dcg_at_k(actual_relevances, k)

        # ç†æƒ³DCG
        ideal_relevances = sorted(relevance_scores, reverse=True)
        ideal_dcg = dcg_at_k(ideal_relevances, k)

        if ideal_dcg == 0:
            return 0.0

        return actual_dcg / ideal_dcg
```

### å¤šæ ·æ€§æŒ‡æ ‡

```python
@staticmethod
def intra_list_diversity(recommendations: List[str],
                        item_features: Dict[str, np.ndarray]) -> float:
    """åˆ—è¡¨å†…å¤šæ ·æ€§"""
    if len(recommendations) < 2:
        return 0.0

    total_similarity = 0.0
    pair_count = 0

    for i in range(len(recommendations)):
        for j in range(i + 1, len(recommendations)):
            item_i = recommendations[i]
            item_j = recommendations[j]

            if item_i in item_features and item_j in item_features:
                similarity = np.dot(item_features[item_i], item_features[item_j])
                total_similarity += similarity
                pair_count += 1

    if pair_count == 0:
        return 0.0

    avg_similarity = total_similarity / pair_count
    return 1.0 - avg_similarity  # è½¬æ¢ä¸ºå¤šæ ·æ€§åˆ†æ•°
```

### æ–°é¢–æ€§æŒ‡æ ‡

```python
@staticmethod
def novelty(recommendations: List[str],
           item_popularity: Dict[str, float]) -> float:
    """æ¨èæ–°é¢–æ€§"""
    if not recommendations:
        return 0.0

    # è®¡ç®—æ¯ä¸ªç‰©å“çš„è´Ÿå¯¹æ•°æµè¡Œåº¦
    novelty_scores = []
    for item in recommendations:
        popularity = item_popularity.get(item, 0.001)  # é¿å…é™¤é›¶
        novelty_score = -np.log2(popularity)
        novelty_scores.append(novelty_score)

    return np.mean(novelty_scores)
```

## ğŸ¯ ç®—æ³•ä¼˜åŒ–ç­–ç•¥

### 1. å†·å¯åŠ¨é—®é¢˜è§£å†³

```python
class ColdStartSolver:
    """å†·å¯åŠ¨é—®é¢˜è§£å†³"""

    def handle_new_user(self, user_id: str,
                       minimal_info: Dict[str, Any]) -> List[str]:
        """å¤„ç†æ–°ç”¨æˆ·å†·å¯åŠ¨"""

        # ç­–ç•¥1ï¼šåŸºäºäººå£ç»Ÿè®¡å­¦çš„æ¨è
        if "demographics" in minimal_info:
            demo_recommendations = self._demographic_based_recommendation(
                minimal_info["demographics"]
            )

        # ç­–ç•¥2ï¼šåŸºäºæ³¨å†Œæ—¶é€‰æ‹©çš„å…´è¶£
        if "interests" in minimal_info:
            interest_recommendations = self._interest_based_recommendation(
                minimal_info["interests"]
            )

        # ç­–ç•¥3ï¼šçƒ­é—¨ç‰©å“æ¨è
        popular_recommendations = self._get_popular_items()

        # æ··åˆç­–ç•¥
        final_recommendations = self._combine_cold_start_strategies([
            demo_recommendations,
            interest_recommendations,
            popular_recommendations
        ])

        return final_recommendations[:10]
```

### 2. å®æ—¶æ¨èä¼˜åŒ–

```python
class RealTimeRecommendation:
    """å®æ—¶æ¨èä¼˜åŒ–"""

    def __init__(self):
        self.user_state_cache = {}
        self.item_candidate_pool = {}
        self.precomputed_similarities = {}

    async def realtime_recommend(self, user_id: str,
                               context: Dict[str, Any]) -> List[str]:
        """å®æ—¶æ¨èç”Ÿæˆ"""

        # 1. è·å–ç”¨æˆ·å½“å‰çŠ¶æ€
        user_state = await self._get_user_state(user_id)

        # 2. åŸºäºä¸Šä¸‹æ–‡è¿‡æ»¤å€™é€‰æ± 
        filtered_candidates = self._filter_by_context(
            self.item_candidate_pool[user_id], context
        )

        # 3. å¿«é€Ÿç›¸ä¼¼åº¦è®¡ç®—
        scores = []
        for item in filtered_candidates:
            score = self._fast_similarity_calculation(user_state, item)
            scores.append((item, score))

        # 4. æ’åºå¹¶è¿”å›
        scores.sort(key=lambda x: x[1], reverse=True)
        return [item for item, _ in scores[:10]]
```

è¿™å¥—æ¨èç®—æ³•ä½“ç³»é€šè¿‡TDDæ–¹æ³•è®ºç¡®ä¿äº†ä»£ç è´¨é‡å’Œç®—æ³•å‡†ç¡®æ€§ï¼Œä¸ºç™¾ä¸‡çº§æ™ºèƒ½ä½“å¹³å°æä¾›äº†é«˜è´¨é‡çš„æ¨èæœåŠ¡ã€‚æ¯ä¸ªç®—æ³•éƒ½ç»è¿‡å……åˆ†æµ‹è¯•ï¼Œå…·å¤‡è‰¯å¥½çš„æ‰©å±•æ€§å’Œæ€§èƒ½è¡¨ç°ã€‚