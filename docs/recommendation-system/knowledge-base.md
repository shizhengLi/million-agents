# æ¨èç³»ç»Ÿæ ¸å¿ƒçŸ¥è¯†ä½“ç³»

## ğŸ“š çŸ¥è¯†å›¾è°±æ¦‚è§ˆ

```
æ¨èç³»ç»ŸçŸ¥è¯†ä½“ç³»ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ğŸ¯ æ¨èç³»ç»Ÿæ ¸å¿ƒç†è®º                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   åŸºç¡€ç†è®º   â”‚   ç®—æ³•åŸç†   â”‚   å·¥ç¨‹å®è·µ   â”‚   è¯„ä¼°ä½“ç³»   â”‚ â”‚
â”‚  â”‚  â€¢ ååŒè¿‡æ»¤  â”‚  â€¢ ç›¸ä¼¼åº¦   â”‚  â€¢ æ¶æ„è®¾è®¡  â”‚  â€¢ å‡†ç¡®æ€§   â”‚ â”‚
â”‚  â”‚  â€¢ å†…å®¹æ¨è  â”‚  â€¢ çŸ©é˜µåˆ†è§£  â”‚  â€¢ æ€§èƒ½ä¼˜åŒ–  â”‚  â€¢ å¤šæ ·æ€§   â”‚ â”‚
â”‚  â”‚  â€¢ æ·±åº¦å­¦ä¹   â”‚  â€¢ å› å­åˆ†è§£  â”‚  â€¢ åˆ†å¸ƒå¼   â”‚  â€¢ æ–°é¢–æ€§   â”‚ â”‚
â”‚  â”‚  â€¢ å¤šè‡‚èµŒåš  â”‚  â€¢ å›¾ç¥ç»ç½‘ç»œâ”‚  â€¢ å®æ—¶è®¡ç®—  â”‚  â€¢ å•†ä¸šæŒ‡æ ‡ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ åŸºç¡€ç†è®ºçŸ¥è¯†

### 1. æ¨èç³»ç»Ÿåˆ†ç±»

#### ååŒè¿‡æ»¤ (Collaborative Filtering)
```python
"""
ååŒè¿‡æ»¤æ ¸å¿ƒæ€æƒ³ï¼š
- åŸºäºç”¨æˆ·çš„ç›¸ä¼¼è¡Œä¸ºè¿›è¡Œæ¨è
- ä¸éœ€è¦ç‰©å“çš„å†…å®¹ä¿¡æ¯
- å­˜åœ¨å†·å¯åŠ¨å’Œç¨€ç–æ€§é—®é¢˜
"""

# ç”¨æˆ·ç›¸ä¼¼åº¦è®¡ç®—
def user_similarity(user_a_ratings, user_b_ratings):
    """ç”¨æˆ·ç›¸ä¼¼åº¦è®¡ç®—çš„æ ¸å¿ƒå…¬å¼"""
    common_items = set(user_a_ratings.keys()) & set(user_b_ratings.keys())

    if not common_items:
        return 0.0

    # çš®å°”é€Šç›¸å…³ç³»æ•°
    mean_a = np.mean([user_a_ratings[item] for item in common_items])
    mean_b = np.mean([user_b_ratings[item] for item in common_items])

    numerator = sum((user_a_ratings[item] - mean_a) *
                   (user_b_ratings[item] - mean_b)
                   for item in common_items)

    sum_sq_a = sum((user_a_ratings[item] - mean_a) ** 2
                  for item in common_items)
    sum_sq_b = sum((user_b_ratings[item] - mean_b) ** 2
                  for item in common_items)

    denominator = np.sqrt(sum_sq_a * sum_sq_b)

    return numerator / denominator if denominator != 0 else 0.0
```

#### å†…å®¹æ¨è (Content-Based Recommendation)
```python
"""
å†…å®¹æ¨èæ ¸å¿ƒæ€æƒ³ï¼š
- åŸºäºç‰©å“ç‰¹å¾å’Œç”¨æˆ·å†å²åå¥½
- éœ€è¦æå–ç‰©å“çš„å†…å®¹ç‰¹å¾
- å¯ä»¥è§£å†³å†·å¯åŠ¨é—®é¢˜
"""

# TF-IDFç®—æ³•
def tfidf_score(term, document, corpus):
    """TF-IDFè®¡ç®—"""
    # è¯é¢‘ (TF)
    tf = document.count(term) / len(document)

    # é€†æ–‡æ¡£é¢‘ç‡ (IDF)
    doc_count = sum(1 for doc in corpus if term in doc)
    idf = np.log(len(corpus) / (doc_count + 1))

    return tf * idf

# ä½™å¼¦ç›¸ä¼¼åº¦
def cosine_similarity(vector_a, vector_b):
    """ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—"""
    dot_product = np.dot(vector_a, vector_b)
    norm_a = np.linalg.norm(vector_a)
    norm_b = np.linalg.norm(vector_b)

    if norm_a == 0 or norm_b == 0:
        return 0.0

    return dot_product / (norm_a * norm_b)
```

### 2. çŸ©é˜µåˆ†è§£ (Matrix Factorization)

#### åŸºç¡€çŸ©é˜µåˆ†è§£
```python
"""
çŸ©é˜µåˆ†è§£æ•°å­¦åŸç†ï¼š
- å°†ç”¨æˆ·-ç‰©å“è¯„åˆ†çŸ©é˜µåˆ†è§£ä¸ºç”¨æˆ·ç‰¹å¾çŸ©é˜µå’Œç‰©å“ç‰¹å¾çŸ©é˜µ
- R â‰ˆ P Ã— Q^T
- P: ç”¨æˆ·ç‰¹å¾çŸ©é˜µ (m Ã— k)
- Q: ç‰©å“ç‰¹å¾çŸ©é˜µ (n Ã— k)
- k: éšå› å­æ•°é‡
"""

class BasicMatrixFactorization:
    """åŸºç¡€çŸ©é˜µåˆ†è§£å®ç°"""

    def __init__(self, n_factors=50, learning_rate=0.01, regularization=0.01):
        self.n_factors = n_factors
        self.learning_rate = learning_rate
        self.regularization = regularization

    def fit(self, ratings_matrix, n_epochs=100):
        """è®­ç»ƒçŸ©é˜µåˆ†è§£æ¨¡å‹"""
        n_users, n_items = ratings_matrix.shape

        # åˆå§‹åŒ–ç‰¹å¾çŸ©é˜µ
        self.P = np.random.normal(0, 0.1, (n_users, self.n_factors))
        self.Q = np.random.normal(0, 0.1, (n_items, self.n_factors))

        # è®­ç»ƒè¿‡ç¨‹
        for epoch in range(n_epochs):
            for u, i, r in self._get_ratings(ratings_matrix):
                # é¢„æµ‹è¯„åˆ†
                prediction = np.dot(self.P[u], self.Q[i])

                # è®¡ç®—è¯¯å·®
                error = r - prediction

                # æ¢¯åº¦ä¸‹é™æ›´æ–°
                self.P[u] += self.learning_rate * (error * self.Q[i] -
                                                 self.regularization * self.P[u])
                self.Q[i] += self.learning_rate * (error * self.P[u] -
                                                 self.regularization * self.Q[i])

    def predict(self, user_id, item_id):
        """é¢„æµ‹ç”¨æˆ·å¯¹ç‰©å“çš„è¯„åˆ†"""
        return np.dot(self.P[user_id], self.Q[item_id])
```

#### SVD++ç®—æ³•
```python
class SVDPlusPlus:
    """SVD++ç®—æ³•å®ç°"""

    def __init__(self, n_factors=50, learning_rate=0.01, regularization=0.01):
        self.n_factors = n_factors
        self.learning_rate = learning_rate
        self.regularization = regularization

    def fit(self, ratings_matrix, n_epochs=20):
        """SVD++è®­ç»ƒ"""
        n_users, n_items = ratings_matrix.shape

        # åˆå§‹åŒ–å‚æ•°
        self.user_factors = np.random.normal(0, 0.1, (n_users, self.n_factors))
        self.item_factors = np.random.normal(0, 0.1, (n_items, self.n_factors))
        self.user_bias = np.zeros(n_users)
        self.item_bias = np.zeros(n_items)
        self.global_bias = np.mean(ratings_matrix[ratings_matrix > 0])

        # éšå¼åé¦ˆå‚æ•°
        self.implicit_factors = np.random.normal(0, 0.1,
                                               (n_items, self.n_factors))

        for epoch in range(n_epochs):
            for u, i, r in self._get_ratings(ratings_matrix):
                # è·å–ç”¨æˆ·æ˜¾å¼å’Œéšå¼åé¦ˆ
                explicit_items = self._get_user_items(u, ratings_matrix)

                # è®¡ç®—éšå¼åé¦ˆå‘é‡
                implicit_sum = np.zeros(self.n_factors)
                for j in explicit_items:
                    implicit_sum += self.implicit_factors[j]

                implicit_sum /= np.sqrt(len(explicit_items))

                # é¢„æµ‹è¯„åˆ†
                prediction = (self.global_bias + self.user_bias[u] +
                            self.item_bias[i] +
                            np.dot(self.user_factors[u], self.item_factors[i]) +
                            np.dot(self.user_factors[u], implicit_sum))

                # è®¡ç®—è¯¯å·®å¹¶æ›´æ–°å‚æ•°
                error = r - prediction
                self._update_parameters(u, i, error, explicit_items, implicit_sum)
```

### 3. æ·±åº¦å­¦ä¹ æ¨èç®—æ³•

#### Neural Collaborative Filtering (NCF)
```python
"""
NCFæ¶æ„è®¾è®¡ï¼š
- GMFéƒ¨åˆ†ï¼šå¹¿ä¹‰çŸ©é˜µåˆ†è§£
- MLPéƒ¨åˆ†ï¼šå¤šå±‚æ„ŸçŸ¥æœº
- èåˆå±‚ï¼šç»“åˆGMFå’ŒMLPçš„è¾“å‡º
"""

import torch
import torch.nn as nn

class NeuralCollaborativeFiltering(nn.Module):
    """ç¥ç»ååŒè¿‡æ»¤ç½‘ç»œ"""

    def __init__(self, n_users, n_items, embedding_dim=64, hidden_layers=[128, 64]):
        super().__init__()
        self.n_users = n_users
        self.n_items = n_items
        self.embedding_dim = embedding_dim

        # åµŒå…¥å±‚
        self.user_embedding = nn.Embedding(n_users, embedding_dim)
        self.item_embedding = nn.Embedding(n_items, embedding_dim)

        # GMFéƒ¨åˆ†
        self.gmf_user_embedding = nn.Embedding(n_users, embedding_dim)
        self.gmf_item_embedding = nn.Embedding(n_items, embedding_dim)

        # MLPéƒ¨åˆ†
        mlp_input_dim = embedding_dim * 2
        mlp_layers = []
        current_dim = mlp_input_dim

        for hidden_dim in hidden_layers:
            mlp_layers.append(nn.Linear(current_dim, hidden_dim))
            mlp_layers.append(nn.ReLU())
            mlp_layers.append(nn.Dropout(0.2))
            current_dim = hidden_dim

        self.mlp = nn.Sequential(*mlp_layers)

        # èåˆå±‚
        self.fusion = nn.Linear(embedding_dim + current_dim, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, user_ids, item_ids):
        # GMFéƒ¨åˆ†
        gmf_user = self.gmf_user_embedding(user_ids)
        gmf_item = self.gmf_item_embedding(item_ids)
        gmf_output = gmf_user * gmf_item

        # MLPéƒ¨åˆ†
        mlp_user = self.user_embedding(user_ids)
        mlp_item = self.item_embedding(item_ids)
        mlp_input = torch.cat([mlp_user, mlp_item], dim=1)
        mlp_output = self.mlp(mlp_input)

        # èåˆé¢„æµ‹
        fusion_input = torch.cat([gmf_output, mlp_output], dim=1)
        output = self.fusion(fusion_input)
        return self.sigmoid(output)
```

#### DeepFMæ¨¡å‹
```python
"""
DeepFMæ¶æ„ï¼š
- FMéƒ¨åˆ†ï¼šå› å­åˆ†è§£æœºï¼Œæ•è·ä½é˜¶ç‰¹å¾äº¤äº’
- Deepéƒ¨åˆ†ï¼šæ·±åº¦ç¥ç»ç½‘ç»œï¼Œæ•è·é«˜é˜¶ç‰¹å¾äº¤äº’
- å…±äº«è¾“å…¥ï¼šç›¸åŒçš„ç‰¹å¾åµŒå…¥
"""

class DeepFM(nn.Module):
    """Deep Factorization Machine"""

    def __init__(self, feature_dims, embedding_dim=8, hidden_dims=[128, 64]):
        super().__init__()
        self.feature_dims = feature_dims
        self.embedding_dim = embedding_dim

        # ç‰¹å¾åµŒå…¥
        self.embeddings = nn.ModuleList([
            nn.Embedding(dim, embedding_dim) for dim in feature_dims
        ])

        # FMä¸€é˜¶é¡¹
        self.fm_first_order = nn.ModuleList([
            nn.Embedding(dim, 1) for dim in feature_dims
        ])

        # æ·±åº¦ç½‘ç»œ
        deep_input_dim = len(feature_dims) * embedding_dim
        deep_layers = []
        current_dim = deep_input_dim

        for hidden_dim in hidden_dims:
            deep_layers.append(nn.Linear(current_dim, hidden_dim))
            deep_layers.append(nn.ReLU())
            deep_layers.append(nn.Dropout(0.3))
            current_dim = hidden_dim

        self.deep = nn.Sequential(*deep_layers)

        # è¾“å‡ºå±‚
        self.output_layer = nn.Linear(current_dim + 1, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, feature_indices):
        # è·å–åµŒå…¥
        embeddings = [emb(indices) for emb, indices in
                     zip(self.embeddings, feature_indices)]

        # FMä¸€é˜¶é¡¹
        first_order = [emb(indices) for emb, indices in
                      zip(self.fm_first_order, feature_indices)]
        first_order_sum = sum(first_order)

        # FMäºŒé˜¶é¡¹
        embeddings_stack = torch.stack(embeddings, dim=1)
        square_of_sum = torch.sum(embeddings_stack, dim=1) ** 2
        sum_of_square = torch.sum(embeddings_stack ** 2, dim=1)
        fm_second_order = 0.5 * (square_of_sum - sum_of_square).sum(dim=1, keepdim=True)

        # æ·±åº¦ç½‘ç»œ
        deep_input = torch.cat(embeddings, dim=1)
        deep_output = self.deep(deep_input)

        # ç»„åˆè¾“å‡º
        output_input = torch.cat([first_order_sum, fm_second_order, deep_output], dim=1)
        output = self.output_layer(output_input)

        return self.sigmoid(output)
```

### 4. å›¾ç¥ç»ç½‘ç»œæ¨è

#### GraphSAGEåœ¨æ¨èä¸­çš„åº”ç”¨
```python
"""
å›¾ç¥ç»ç½‘ç»œåœ¨æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨ï¼š
- ç”¨æˆ·-ç‰©å“äºŒéƒ¨å›¾å»ºæ¨¡
- èŠ‚ç‚¹åµŒå…¥å­¦ä¹ 
- é‚»å±…èšåˆå’Œä¼ æ’­
"""

import dgl
import dgl.nn as dglnn

class GraphSAGERecommender(nn.Module):
    """åŸºäºGraphSAGEçš„æ¨èæ¨¡å‹"""

    def __init__(self, in_dim, hidden_dim=64, n_layers=2):
        super().__init__()
        self.layers = nn.ModuleList()

        # GraphSAGEå±‚
        self.layers.append(dglnn.SAGEConv(in_dim, hidden_dim, 'mean'))
        for _ in range(n_layers - 1):
            self.layers.append(dglnn.SAGEConv(hidden_dim, hidden_dim, 'mean'))

        # é¢„æµ‹å±‚
        self.predictor = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )

    def forward(self, graph, features, user_nodes, item_nodes):
        # å›¾å·ç§¯
        h = features
        for layer in self.layers:
            h = layer(graph, h)

        # è·å–ç”¨æˆ·å’Œç‰©å“åµŒå…¥
        user_embeddings = h[user_nodes]
        item_embeddings = h[item_nodes]

        # é¢„æµ‹äº¤äº’æ¦‚ç‡
        user_item_pairs = torch.cat([user_embeddings, item_embeddings], dim=1)
        scores = self.predictor(user_item_pairs)

        return scores
```

## ğŸ”§ æœºå™¨å­¦ä¹ åŸºç¡€

### 1. ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•

```python
class SimilarityMetrics:
    """ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•é›†åˆ"""

    @staticmethod
    def euclidean_distance(vector_a, vector_b):
        """æ¬§æ°è·ç¦»"""
        return np.linalg.norm(vector_a - vector_b)

    @staticmethod
    def manhattan_distance(vector_a, vector_b):
        """æ›¼å“ˆé¡¿è·ç¦»"""
        return np.sum(np.abs(vector_a - vector_b))

    @staticmethod
    def jaccard_similarity(set_a, set_b):
        """Jaccardç›¸ä¼¼åº¦"""
        intersection = len(set_a & set_b)
        union = len(set_a | set_b)
        return intersection / union if union != 0 else 0.0

    @staticmethod
    def pearson_correlation(x, y):
        """çš®å°”é€Šç›¸å…³ç³»æ•°"""
        if len(x) != len(y):
            raise ValueError("å‘é‡é•¿åº¦å¿…é¡»ç›¸ç­‰")

        n = len(x)
        if n == 0:
            return 0.0

        mean_x, mean_y = np.mean(x), np.mean(y)

        if np.std(x) == 0 or np.std(y) == 0:
            return 0.0

        correlation_matrix = np.corrcoef(x, y)
        return correlation_matrix[0, 1]

    @staticmethod
    def spearman_correlation(x, y):
        """æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°"""
        if len(x) != len(y):
            raise ValueError("å‘é‡é•¿åº¦å¿…é¡»ç›¸ç­‰")

        # è½¬æ¢ä¸ºç§©
        rank_x = np.argsort(np.argsort(x))
        rank_y = np.argsort(np.argsort(y))

        # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
        return SimilarityMetrics.pearson_correlation(rank_x, rank_y)
```

### 2. é™ç»´æŠ€æœ¯

#### PCA (ä¸»æˆåˆ†åˆ†æ)
```python
class PCA:
    """ä¸»æˆåˆ†åˆ†æå®ç°"""

    def __init__(self, n_components):
        self.n_components = n_components
        self.components = None
        self.mean = None
        self.explained_variance = None

    def fit(self, X):
        """è®­ç»ƒPCAæ¨¡å‹"""
        # ä¸­å¿ƒåŒ–
        self.mean = np.mean(X, axis=0)
        X_centered = X - self.mean

        # è®¡ç®—åæ–¹å·®çŸ©é˜µ
        cov_matrix = np.cov(X_centered.T)

        # ç‰¹å¾åˆ†è§£
        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)

        # æŒ‰ç‰¹å¾å€¼æ’åº
        sorted_indices = np.argsort(eigenvalues)[::-1]
        self.components = eigenvectors[:, sorted_indices[:self.n_components]]
        self.explained_variance = eigenvalues[sorted_indices[:self.n_components]]

    def transform(self, X):
        """é™ç»´å˜æ¢"""
        X_centered = X - self.mean
        return np.dot(X_centered, self.components)

    def fit_transform(self, X):
        """è®­ç»ƒå¹¶å˜æ¢"""
        self.fit(X)
        return self.transform(X)
```

#### SVD (å¥‡å¼‚å€¼åˆ†è§£)
```python
class TruncatedSVD:
    """æˆªæ–­å¥‡å¼‚å€¼åˆ†è§£"""

    def __init__(self, n_components):
        self.n_components = n_components
        self.components = None
        self.singular_values = None
        self.explained_variance = None

    def fit_transform(self, X):
        """è®­ç»ƒå¹¶å˜æ¢"""
        # æ‰§è¡ŒSVD
        U, S, Vt = np.linalg.svd(X, full_matrices=False)

        # æˆªæ–­åˆ°æŒ‡å®šç»„ä»¶æ•°
        self.components = Vt[:self.n_components].T
        self.singular_values = S[:self.n_components]

        # è®¡ç®—è§£é‡Šæ–¹å·®
        self.explained_variance = (S[:self.n_components] ** 2) / (X.shape[0] - 1)

        # è¿”å›é™ç»´åçš„æ•°æ®
        X_reduced = U[:, :self.n_components] * S[:self.n_components]

        return X_reduced
```

### 3. èšç±»ç®—æ³•

#### K-Meansèšç±»
```python
class KMeans:
    """K-Meansèšç±»ç®—æ³•"""

    def __init__(self, n_clusters=8, max_iter=300, random_state=None):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.random_state = random_state
        self.centroids = None
        self.labels = None

    def fit(self, X):
        """è®­ç»ƒK-Meansæ¨¡å‹"""
        n_samples, n_features = X.shape

        # åˆå§‹åŒ–è´¨å¿ƒ
        if self.random_state:
            np.random.seed(self.random_state)

        random_indices = np.random.choice(n_samples, self.n_clusters, replace=False)
        self.centroids = X[random_indices]

        for _ in range(self.max_iter):
            # åˆ†é…æ ·æœ¬åˆ°æœ€è¿‘çš„è´¨å¿ƒ
            distances = np.sqrt(((X - self.centroids[:, np.newaxis]) ** 2).sum(axis=2))
            self.labels = np.argmin(distances, axis=0)

            # æ›´æ–°è´¨å¿ƒ
            new_centroids = np.zeros((self.n_clusters, n_features))
            for i in range(self.n_clusters):
                cluster_points = X[self.labels == i]
                if len(cluster_points) > 0:
                    new_centroids[i] = np.mean(cluster_points, axis=0)
                else:
                    new_centroids[i] = self.centroids[i]

            # æ£€æŸ¥æ”¶æ•›
            if np.allclose(self.centroids, new_centroids):
                break

            self.centroids = new_centroids

    def predict(self, X):
        """é¢„æµ‹æ ·æœ¬çš„èšç±»æ ‡ç­¾"""
        distances = np.sqrt(((X - self.centroids[:, np.newaxis]) ** 2).sum(axis=2))
        return np.argmin(distances, axis=0)
```

## ğŸ“Š ç»Ÿè®¡å­¦åŸºç¡€

### 1. æ¦‚ç‡åˆ†å¸ƒ

```python
class ProbabilityDistributions:
    """å¸¸è§æ¦‚ç‡åˆ†å¸ƒ"""

    @staticmethod
    def normal_distribution(x, mu=0, sigma=1):
        """æ­£æ€åˆ†å¸ƒæ¦‚ç‡å¯†åº¦å‡½æ•°"""
        return (1 / (sigma * np.sqrt(2 * np.pi))) * \
               np.exp(-0.5 * ((x - mu) / sigma) ** 2)

    @staticmethod
    def binomial_distribution(k, n, p):
        """äºŒé¡¹åˆ†å¸ƒæ¦‚ç‡è´¨é‡å‡½æ•°"""
        from math import comb
        return comb(n, k) * (p ** k) * ((1 - p) ** (n - k))

    @staticmethod
    def poisson_distribution(k, lam):
        """æ³Šæ¾åˆ†å¸ƒæ¦‚ç‡è´¨é‡å‡½æ•°"""
        return (lam ** k * np.exp(-lam)) / np.math.factorial(k)
```

### 2. å‡è®¾æ£€éªŒ

```python
class HypothesisTesting:
    """å‡è®¾æ£€éªŒæ–¹æ³•"""

    @staticmethod
    def t_test(sample1, sample2, alpha=0.05):
        """ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ"""
        from scipy import stats

        t_statistic, p_value = stats.ttest_ind(sample1, sample2)

        result = {
            't_statistic': t_statistic,
            'p_value': p_value,
            'reject_null': p_value < alpha,
            'confidence_level': 1 - alpha
        }

        return result

    @staticmethod
    def chi_squared_test(observed, expected, alpha=0.05):
        """å¡æ–¹æ£€éªŒ"""
        from scipy import stats

        chi2_statistic, p_value = stats.chisquare(observed, expected)

        result = {
            'chi2_statistic': chi2_statistic,
            'p_value': p_value,
            'reject_null': p_value < alpha,
            'degrees_of_freedom': len(observed) - 1
        }

        return result
```

### 3. ç›¸å…³æ€§åˆ†æ

```python
class CorrelationAnalysis:
    """ç›¸å…³æ€§åˆ†æå·¥å…·"""

    @staticmethod
    def correlation_matrix(data):
        """è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ"""
        return np.corrcoef(data.T)

    @staticmethod
    def partial_correlation(x, y, control_vars):
        """åç›¸å…³ç³»æ•°"""
        from scipy import stats

        # è®¡ç®—æ®‹å·®
        def residual_regression(y, X):
            X = np.column_stack([np.ones(len(X)), X])
            coefficients = np.linalg.lstsq(X, y, rcond=None)[0]
            predicted = X @ coefficients
            return y - predicted

        # æ§åˆ¶å˜é‡çš„æ®‹å·®
        x_residual = residual_regression(x, control_vars)
        y_residual = residual_regression(y, control_vars)

        # è®¡ç®—æ®‹å·®çš„ç›¸å…³æ€§
        correlation, _ = stats.pearsonr(x_residual, y_residual)

        return correlation
```

## ğŸ¯ ä¼˜åŒ–ç®—æ³•

### 1. æ¢¯åº¦ä¸‹é™

```python
class GradientDescent:
    """æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•"""

    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.tolerance = tolerance

    def minimize(self, objective_function, gradient_function, initial_params):
        """æœ€å°åŒ–ç›®æ ‡å‡½æ•°"""
        params = np.array(initial_params, dtype=float)

        for iteration in range(self.max_iterations):
            # è®¡ç®—æ¢¯åº¦
            gradient = gradient_function(params)

            # æ›´æ–°å‚æ•°
            new_params = params - self.learning_rate * gradient

            # æ£€æŸ¥æ”¶æ•›
            if np.linalg.norm(new_params - params) < self.tolerance:
                break

            params = new_params

        return params, iteration

    def stochastic_minimize(self, objective_function, gradient_function,
                           initial_params, data, batch_size=32):
        """éšæœºæ¢¯åº¦ä¸‹é™"""
        params = np.array(initial_params, dtype=float)
        n_samples = len(data)

        for iteration in range(self.max_iterations):
            # éšæœºé‡‡æ ·
            batch_indices = np.random.choice(n_samples, batch_size, replace=False)
            batch_data = data[batch_indices]

            # è®¡ç®—æ‰¹é‡æ¢¯åº¦
            gradient = gradient_function(params, batch_data)

            # æ›´æ–°å‚æ•°
            params = params - self.learning_rate * gradient

            # å­¦ä¹ ç‡è¡°å‡
            self.learning_rate *= 0.999

        return params
```

### 2. é—ä¼ ç®—æ³•

```python
class GeneticAlgorithm:
    """é—ä¼ ç®—æ³•ä¼˜åŒ–"""

    def __init__(self, population_size=100, mutation_rate=0.01,
                 crossover_rate=0.8, elitism_rate=0.1):
        self.population_size = population_size
        self.mutation_rate = mutation_rate
        self.crossover_rate = crossover_rate
        self.elitism_rate = elitism_rate

    def optimize(self, fitness_function, chromosome_length, generations=100):
        """é—ä¼ ç®—æ³•ä¼˜åŒ–"""
        # åˆå§‹åŒ–ç§ç¾¤
        population = np.random.rand(self.population_size, chromosome_length)

        for generation in range(generations):
            # è®¡ç®—é€‚åº”åº¦
            fitness_scores = np.array([fitness_function(chromosome)
                                     for chromosome in population])

            # é€‰æ‹©
            selected_indices = self._selection(fitness_scores)
            selected_population = population[selected_indices]

            # äº¤å‰
            offspring = self._crossover(selected_population)

            # å˜å¼‚
            offspring = self._mutation(offspring)

            # ç²¾è‹±ä¿ç•™
            elite_size = int(self.population_size * self.elitism_rate)
            elite_indices = np.argsort(fitness_scores)[-elite_size:]
            elite = population[elite_indices]

            # æ–°ç§ç¾¤
            population = np.vstack([elite, offspring[:self.population_size - elite_size]])

        # è¿”å›æœ€ä¼˜è§£
        final_fitness = np.array([fitness_function(chromosome)
                                 for chromosome in population])
        best_index = np.argmax(final_fitness)

        return population[best_index], final_fitness[best_index]

    def _selection(self, fitness_scores):
        """è½®ç›˜èµŒé€‰æ‹©"""
        probabilities = fitness_scores / np.sum(fitness_scores)
        selected_indices = np.random.choice(
            len(fitness_scores),
            size=self.population_size,
            p=probabilities
        )
        return selected_indices

    def _crossover(self, population):
        """å•ç‚¹äº¤å‰"""
        offspring = []
        for i in range(0, len(population) - 1, 2):
            parent1, parent2 = population[i], population[i + 1]

            if np.random.rand() < self.crossover_rate:
                crossover_point = np.random.randint(1, len(parent1))
                child1 = np.concatenate([parent1[:crossover_point],
                                        parent2[crossover_point:]])
                child2 = np.concatenate([parent2[:crossover_point],
                                        parent1[crossover_point:]])
                offspring.extend([child1, child2])
            else:
                offspring.extend([parent1, parent2])

        return np.array(offspring)

    def _mutation(self, population):
        """éšæœºå˜å¼‚"""
        for chromosome in population:
            for i in range(len(chromosome)):
                if np.random.rand() < self.mutation_rate:
                    chromosome[i] = np.random.rand()
        return population
```

## ğŸ” è¯„ä¼°æŒ‡æ ‡ä½“ç³»

### 1. åˆ†ç±»æŒ‡æ ‡

```python
class ClassificationMetrics:
    """åˆ†ç±»è¯„ä¼°æŒ‡æ ‡"""

    @staticmethod
    def confusion_matrix(y_true, y_pred):
        """æ··æ·†çŸ©é˜µ"""
        classes = np.unique(np.concatenate([y_true, y_pred]))
        n_classes = len(classes)

        cm = np.zeros((n_classes, n_classes), dtype=int)

        for i, true_class in enumerate(classes):
            for j, pred_class in enumerate(classes):
                cm[i, j] = np.sum((y_true == true_class) & (y_pred == pred_class))

        return cm

    @staticmethod
    def precision_recall_f1(y_true, y_pred):
        """ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°"""
        cm = ClassificationMetrics.confusion_matrix(y_true, y_pred)

        precision = np.diag(cm) / np.sum(cm, axis=0)
        recall = np.diag(cm) / np.sum(cm, axis=1)
        f1 = 2 * (precision * recall) / (precision + recall)

        return precision, recall, f1

    @staticmethod
    def roc_auc_score(y_true, y_scores):
        """ROC-AUCåˆ†æ•°"""
        from sklearn.metrics import roc_auc_score
        return roc_auc_score(y_true, y_scores)
```

### 2. å›å½’æŒ‡æ ‡

```python
class RegressionMetrics:
    """å›å½’è¯„ä¼°æŒ‡æ ‡"""

    @staticmethod
    def mean_squared_error(y_true, y_pred):
        """å‡æ–¹è¯¯å·®"""
        return np.mean((y_true - y_pred) ** 2)

    @staticmethod
    def mean_absolute_error(y_true, y_pred):
        """å¹³å‡ç»å¯¹è¯¯å·®"""
        return np.mean(np.abs(y_true - y_pred))

    @staticmethod
    def r2_score(y_true, y_pred):
        """RÂ²å†³å®šç³»æ•°"""
        ss_res = np.sum((y_true - y_pred) ** 2)
        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
        return 1 - (ss_res / ss_tot)
```

### 3. æ¨èç³»ç»Ÿä¸“ç”¨æŒ‡æ ‡

```python
class RecommendationMetrics:
    """æ¨èç³»ç»Ÿä¸“ç”¨è¯„ä¼°æŒ‡æ ‡"""

    @staticmethod
    def hit_rate_at_k(recommended, relevant, k):
        """Hit Rate @ K"""
        recommended_k = recommended[:k]
        return int(any(item in relevant for item in recommended_k))

    @staticmethod
    def mean_reciprocal_rank(recommended, relevant):
        """å¹³å‡å€’æ•°æ’å"""
        for i, item in enumerate(recommended, 1):
            if item in relevant:
                return 1.0 / i
        return 0.0

    @staticmethod
    def coverage(all_items, recommended_items):
        """è¦†ç›–ç‡"""
        recommended_set = set(recommended_items)
        return len(recommended_set) / len(all_items)

    @staticmethod
    def serendipity(recommended, expected, item_similarity_matrix):
        """æ„å¤–æ€§/æ–°é¢–æ€§"""
        serendipity_scores = []

        for item in recommended:
            if item not in expected:
                # è®¡ç®—ä¸æœŸæœ›ç‰©å“çš„å¹³å‡ç›¸ä¼¼åº¦
                similarities = []
                for expected_item in expected:
                    if item in item_similarity_matrix and expected_item in item_similarity_matrix[item]:
                        similarities.append(item_similarity_matrix[item][expected_item])

                if similarities:
                    avg_similarity = np.mean(similarities)
                    serendipity_scores.append(1 - avg_similarity)

        return np.mean(serendipity_scores) if serendipity_scores else 0.0
```

è¿™å¥—çŸ¥è¯†ä½“ç³»ä¸ºæ¨èç³»ç»Ÿçš„å­¦ä¹ å’Œå®è·µæä¾›äº†åšå®çš„ç†è®ºåŸºç¡€ï¼Œæ¶µç›–äº†ä»åŸºç¡€æ¦‚å¿µåˆ°é«˜çº§ç®—æ³•çš„å®Œæ•´çŸ¥è¯†æ¡†æ¶ã€‚