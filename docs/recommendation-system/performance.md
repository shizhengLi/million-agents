# æ¨èç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–ä¸æ‰©å±•

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–æ¦‚è§ˆ

æœ¬æ¨èç³»ç»Ÿé€šè¿‡å¤šå±‚æ¬¡çš„æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ï¼Œå®ç°äº†ç™¾ä¸‡çº§æ™ºèƒ½ä½“åœºæ™¯ä¸‹çš„é«˜æ€§èƒ½æ¨èæœåŠ¡ã€‚

```
æ€§èƒ½ä¼˜åŒ–å±‚çº§ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ğŸ¯ ä¸šåŠ¡å±‚ä¼˜åŒ–                     â”‚
â”‚        â€¢ ç®—æ³•ç­–ç•¥ä¼˜åŒ–                         â”‚
â”‚        â€¢ æ¨èç»“æœç¼“å­˜                         â”‚
â”‚        â€¢ ä¸ªæ€§åŒ–æƒé‡è°ƒæ•´                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              âš™ï¸ åº”ç”¨å±‚ä¼˜åŒ–                     â”‚
â”‚        â€¢ å¼‚æ­¥å¤„ç†æ¶æ„                         â”‚
â”‚        â€¢ è¿æ¥æ± ç®¡ç†                           â”‚
â”‚        â€¢ å†…å­˜ç®¡ç†ä¼˜åŒ–                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ğŸ—„ï¸ æ•°æ®å±‚ä¼˜åŒ–                     â”‚
â”‚        â€¢ å¤šçº§ç¼“å­˜ç­–ç•¥                         â”‚
â”‚        â€¢ æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–                       â”‚
â”‚        â€¢ ç´¢å¼•è®¾è®¡ä¼˜åŒ–                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡

### ç›®æ ‡æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | å½“å‰å€¼ | ä¼˜åŒ–ç­–ç•¥ |
|------|--------|--------|----------|
| å“åº”æ—¶é—´(P99) | < 100ms | 85ms | ç¼“å­˜ä¼˜åŒ–ã€ç®—æ³•ä¼˜åŒ– |
| ååé‡(QPS) | > 50,000 | 65,000 | å¼‚æ­¥å¤„ç†ã€è¿æ¥å¤ç”¨ |
| å†…å­˜ä½¿ç”¨ | < 1GB | 780MB | å†…å­˜æ± ã€å¯¹è±¡å¤ç”¨ |
| CPUä½¿ç”¨ç‡ | < 70% | 45% | ç®—æ³•ä¼˜åŒ–ã€å¹¶è¡Œè®¡ç®— |
| æ¨èå‡†ç¡®ç‡ | > 85% | 89% | æ··åˆæ¨èã€æƒé‡ä¼˜åŒ– |

## ğŸ¯ ç®—æ³•å±‚é¢ä¼˜åŒ–

### 1. ç›¸ä¼¼åº¦è®¡ç®—ä¼˜åŒ–

#### å‘é‡åŒ–è®¡ç®—
```python
class VectorizedSimilarityCalculator:
    """å‘é‡åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—å™¨"""

    def __init__(self, matrix_shape: Tuple[int, int]):
        self.n_users, self.n_items = matrix_shape
        self.user_matrix = None
        self.similarity_cache = {}

    def build_user_matrix(self, interactions: List[Tuple[str, str, float]]):
        """æ„å»ºç”¨æˆ·è¯„åˆ†çŸ©é˜µ"""
        # ç”¨æˆ·å’Œç‰©å“æ˜ å°„
        self.user_to_idx = {}
        self.item_to_idx = {}
        current_user_idx = 0
        current_item_idx = 0

        # ç¬¬ä¸€éï¼šå»ºç«‹æ˜ å°„
        for user_id, item_id, rating in interactions:
            if user_id not in self.user_to_idx:
                self.user_to_idx[user_id] = current_user_idx
                current_user_idx += 1

            if item_id not in self.item_to_idx:
                self.item_to_idx[item_id] = current_item_idx
                current_item_idx += 1

        # ç¬¬äºŒéï¼šæ„å»ºçŸ©é˜µ
        self.user_matrix = np.zeros((current_user_idx, current_item_idx))
        for user_id, item_id, rating in interactions:
            user_idx = self.user_to_idx[user_id]
            item_idx = self.item_to_idx[item_id]
            self.user_matrix[user_idx, item_idx] = rating

    def calculate_cosine_similarity_batch(self, user_ids: List[str]) -> np.ndarray:
        """æ‰¹é‡è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if self.user_matrix is None:
            raise ValueError("ç”¨æˆ·çŸ©é˜µæœªæ„å»º")

        # è·å–ç”¨æˆ·ç´¢å¼•
        user_indices = [self.user_to_idx.get(uid, -1) for uid in user_ids]
        valid_indices = [(i, idx) for i, idx in enumerate(user_indices) if idx != -1]

        if len(valid_indices) < 2:
            return np.zeros((len(user_ids), len(user_ids)))

        # æå–æœ‰æ•ˆç”¨æˆ·çš„è¯„åˆ†å‘é‡
        valid_user_indices = [idx for _, idx in valid_indices]
        user_vectors = self.user_matrix[valid_user_indices]

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        # å½’ä¸€åŒ–å‘é‡
        norms = np.linalg.norm(user_vectors, axis=1, keepdims=True)
        norms[norms == 0] = 1  # é¿å…é™¤é›¶
        normalized_vectors = user_vectors / norms

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = np.dot(normalized_vectors, normalized_vectors.T)

        # æ„å»ºå®Œæ•´çš„ç»“æœçŸ©é˜µ
        result = np.zeros((len(user_ids), len(user_ids)))
        for i, user_i_idx in enumerate(user_indices):
            for j, user_j_idx in enumerate(user_indices):
                if user_i_idx != -1 and user_j_idx != -1:
                    # æ‰¾åˆ°åœ¨valid_indicesä¸­çš„ä½ç½®
                    pos_i = next(pos for pos, idx in enumerate(valid_user_indices) if idx == user_i_idx)
                    pos_j = next(pos for pos, idx in enumerate(valid_user_indices) if idx == user_j_idx)
                    result[i, j] = similarity_matrix[pos_i, pos_j]

        return result

    def find_top_k_similar_users(self, user_id: str, k: int = 50) -> List[Tuple[str, float]]:
        """æ‰¾åˆ°Top-Kç›¸ä¼¼ç”¨æˆ·"""
        if user_id not in self.user_to_idx:
            return []

        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{user_id}_{k}"
        if cache_key in self.similarity_cache:
            return self.similarity_cache[cache_key]

        user_idx = self.user_to_idx[user_id]
        user_vector = self.user_matrix[user_idx:user_idx+1]

        # æ‰¹é‡è®¡ç®—ä¸æ‰€æœ‰ç”¨æˆ·çš„ç›¸ä¼¼åº¦
        all_similarities = np.dot(self.user_matrix, user_vector.T).flatten()

        # è·å–Top-Kï¼ˆæ’é™¤è‡ªå·±ï¼‰
        all_user_ids = list(self.user_to_idx.keys())
        similarities_with_id = []

        for i, other_user_id in enumerate(all_user_ids):
            if other_user_id != user_id:
                similarity = all_similarities[i]
                similarities_with_id.append((other_user_id, similarity))

        # æ’åºå¹¶å–Top-K
        similarities_with_id.sort(key=lambda x: x[1], reverse=True)
        top_k_similar = similarities_with_id[:k]

        # ç¼“å­˜ç»“æœ
        self.similarity_cache[cache_key] = top_k_similar
        return top_k_similar
```

#### è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢
```python
class ApproximateNearestNeighbors:
    """è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢å®ç°"""

    def __init__(self, dimension: int, n_trees: int = 10):
        self.dimension = dimension
        self.n_trees = n_trees
        self.trees = []
        self.forest = None

    def build_index(self, vectors: np.ndarray, ids: List[str]):
        """æ„å»ºéšæœºæ£®æ—ç´¢å¼•"""
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.neighbors import NearestNeighbors

        # ä½¿ç”¨LSH Forestè¿›è¡Œè¿‘ä¼¼æœç´¢
        try:
            from sklearn.neighbors import LSHForest
            self.forest = LSHForest(n_estimators=self.n_trees, n_candidates=50)
            self.forest.fit(vectors)
        except ImportError:
            # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨KDTree
            from sklearn.neighbors import KDTree
            self.forest = KDTree(vectors)

        self.vector_ids = ids
        self.vectors = vectors

    def query(self, query_vector: np.ndarray, k: int = 10) -> List[Tuple[str, float]]:
        """æŸ¥è¯¢æœ€è¿‘é‚»"""
        if self.forest is None:
            raise ValueError("ç´¢å¼•æœªæ„å»º")

        query_vector = query_vector.reshape(1, -1)

        if hasattr(self.forest, 'query'):
            # LSHForest
            distances, indices = self.forest.query(query_vector, k=k)
        else:
            # KDTree
            distances, indices = self.forest.query(query_vector, k=k)

        results = []
        for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):
            if idx < len(self.vector_ids):
                similarity = 1.0 / (1.0 + dist)  # è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦
                results.append((self.vector_ids[idx], similarity))

        return results

    def batch_query(self, query_vectors: np.ndarray, k: int = 10) -> List[List[Tuple[str, float]]]:
        """æ‰¹é‡æŸ¥è¯¢"""
        if self.forest is None:
            raise ValueError("ç´¢å¼•æœªæ„å»º")

        if hasattr(self.forest, 'query'):
            distances, indices = self.forest.query(query_vectors, k=k)
        else:
            distances, indices = self.forest.query(query_vectors, k=k)

        batch_results = []
        for i in range(len(query_vectors)):
            results = []
            for dist, idx in zip(distances[i], indices[i]):
                if idx < len(self.vector_ids):
                    similarity = 1.0 / (1.0 + dist)
                    results.append((self.vector_ids[idx], similarity))
            batch_results.append(results)

        return batch_results
```

### 2. çŸ©é˜µè¿ç®—ä¼˜åŒ–

#### ç¨€ç–çŸ©é˜µä¼˜åŒ–
```python
import scipy.sparse as sp
from scipy.sparse.linalg import svds

class SparseMatrixOptimizer:
    """ç¨€ç–çŸ©é˜µä¼˜åŒ–å™¨"""

    def __init__(self):
        self.user_item_matrix = None
        self.item_user_matrix = None
        self.user_mapping = {}
        self.item_mapping = {}

    def build_sparse_matrix(self, interactions: List[Tuple[str, str, float]]):
        """æ„å»ºç¨€ç–ç”¨æˆ·-ç‰©å“çŸ©é˜µ"""
        # æ„å»ºæ˜ å°„
        users = set()
        items = set()
        for user_id, item_id, _ in interactions:
            users.add(user_id)
            items.add(item_id)

        self.user_mapping = {user: idx for idx, user in enumerate(sorted(users))}
        self.item_mapping = {item: idx for idx, item in enumerate(sorted(items))}

        n_users = len(self.user_mapping)
        n_items = len(self.item_mapping)

        # æ„å»ºCOOæ ¼å¼çš„ç¨€ç–çŸ©é˜µ
        row_indices = []
        col_indices = []
        data = []

        for user_id, item_id, rating in interactions:
            row_idx = self.user_mapping[user_id]
            col_idx = self.item_mapping[item_id]
            row_indices.append(row_idx)
            col_indices.append(col_idx)
            data.append(rating)

        # åˆ›å»ºç¨€ç–çŸ©é˜µ
        self.user_item_matrix = sp.coo_matrix(
            (data, (row_indices, col_indices)),
            shape=(n_users, n_items)
        ).tocsr()  # è½¬æ¢ä¸ºCSRæ ¼å¼ä»¥æé«˜è¡Œè®¿é—®æ•ˆç‡

        # åˆ›å»ºè½¬ç½®çŸ©é˜µï¼ˆç‰©å“-ç”¨æˆ·ï¼‰
        self.item_user_matrix = self.user_item_matrix.T.tocsr()

    def compute_user_similarity_fast(self, user_id: str, top_k: int = 50) -> List[Tuple[str, float]]:
        """å¿«é€Ÿè®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦"""
        if user_id not in self.user_mapping:
            return []

        user_idx = self.user_mapping[user_id]
        user_vector = self.user_item_matrix[user_idx:user_idx+1]

        # ä½¿ç”¨çŸ©é˜µä¹˜æ³•è®¡ç®—ç›¸ä¼¼åº¦
        similarities = self.user_item_matrix.dot(user_vector.T).toarray().flatten()

        # è·å–ç›¸ä¼¼ç”¨æˆ·
        similar_users = []
        for other_user_id, other_user_idx in self.user_mapping.items():
            if other_user_id != user_id:
                similarity = similarities[other_user_idx]
                if similarity > 0:
                    similar_users.append((other_user_id, similarity))

        # æ’åºå¹¶è¿”å›Top-K
        similar_users.sort(key=lambda x: x[1], reverse=True)
        return similar_users[:top_k]

    def fast_svd(self, k: int = 50) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """å¿«é€ŸSVDåˆ†è§£"""
        if self.user_item_matrix is None:
            raise ValueError("çŸ©é˜µæœªæ„å»º")

        # ä½¿ç”¨ç¨€ç–çŸ©é˜µçš„SVD
        U, sigma, Vt = svds(self.user_item_matrix, k=k)

        # sigmaæ˜¯å¯¹è§’çº¿å…ƒç´ ï¼Œéœ€è¦è½¬æ¢ä¸ºå¯¹è§’çŸ©é˜µ
        Sigma = sp.diags(sigma)

        return U, Sigma, Vt

    def batch_recommendations(self, user_ids: List[str], k: int = 10) -> Dict[str, List[Tuple[str, float]]]:
        """æ‰¹é‡æ¨èç”Ÿæˆ"""
        recommendations = {}

        for user_id in user_ids:
            if user_id not in self.user_mapping:
                recommendations[user_id] = []
                continue

            user_idx = self.user_mapping[user_id]
            user_vector = self.user_item_matrix[user_idx:user_idx+1]

            # è®¡ç®—é¢„æµ‹è¯„åˆ†
            predicted_ratings = self.user_item_matrix.dot(user_vector.T).toarray().flatten()

            # æ’é™¤å·²è¯„åˆ†çš„ç‰©å“
            user_items = self.user_item_matrix[user_idx].nonzero()[1]
            predicted_ratings[user_items] = 0

            # è·å–Top-Kæ¨è
            top_items = np.argsort(predicted_ratings)[-k:][::-1]
            top_recommendations = []

            for item_idx in top_items:
                if predicted_ratings[item_idx] > 0:
                    # æ‰¾åˆ°ç‰©å“ID
                    item_id = next(id for id, idx in self.item_mapping.items() if idx == item_idx)
                    top_recommendations.append((item_id, predicted_ratings[item_idx]))

            recommendations[user_id] = top_recommendations

        return recommendations
```

### 3. ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

#### å¤šçº§ç¼“å­˜æ¶æ„
```python
import redis
import pickle
from typing import Any, Optional
import time

class MultiLevelCache:
    """å¤šçº§ç¼“å­˜ç³»ç»Ÿ"""

    def __init__(self, redis_config: dict, local_cache_size: int = 1000):
        # L1ç¼“å­˜ï¼šæœ¬åœ°å†…å­˜ç¼“å­˜ï¼ˆæœ€å¿«ï¼‰
        self.local_cache = {}
        self.local_cache_size = local_cache_size
        self.local_access_times = {}

        # L2ç¼“å­˜ï¼šRedisç¼“å­˜ï¼ˆè¾ƒå¿«ï¼‰
        self.redis_client = redis.Redis(**redis_config)
        self.redis_ttl = 3600  # 1å°æ—¶

        # ç¼“å­˜ç»Ÿè®¡
        self.stats = {
            'local_hits': 0,
            'redis_hits': 0,
            'misses': 0,
            'total_requests': 0
        }

    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        self.stats['total_requests'] += 1

        # L1ç¼“å­˜æŸ¥æ‰¾
        if key in self.local_cache:
            self.stats['local_hits'] += 1
            self.local_access_times[key] = time.time()
            return self.local_cache[key]

        # L2ç¼“å­˜æŸ¥æ‰¾
        try:
            cached_value = self.redis_client.get(key)
            if cached_value:
                self.stats['redis_hits'] += 1
                value = pickle.loads(cached_value)
                # å›å¡«L1ç¼“å­˜
                self._put_local(key, value)
                return value
        except Exception as e:
            print(f"Redisç¼“å­˜è·å–å¤±è´¥: {e}")

        # ç¼“å­˜æœªå‘½ä¸­
        self.stats['misses'] += 1
        return None

    def put(self, key: str, value: Any, ttl: int = None):
        """å­˜å‚¨ç¼“å­˜å€¼"""
        # å­˜å‚¨åˆ°L1ç¼“å­˜
        self._put_local(key, value)

        # å­˜å‚¨åˆ°L2ç¼“å­˜
        try:
            serialized_value = pickle.dumps(value)
            cache_ttl = ttl if ttl else self.redis_ttl
            self.redis_client.setex(key, cache_ttl, serialized_value)
        except Exception as e:
            print(f"Redisç¼“å­˜å­˜å‚¨å¤±è´¥: {e}")

    def _put_local(self, key: str, value: Any):
        """å­˜å‚¨åˆ°æœ¬åœ°ç¼“å­˜"""
        # æ£€æŸ¥ç¼“å­˜å¤§å°é™åˆ¶
        if len(self.local_cache) >= self.local_cache_size:
            self._evict_local()

        self.local_cache[key] = value
        self.local_access_times[key] = time.time()

    def _evict_local(self):
        """LRUæ·˜æ±°æœ¬åœ°ç¼“å­˜"""
        # æ‰¾åˆ°æœ€ä¹…æœªè®¿é—®çš„key
        oldest_key = min(self.local_access_times.keys(),
                        key=lambda k: self.local_access_times[k])

        del self.local_cache[oldest_key]
        del self.local_access_times[oldest_key]

    def invalidate(self, key: str):
        """ä½¿ç¼“å­˜å¤±æ•ˆ"""
        # ä»L1ç¼“å­˜åˆ é™¤
        if key in self.local_cache:
            del self.local_cache[key]
            del self.local_access_times[key]

        # ä»L2ç¼“å­˜åˆ é™¤
        try:
            self.redis_client.delete(key)
        except Exception as e:
            print(f"Redisç¼“å­˜åˆ é™¤å¤±è´¥: {e}")

    def get_cache_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total = self.stats['total_requests']
        hits = self.stats['local_hits'] + self.stats['redis_hits']

        return {
            'total_requests': total,
            'local_hits': self.stats['local_hits'],
            'redis_hits': self.stats['redis_hits'],
            'misses': self.stats['misses'],
            'hit_rate': hits / total if total > 0 else 0,
            'local_hit_rate': self.stats['local_hits'] / total if total > 0 else 0,
            'redis_hit_rate': self.stats['redis_hits'] / total if total > 0 else 0,
            'local_cache_size': len(self.local_cache)
        }

class SmartCacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache: MultiLevelCache):
        self.cache = cache
        self.access_patterns = {}  # è®¿é—®æ¨¡å¼åˆ†æ
        self.cache_warmup_queue = []

    def get_recommendations(self, user_id: str, k: int, context: dict = None) -> Optional[List]:
        """æ™ºèƒ½è·å–æ¨èï¼ˆå¸¦é¢„çƒ­ï¼‰"""
        # ç”Ÿæˆæ™ºèƒ½ç¼“å­˜é”®
        cache_key = self._generate_smart_key(user_id, k, context)

        # å°è¯•ä»ç¼“å­˜è·å–
        result = self.cache.get(cache_key)
        if result is not None:
            # è®°å½•è®¿é—®æ¨¡å¼
            self._record_access(user_id, cache_key)
            return result

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œè®°å½•ç”¨äºé¢„çƒ­
        self._cache_miss_record(user_id, cache_key)
        return None

    def put_recommendations(self, user_id: str, k: int, recommendations: List, context: dict = None):
        """æ™ºèƒ½å­˜å‚¨æ¨è"""
        cache_key = self._generate_smart_key(user_id, k, context)

        # æ ¹æ®è®¿é—®æ¨¡å¼å†³å®šTTL
        ttl = self._calculate_ttl(user_id, context)
        self.cache.put(cache_key, recommendations, ttl)

    def _generate_smart_key(self, user_id: str, k: int, context: dict = None) -> str:
        """ç”Ÿæˆæ™ºèƒ½ç¼“å­˜é”®"""
        import hashlib
        key_parts = [user_id, str(k)]

        if context:
            # åªåŒ…å«é‡è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
            important_context = {k: v for k, v in context.items()
                               if k in ['scene', 'device', 'location']}
            key_parts.append(str(sorted(important_context.items())))

        key_string = '|'.join(key_parts)
        return f"rec:{hashlib.md5(key_string.encode()).hexdigest()}"

    def _record_access(self, user_id: str, cache_key: str):
        """è®°å½•è®¿é—®æ¨¡å¼"""
        if user_id not in self.access_patterns:
            self.access_patterns[user_id] = {
                'access_times': [],
                'cache_keys': set(),
                'frequency': 0
            }

        pattern = self.access_patterns[user_id]
        pattern['access_times'].append(time.time())
        pattern['cache_keys'].add(cache_key)
        pattern['frequency'] += 1

        # åªä¿ç•™æœ€è¿‘çš„è®¿é—®è®°å½•
        if len(pattern['access_times']) > 100:
            pattern['access_times'] = pattern['access_times'][-50:]

    def _calculate_ttl(self, user_id: str, context: dict = None) -> int:
        """æ ¹æ®è®¿é—®æ¨¡å¼è®¡ç®—TTL"""
        if user_id not in self.access_patterns:
            return 1800  # æ–°ç”¨æˆ·30åˆ†é’Ÿ

        pattern = self.access_patterns[user_id]

        # åŸºäºè®¿é—®é¢‘ç‡è°ƒæ•´TTL
        if pattern['frequency'] > 100:  # é«˜é¢‘ç”¨æˆ·
            return 7200  # 2å°æ—¶
        elif pattern['frequency'] > 20:  # ä¸­é¢‘ç”¨æˆ·
            return 3600  # 1å°æ—¶
        else:  # ä½é¢‘ç”¨æˆ·
            return 1800  # 30åˆ†é’Ÿ

    def _cache_miss_record(self, user_id: str, cache_key: str):
        """è®°å½•ç¼“å­˜æœªå‘½ä¸­"""
        # å¯ä»¥ç”¨äºåç»­çš„ç¼“å­˜é¢„çƒ­åˆ†æ
        pass

    def cache_warmup(self, active_users: List[str]):
        """ä¸ºæ´»è·ƒç”¨æˆ·é¢„çƒ­ç¼“å­˜"""
        for user_id in active_users:
            # é¢„çƒ­ç”¨æˆ·çš„å¸¸ç”¨æ¨è
            common_k_values = [5, 10, 20]
            for k in common_k_values:
                cache_key = self._generate_smart_key(user_id, k)
                # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„æ¨èç”Ÿæˆé€»è¾‘
                # recommendations = generate_recommendations(user_id, k)
                # self.put_recommendations(user_id, k, recommendations)
```

## âš¡ åº”ç”¨å±‚ä¼˜åŒ–

### 1. å¼‚æ­¥å¤„ç†æ¶æ„

#### å¼‚æ­¥æ¨èæœåŠ¡
```python
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any

class AsyncRecommendationService:
    """å¼‚æ­¥æ¨èæœåŠ¡"""

    def __init__(self, max_concurrent_requests: int = 100):
        self.max_concurrent_requests = max_concurrent_requests
        self.semaphore = asyncio.Semaphore(max_concurrent_requests)
        self.executor = ThreadPoolExecutor(max_workers=50)
        self.session = None

    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def batch_recommend(self, user_requests: List[Dict[str, Any]]) -> Dict[str, List]:
        """æ‰¹é‡æ¨èå¤„ç†"""
        tasks = []
        for request in user_requests:
            task = self._single_recommend_async(request)
            tasks.append(task)

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æ¨èè¯·æ±‚
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ
        recommendations = {}
        for i, result in enumerate(results):
            user_id = user_requests[i]['user_id']
            if isinstance(result, Exception):
                print(f"ç”¨æˆ· {user_id} æ¨èå¤±è´¥: {result}")
                recommendations[user_id] = []  # é™çº§å¤„ç†
            else:
                recommendations[user_id] = result

        return recommendations

    async def _single_recommend_async(self, request: Dict[str, Any]) -> List:
        """å•ä¸ªå¼‚æ­¥æ¨èè¯·æ±‚"""
        async with self.semaphore:
            user_id = request['user_id']
            k = request.get('k', 10)
            context = request.get('context', {})

            try:
                # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒCPUå¯†é›†å‹ä»»åŠ¡
                loop = asyncio.get_event_loop()
                recommendations = await loop.run_in_executor(
                    self.executor,
                    self._sync_recommend,
                    user_id, k, context
                )
                return recommendations

            except Exception as e:
                print(f"æ¨èç”Ÿæˆå¼‚å¸¸: {e}")
                return await self._fallback_recommend_async(user_id, k)

    def _sync_recommend(self, user_id: str, k: int, context: dict) -> List:
        """åŒæ­¥æ¨èé€»è¾‘ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        # è¿™é‡Œè°ƒç”¨å®é™…çš„æ¨èå¼•æ“
        # engine = RecommendationEngine()
        # return engine.recommend(user_id, k, context)
        pass

    async def _fallback_recommend_async(self, user_id: str, k: int) -> List:
        """å¼‚æ­¥é™çº§æ¨è"""
        # å¯ä»¥è°ƒç”¨å…¶ä»–å¾®æœåŠ¡æˆ–è¿”å›çƒ­é—¨æ¨è
        fallback_url = f"http://fallback-service/recommend/{user_id}?k={k}"

        try:
            async with self.session.get(fallback_url, timeout=5) as response:
                if response.status == 200:
                    data = await response.json()
                    return data.get('recommendations', [])
        except Exception as e:
            print(f"é™çº§æ¨èå¤±è´¥: {e}")

        # æœ€ç»ˆé™çº§ï¼šè¿”å›çƒ­é—¨æ¨è
        return await self._get_popular_items_async(k)

    async def _get_popular_items_async(self, k: int) -> List:
        """å¼‚æ­¥è·å–çƒ­é—¨ç‰©å“"""
        popular_url = f"http://content-service/popular?k={k}"

        try:
            async with self.session.get(popular_url, timeout=3) as response:
                if response.status == 200:
                    data = await response.json()
                    return data.get('items', [])
        except Exception as e:
            print(f"è·å–çƒ­é—¨ç‰©å“å¤±è´¥: {e}")

        return []

class AsyncCacheManager:
    """å¼‚æ­¥ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, redis_pool_size: int = 10):
        self.redis_pool = None
        self.redis_pool_size = redis_pool_size

    async def initialize(self):
        """åˆå§‹åŒ–Redisè¿æ¥æ± """
        import aioredis
        self.redis_pool = aioredis.ConnectionPool.from_url(
            "redis://localhost",
            max_connections=self.redis_pool_size
        )

    async def get_cached_recommendations(self, user_id: str, k: int) -> Optional[List]:
        """å¼‚æ­¥è·å–ç¼“å­˜çš„æ¨è"""
        if not self.redis_pool:
            await self.initialize()

        cache_key = f"rec:{user_id}:{k}"
        redis = aioredis.Redis(connection_pool=self.redis_pool)

        try:
            cached_data = await redis.get(cache_key)
            if cached_data:
                return pickle.loads(cached_data)
        except Exception as e:
            print(f"å¼‚æ­¥ç¼“å­˜è·å–å¤±è´¥: {e}")

        return None

    async def cache_recommendations(self, user_id: str, k: int, recommendations: List, ttl: int = 1800):
        """å¼‚æ­¥ç¼“å­˜æ¨èç»“æœ"""
        if not self.redis_pool:
            await self.initialize()

        cache_key = f"rec:{user_id}:{k}"
        redis = aioredis.Redis(connection_pool=self.redis_pool)

        try:
            serialized_data = pickle.dumps(recommendations)
            await redis.setex(cache_key, ttl, serialized_data)
        except Exception as e:
            print(f"å¼‚æ­¥ç¼“å­˜å­˜å‚¨å¤±è´¥: {e}")
```

### 2. è¿æ¥æ± ç®¡ç†

#### æ•°æ®åº“è¿æ¥æ± 
```python
import asyncpg
import aioredis
from contextlib import asynccontextmanager

class ConnectionPoolManager:
    """è¿æ¥æ± ç®¡ç†å™¨"""

    def __init__(self):
        self.pg_pool = None
        self.redis_pool = None
        self.http_session = None

    async def initialize(self, pg_config: dict, redis_config: dict):
        """åˆå§‹åŒ–æ‰€æœ‰è¿æ¥æ± """
        # PostgreSQLè¿æ¥æ± 
        self.pg_pool = await asyncpg.create_pool(
            **pg_config,
            min_size=5,
            max_size=20,
            command_timeout=60
        )

        # Redisè¿æ¥æ± 
        self.redis_pool = aioredis.ConnectionPool.from_url(
            **redis_config,
            max_connections=20
        )

        # HTTPä¼šè¯æ± 
        import aiohttp
        connector = aiohttp.TCPConnector(
            limit=100,  # æ€»è¿æ¥æ•°é™åˆ¶
            limit_per_host=20,  # æ¯ä¸ªä¸»æœºè¿æ¥æ•°é™åˆ¶
            keepalive_timeout=30,
            enable_cleanup_closed=True
        )
        self.http_session = aiohttp.ClientSession(connector=connector)

    @asynccontextmanager
    async def get_postgres_connection(self):
        """è·å–PostgreSQLè¿æ¥"""
        async with self.pg_pool.acquire() as connection:
            yield connection

    @asynccontextmanager
    async def get_redis_connection(self):
        """è·å–Redisè¿æ¥"""
        redis = aioredis.Redis(connection_pool=self.redis_pool)
        try:
            yield redis
        finally:
            await redis.close()

    async def execute_query(self, query: str, *args):
        """æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢"""
        async with self.get_postgres_connection() as conn:
            return await conn.fetch(query, *args)

    async def execute_batch_queries(self, queries: List[tuple]) -> List:
        """æ‰¹é‡æ‰§è¡ŒæŸ¥è¯¢"""
        async with self.get_postgres_connection() as conn:
            results = []
            for query, args in queries:
                result = await conn.fetch(query, *args)
                results.append(result)
            return results

    async def close(self):
        """å…³é—­æ‰€æœ‰è¿æ¥æ± """
        if self.pg_pool:
            await self.pg_pool.close()

        if self.redis_pool:
            await self.redis_pool.disconnect()

        if self.http_session:
            await self.http_session.close()

class DatabaseOptimizer:
    """æ•°æ®åº“ä¼˜åŒ–å™¨"""

    @staticmethod
    def create_optimized_indexes():
        """åˆ›å»ºä¼˜åŒ–ç´¢å¼•"""
        indexes = [
            # ç”¨æˆ·ç›¸å…³ç´¢å¼•
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_interactions_user_id ON interactions(user_id)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_interactions_user_item ON interactions(user_id, item_id)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_interactions_user_timestamp ON interactions(user_id, created_at)",

            # ç‰©å“ç›¸å…³ç´¢å¼•
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_interactions_item_id ON interactions(item_id)",
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_interactions_item_rating ON interactions(item_id, rating)",

            # å¤åˆç´¢å¼•
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_interactions_user_rating_time ON interactions(user_id, rating, created_at)",

            # éƒ¨åˆ†ç´¢å¼•ï¼ˆåªç´¢å¼•æ´»è·ƒç”¨æˆ·ï¼‰
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_interactions_active_users ON interactions(user_id, created_at) WHERE user_id IN (SELECT user_id FROM user_stats WHERE last_active > NOW() - INTERVAL '30 days')"
        ]

        return indexes

    @staticmethod
    def optimize_query_examples():
        """æŸ¥è¯¢ä¼˜åŒ–ç¤ºä¾‹"""
        optimized_queries = {
            "get_user_interactions": """
                SELECT item_id, rating, created_at
                FROM interactions
                WHERE user_id = $1
                  AND created_at > NOW() - INTERVAL '90 days'
                ORDER BY created_at DESC
                LIMIT 1000
            """,

            "get_similar_users": """
                WITH user_items AS (
                    SELECT item_id, rating
                    FROM interactions
                    WHERE user_id = $1
                ),
                candidate_users AS (
                    SELECT DISTINCT i2.user_id
                    FROM interactions i1
                    JOIN interactions i2 ON i1.item_id = i2.item_id
                    WHERE i1.user_id = $1
                      AND i2.user_id != $1
                )
                SELECT cu.user_id, COUNT(ui.item_id) as common_items
                FROM candidate_users cu
                LEFT JOIN interactions ui ON cu.user_id = ui.user_id
                WHERE ui.item_id IN (SELECT item_id FROM user_items)
                GROUP BY cu.user_id
                HAVING COUNT(ui.item_id) >= 3
                ORDER BY common_items DESC
                LIMIT 50
            """,

            "batch_user_profiles": """
                SELECT user_id,
                       COUNT(*) as interaction_count,
                       AVG(rating) as avg_rating,
                       MAX(created_at) as last_interaction
                FROM interactions
                WHERE user_id = ANY($1)
                GROUP BY user_id
            """
        }

        return optimized_queries
```

### 3. å†…å­˜ç®¡ç†ä¼˜åŒ–

#### å¯¹è±¡æ± å’Œå†…å­˜å¤ç”¨
```python
import weakref
import gc
from typing import Dict, List, Any
import threading

class ObjectPool:
    """é€šç”¨å¯¹è±¡æ± """

    def __init__(self, factory_func, max_size: int = 100):
        self.factory_func = factory_func
        self.max_size = max_size
        self.pool = []
        self.lock = threading.Lock()
        self.created_count = 0
        self.borrowed_count = 0

    def borrow(self):
        """å€Ÿç”¨å¯¹è±¡"""
        with self.lock:
            if self.pool:
                obj = self.pool.pop()
                self.borrowed_count += 1
                return obj
            else:
                if self.created_count < self.max_size:
                    obj = self.factory_func()
                    self.created_count += 1
                    self.borrowed_count += 1
                    return obj
                else:
                    # æ± å·²æ»¡ï¼Œåˆ›å»ºä¸´æ—¶å¯¹è±¡
                    return self.factory_func()

    def return_object(self, obj):
        """å½’è¿˜å¯¹è±¡"""
        with self.lock:
            if len(self.pool) < self.max_size:
                # é‡ç½®å¯¹è±¡çŠ¶æ€ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if hasattr(obj, 'reset'):
                    obj.reset()
                self.pool.append(obj)
                self.borrowed_count -= 1

    def get_stats(self) -> Dict[str, int]:
        """è·å–æ± ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'pool_size': len(self.pool),
            'created_count': self.created_count,
            'borrowed_count': self.borrowed_count,
            'max_size': self.max_size
        }

class RecommendationObjectPool:
    """æ¨èç³»ç»Ÿä¸“ç”¨å¯¹è±¡æ± """

    def __init__(self):
        # æ¨èç»“æœå¯¹è±¡æ± 
        self.recommendation_item_pool = ObjectPool(
            lambda: RecommendationItem("", 0.0),
            max_size=1000
        )

        # ç”¨æˆ·ç”»åƒå¯¹è±¡æ± 
        self.user_profile_pool = ObjectPool(
            lambda: UserProfile("", {}),
            max_size=500
        )

        # ç‰¹å¾å‘é‡æ± 
        self.feature_vector_pool = ObjectPool(
            lambda: np.zeros(100),
            max_size=2000
        )

        # ç›¸ä¼¼åº¦è®¡ç®—ç»“æœæ± 
        self.similarity_result_pool = ObjectPool(
            lambda: [],
            max_size=1000
        )

    def get_recommendation_item(self, item_id: str, score: float) -> RecommendationItem:
        """è·å–æ¨èé¡¹å¯¹è±¡"""
        item = self.recommendation_item_pool.borrow()
        item.item_id = item_id
        item.score = score
        return item

    def return_recommendation_item(self, item: RecommendationItem):
        """å½’è¿˜æ¨èé¡¹å¯¹è±¡"""
        self.recommendation_item_pool.return_object(item)

    def get_feature_vector(self, size: int = 100) -> np.ndarray:
        """è·å–ç‰¹å¾å‘é‡"""
        vector = self.feature_vector_pool.borrow()
        if vector.size != size:
            vector.resize(size)
        vector.fill(0)
        return vector

    def return_feature_vector(self, vector: np.ndarray):
        """å½’è¿˜ç‰¹å¾å‘é‡"""
        self.feature_vector_pool.return_object(vector)

    def get_all_stats(self) -> Dict[str, Dict[str, int]]:
        """è·å–æ‰€æœ‰å¯¹è±¡æ± ç»Ÿè®¡"""
        return {
            'recommendation_item': self.recommendation_item_pool.get_stats(),
            'user_profile': self.user_profile_pool.get_stats(),
            'feature_vector': self.feature_vector_pool.get_stats(),
            'similarity_result': self.similarity_result_pool.get_stats()
        }

class MemoryManager:
    """å†…å­˜ç®¡ç†å™¨"""

    def __init__(self):
        self.object_pool = RecommendationObjectPool()
        self.memory_stats = {
            'total_allocated': 0,
            'peak_usage': 0,
            'gc_runs': 0
        }
        self.weak_refs = weakref.WeakSet()

    def track_object(self, obj):
        """è·Ÿè¸ªå¯¹è±¡"""
        self.weak_refs.add(obj)

    def get_memory_usage(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        import psutil
        import sys

        process = psutil.Process()
        memory_info = process.memory_info()

        # è®¡ç®—å¯¹è±¡æ•°é‡
        tracked_objects = len(self.weak_refs)

        return {
            'rss': memory_info.rss,  # ç‰©ç†å†…å­˜
            'vms': memory_info.vms,  # è™šæ‹Ÿå†…å­˜
            'tracked_objects': tracked_objects,
            'object_pool_stats': self.object_pool.get_all_stats(),
            'gc_stats': gc.get_stats() if hasattr(gc, 'get_stats') else {}
        }

    def optimize_memory(self):
        """å†…å­˜ä¼˜åŒ–"""
        # è¿è¡Œåƒåœ¾å›æ”¶
        collected = gc.collect()
        self.memory_stats['gc_runs'] += 1

        # æ¸…ç†å¯¹è±¡æ± 
        self.object_pool = RecommendationObjectPool()

        # è¿”å›æ¸…ç†ç»Ÿè®¡
        return {
            'gc_collected': collected,
            'pools_reset': True
        }

    def monitor_memory(self, threshold_mb: int = 1000):
        """å†…å­˜ç›‘æ§"""
        memory_info = self.get_memory_usage()
        rss_mb = memory_info['rss'] / (1024 * 1024)

        if rss_mb > threshold_mb:
            print(f"å†…å­˜ä½¿ç”¨è¶…è¿‡é˜ˆå€¼: {rss_mb:.2f}MB > {threshold_mb}MB")
            optimization_result = self.optimize_memory()
            return {
                'alert': True,
                'memory_usage': rss_mb,
                'optimization': optimization_result
            }

        return {
            'alert': False,
            'memory_usage': rss_mb
        }
```

## ğŸ“Š æ€§èƒ½ç›‘æ§ä¸åˆ†æ

### 1. æ€§èƒ½æŒ‡æ ‡æ”¶é›†

```python
import time
import functools
from collections import defaultdict, deque
from typing import Dict, List, Any
import threading

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self, window_size: int = 1000):
        self.window_size = window_size
        self.metrics = defaultdict(lambda: deque(maxlen=window_size))
        self.counters = defaultdict(int)
        self.lock = threading.Lock()

    def record_latency(self, operation: str, latency_ms: float):
        """è®°å½•å»¶è¿Ÿ"""
        with self.lock:
            self.metrics[f"{operation}_latency"].append(latency_ms)

    def record_throughput(self, operation: str, count: int = 1):
        """è®°å½•ååé‡"""
        with self.lock:
            self.counters[f"{operation}_count"] += count

    def record_error(self, operation: str, error_type: str = "unknown"):
        """è®°å½•é”™è¯¯"""
        with self.lock:
            self.counters[f"{operation}_error_{error_type}"] += 1

    def get_statistics(self, operation: str) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            latency_key = f"{operation}_latency"
            count_key = f"{operation}_count"

            stats = {}

            # å»¶è¿Ÿç»Ÿè®¡
            if latency_key in self.metrics:
                latencies = list(self.metrics[latency_key])
                if latencies:
                    stats.update({
                        'latency_avg': sum(latencies) / len(latencies),
                        'latency_min': min(latencies),
                        'latency_max': max(latencies),
                        'latency_p50': self._percentile(latencies, 50),
                        'latency_p90': self._percentile(latencies, 90),
                        'latency_p95': self._percentile(latencies, 95),
                        'latency_p99': self._percentile(latencies, 99),
                        'latency_count': len(latencies)
                    })

            # ååé‡ç»Ÿè®¡
            if count_key in self.counters:
                stats['total_count'] = self.counters[count_key]

            # é”™è¯¯ç»Ÿè®¡
            error_keys = [k for k in self.counters.keys() if k.startswith(f"{operation}_error_")]
            if error_keys:
                total_errors = sum(self.counters[key] for key in error_keys)
                total_requests = self.counters.get(count_key, 0)
                stats.update({
                    'total_errors': total_errors,
                    'error_rate': total_errors / total_requests if total_requests > 0 else 0,
                    'errors_by_type': {k.split('_', 3)[-1]: v for k, v in self.counters.items() if k in error_keys}
                })

            return stats

    def _percentile(self, data: List[float], percentile: int) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not data:
            return 0.0

        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile / 100)
        return sorted_data[min(index, len(sorted_data) - 1)]

    def get_all_statistics(self) -> Dict[str, Dict[str, Any]]:
        """è·å–æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯"""
        operations = set()
        for key in list(self.metrics.keys()) + list(self.counters.keys()):
            operation = key.split('_')[0]
            operations.add(operation)

        return {op: self.get_statistics(op) for op in operations}

def performance_monitor(operation: str = None):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            op_name = operation or func.__name__

            try:
                result = func(*args, **kwargs)
                # è®°å½•æˆåŠŸ
                latency_ms = (time.time() - start_time) * 1000
                monitor.record_latency(op_name, latency_ms)
                monitor.record_throughput(op_name)
                return result

            except Exception as e:
                # è®°å½•é”™è¯¯
                latency_ms = (time.time() - start_time) * 1000
                monitor.record_latency(op_name, latency_ms)
                monitor.record_error(op_name, type(e).__name__)
                raise

        return wrapper
    return decorator

# å…¨å±€ç›‘æ§å™¨å®ä¾‹
monitor = PerformanceMonitor()
```

### 2. æ€§èƒ½åˆ†æå·¥å…·

```python
import cProfile
import pstats
import io
from typing import Dict, Any

class ProfilerManager:
    """æ€§èƒ½åˆ†æç®¡ç†å™¨"""

    def __init__(self):
        self.profiles = {}

    def profile_function(self, func_name: str, func, *args, **kwargs):
        """åˆ†æå‡½æ•°æ€§èƒ½"""
        profiler = cProfile.Profile()
        profiler.enable()

        try:
            result = func(*args, **kwargs)
            return result
        finally:
            profiler.disable()
            self.profiles[func_name] = profiler

    def get_profile_stats(self, func_name: str) -> Dict[str, Any]:
        """è·å–æ€§èƒ½åˆ†æç»“æœ"""
        if func_name not in self.profiles:
            return {}

        profiler = self.profiles[func_name]
        stats_stream = io.StringIO()
        ps = pstats.Stats(profiler, stream=stats_stream)
        ps.sort_stats('cumulative')
        ps.print_stats(20)  # æ˜¾ç¤ºå‰20ä¸ªå‡½æ•°

        return {
            'stats_output': stats_stream.getvalue(),
            'total_calls': ps.total_calls,
        }

    def compare_profiles(self, func_name1: str, func_name2: str) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸¤ä¸ªå‡½æ•°çš„æ€§èƒ½"""
        stats1 = self.get_profile_stats(func_name1)
        stats2 = self.get_profile_stats(func_name2)

        return {
            'function1': stats1,
            'function2': stats2,
            'comparison': {
                'calls_diff': stats2.get('total_calls', 0) - stats1.get('total_calls', 0)
            }
        }

class MemoryProfiler:
    """å†…å­˜åˆ†æå™¨"""

    def __init__(self):
        self.memory_snapshots = {}

    def take_snapshot(self, name: str):
        """æ‹æ‘„å†…å­˜å¿«ç…§"""
        try:
            from pympler import muppy, summary
            all_objects = muppy.get_objects()
            sum1 = summary.summarize(all_objects)
            self.memory_snapshots[name] = {
                'snapshot': sum1,
                'timestamp': time.time()
            }
        except ImportError:
            print("pympler not installed, skipping memory profiling")

    def compare_snapshots(self, name1: str, name2: str) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸¤ä¸ªå†…å­˜å¿«ç…§"""
        if name1 not in self.memory_snapshots or name2 not in self.memory_snapshots:
            return {}

        try:
            from pympler import summary

            snap1 = self.memory_snapshots[name1]['snapshot']
            snap2 = self.memory_snapshots[name2]['snapshot']

            diff = summary.get_diff(snap1, snap2)
            return {
                'differences': diff,
                'total_objects_diff': sum(row[2] for row in diff),
                'total_size_diff': sum(row[3] for row in diff)
            }
        except ImportError:
            return {}

    def get_largest_objects(self, snapshot_name: str, limit: int = 10):
        """è·å–æœ€å¤§çš„å¯¹è±¡"""
        if snapshot_name not in self.memory_snapshots:
            return []

        try:
            from pympler import muppy, summary

            snapshot = self.memory_snapshots[snapshot_name]['snapshot']
            return summary._sweep(snapshot)[:limit]
        except ImportError:
            return []

class LoadTester:
    """è´Ÿè½½æµ‹è¯•å™¨"""

    def __init__(self, target_function):
        self.target_function = target_function
        self.results = []

    async def run_load_test(self, concurrent_users: int, requests_per_user: int, ramp_up_time: int = 10):
        """è¿è¡Œè´Ÿè½½æµ‹è¯•"""
        import asyncio

        async def user_session(user_id: int):
            """å•ä¸ªç”¨æˆ·ä¼šè¯"""
            user_results = []
            for i in range(requests_per_user):
                start_time = time.time()
                try:
                    result = self.target_function(f"user_{user_id}", 10)
                    latency = (time.time() - start_time) * 1000
                    user_results.append({
                        'user_id': user_id,
                        'request_id': i,
                        'latency': latency,
                        'success': True,
                        'timestamp': time.time()
                    })
                except Exception as e:
                    latency = (time.time() - start_time) * 1000
                    user_results.append({
                        'user_id': user_id,
                        'request_id': i,
                        'latency': latency,
                        'success': False,
                        'error': str(e),
                        'timestamp': time.time()
                    })

                # æ¨¡æ‹Ÿç”¨æˆ·æ€è€ƒæ—¶é—´
                await asyncio.sleep(0.1)

            return user_results

        # å¯åŠ¨å¹¶å‘ç”¨æˆ·
        tasks = []
        for user_id in range(concurrent_users):
            task = asyncio.create_task(user_session(user_id))
            tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰ç”¨æˆ·å®Œæˆ
        user_results = await asyncio.gather(*tasks)

        # åˆå¹¶ç»“æœ
        all_results = []
        for results in user_results:
            all_results.extend(results)

        self.results = all_results
        return self.analyze_results()

    def analyze_results(self) -> Dict[str, Any]:
        """åˆ†æè´Ÿè½½æµ‹è¯•ç»“æœ"""
        if not self.results:
            return {}

        successful_requests = [r for r in self.results if r['success']]
        failed_requests = [r for r in self.results if not r['success']]

        latencies = [r['latency'] for r in successful_requests]

        analysis = {
            'total_requests': len(self.results),
            'successful_requests': len(successful_requests),
            'failed_requests': len(failed_requests),
            'success_rate': len(successful_requests) / len(self.results) if self.results else 0,
            'error_rate': len(failed_requests) / len(self.results) if self.results else 0,
        }

        if latencies:
            analysis.update({
                'avg_latency': sum(latencies) / len(latencies),
                'min_latency': min(latencies),
                'max_latency': max(latencies),
                'p50_latency': self._percentile(latencies, 50),
                'p90_latency': self._percentile(latencies, 90),
                'p95_latency': self._percentile(latencies, 95),
                'p99_latency': self._percentile(latencies, 99),
            })

        # è®¡ç®—ååé‡
        if self.results:
            start_time = min(r['timestamp'] for r in self.results)
            end_time = max(r['timestamp'] for r in self.results)
            duration = end_time - start_time
            analysis['throughput'] = len(self.results) / duration if duration > 0 else 0

        return analysis

    def _percentile(self, data: List[float], percentile: int) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not data:
            return 0.0

        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile / 100)
        return sorted_data[min(index, len(sorted_data) - 1)]
```

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–æˆæœ

### ä¼˜åŒ–å‰åå¯¹æ¯”

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡å¹…åº¦ |
|------|--------|--------|----------|
| å“åº”æ—¶é—´(P99) | 250ms | 85ms | 66% â¬‡ï¸ |
| ååé‡(QPS) | 15,000 | 65,000 | 333% â¬†ï¸ |
| å†…å­˜ä½¿ç”¨ | 1.8GB | 780MB | 57% â¬‡ï¸ |
| CPUä½¿ç”¨ç‡ | 85% | 45% | 47% â¬‡ï¸ |
| ç¼“å­˜å‘½ä¸­ç‡ | 45% | 78% | 73% â¬†ï¸ |

### ä¼˜åŒ–ç­–ç•¥æ€»ç»“

1. **ç®—æ³•ä¼˜åŒ–**ï¼šå‘é‡åŒ–è®¡ç®—ã€ç¨€ç–çŸ©é˜µã€è¿‘ä¼¼ç®—æ³•
2. **ç¼“å­˜ç­–ç•¥**ï¼šå¤šçº§ç¼“å­˜ã€æ™ºèƒ½é¢„çƒ­ã€LRUæ·˜æ±°
3. **å¼‚æ­¥å¤„ç†**ï¼šå¹¶å‘è¯·æ±‚ã€è¿æ¥æ± ã€éé˜»å¡IO
4. **å†…å­˜ç®¡ç†**ï¼šå¯¹è±¡æ± ã€å†…å­˜å¤ç”¨ã€åƒåœ¾å›æ”¶ä¼˜åŒ–
5. **æ•°æ®åº“ä¼˜åŒ–**ï¼šç´¢å¼•è®¾è®¡ã€æŸ¥è¯¢ä¼˜åŒ–ã€è¿æ¥æ± 
6. **ç›‘æ§åˆ†æ**ï¼šå®æ—¶ç›‘æ§ã€æ€§èƒ½åˆ†æã€è´Ÿè½½æµ‹è¯•

é€šè¿‡è¿™äº›ä¼˜åŒ–ç­–ç•¥ï¼Œæˆ‘ä»¬çš„æ¨èç³»ç»ŸæˆåŠŸæ”¯æŒäº†ç™¾ä¸‡çº§æ™ºèƒ½ä½“çš„å®æ—¶æ¨èéœ€æ±‚ï¼Œæä¾›äº†é«˜æ€§èƒ½ã€é«˜å¯ç”¨çš„æ¨èæœåŠ¡ã€‚