# ç¤¾äº¤ç½‘ç»œç®—æ³•é¢è¯•é¢˜åº“ä¸è¯¦è§£

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æ”¶é›†äº†ç¤¾äº¤ç½‘ç»œç®—æ³•ç›¸å…³çš„å¸¸è§é¢è¯•é¢˜ï¼ŒåŒ…æ‹¬åŸºç¡€æ¦‚å¿µã€ç®—æ³•åŸç†ã€ç¼–ç¨‹å®ç°ã€ç³»ç»Ÿè®¾è®¡ç­‰å¤šä¸ªå±‚é¢ã€‚æ¯é“é¢˜éƒ½é…æœ‰è¯¦ç»†çš„ç­”æ¡ˆå’Œè§£æï¼Œå¸®åŠ©å‡†å¤‡æŠ€æœ¯é¢è¯•ã€‚

## ğŸ”¥ åŸºç¡€æ¦‚å¿µç±»

### Q1: ä»€ä¹ˆæ˜¯ç¤¾äº¤ç½‘ç»œåˆ†æï¼Ÿå®ƒæœ‰å“ªäº›ä¸»è¦åº”ç”¨åœºæ™¯ï¼Ÿ

**ç­”æ¡ˆ:**
ç¤¾äº¤ç½‘ç»œåˆ†æï¼ˆSocial Network Analysis, SNAï¼‰æ˜¯ç ”ç©¶ç¤¾ä¼šå®ä½“ä¹‹é—´å…³ç³»å’Œæ¨¡å¼çš„æ–¹æ³•è®ºã€‚å®ƒå°†ç¤¾ä¼šå®ä½“ï¼ˆå¦‚äººã€ç»„ç»‡ã€å›½å®¶ï¼‰è¡¨ç¤ºä¸ºèŠ‚ç‚¹ï¼Œå°†å®ƒä»¬ä¹‹é—´çš„å…³ç³»è¡¨ç¤ºä¸ºè¾¹ï¼Œå½¢æˆç½‘ç»œç»“æ„è¿›è¡Œåˆ†æã€‚

**ä¸»è¦åº”ç”¨åœºæ™¯:**
1. **ç¤¾äº¤å¹³å°**: å¥½å‹æ¨èã€å†…å®¹æ¨èã€å½±å“åŠ›åˆ†æ
2. **å•†ä¸šé¢†åŸŸ**: å®¢æˆ·å…³ç³»ç®¡ç†ã€å¸‚åœºè¥é”€ã€ä¾›åº”é“¾åˆ†æ
3. **ä¿¡æ¯ä¼ æ’­**: ç—…æ¯’ä¼ æ’­æ¨¡å‹ã€èˆ†æƒ…åˆ†æã€ä¿¡æ¯æ‰©æ•£
4. **ç»„ç»‡ç®¡ç†**: å›¢é˜Ÿåä½œåˆ†æã€çŸ¥è¯†ç®¡ç†ã€ç»„ç»‡ç»“æ„ä¼˜åŒ–
5. **å®‰å…¨é¢†åŸŸ**: ææ€–ç½‘ç»œè¯†åˆ«ã€é‡‘èæ¬ºè¯ˆæ£€æµ‹ã€ç½‘ç»œå®‰å…¨

### Q2: è§£é‡Šåº¦ä¸­å¿ƒæ€§ï¼ˆDegree Centralityï¼‰ã€æ¥è¿‘ä¸­å¿ƒæ€§ï¼ˆCloseness Centralityï¼‰å’Œä»‹æ•°ä¸­å¿ƒæ€§ï¼ˆBetweenness Centralityï¼‰çš„åŒºåˆ«ã€‚

**ç­”æ¡ˆ:**
è¿™ä¸‰ç§æ˜¯è¡¡é‡èŠ‚ç‚¹é‡è¦æ€§çš„ä¸åŒæŒ‡æ ‡ï¼š

1. **åº¦ä¸­å¿ƒæ€§**:
   - å®šä¹‰: èŠ‚ç‚¹çš„åº¦æ•°ï¼ˆè¿æ¥æ•°ï¼‰
   - å…¬å¼: `CD(v) = deg(v)`
   - æ„ä¹‰: ç›´æ¥è¿æ¥æ•°é‡ï¼Œè¡¡é‡å±€éƒ¨å½±å“åŠ›
   - åº”ç”¨: è¯†åˆ«æ´»è·ƒç”¨æˆ·ã€æµè¡Œåº¦æ’å

2. **æ¥è¿‘ä¸­å¿ƒæ€§**:
   - å®šä¹‰: èŠ‚ç‚¹åˆ°æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹çš„å¹³å‡è·ç¦»çš„å€’æ•°
   - å…¬å¼: `CC(v) = (n-1) / Î£ d(u,v)`
   - æ„ä¹‰: ä¿¡æ¯ä¼ æ’­æ•ˆç‡ï¼Œè¡¡é‡å…¨å±€å¯è¾¾æ€§
   - åº”ç”¨: è¯†åˆ«ä¿¡æ¯ä¼ æ’­ä¸­å¿ƒã€æœ€ä¼˜ä¿¡æ¯å‘å¸ƒè€…

3. **ä»‹æ•°ä¸­å¿ƒæ€§**:
   - å®šä¹‰: èŠ‚ç‚¹åœ¨æœ€çŸ­è·¯å¾„ä¸­å‡ºç°çš„é¢‘ç‡
   - å…¬å¼: `CB(v) = Î£ Ïƒst(v) / Ïƒst`
   - æ„ä¹‰: æ§åˆ¶ä¿¡æ¯æµåŠ¨çš„èƒ½åŠ›ï¼Œè¡¡é‡æ¡¥æ¢ä½œç”¨
   - åº”ç”¨: è¯†åˆ«å…³é”®è¿æ¥è€…ã€ç½‘ç»œç“¶é¢ˆç‚¹

### Q3: ä»€ä¹ˆæ˜¯ç¤¾åŒºå‘ç°ï¼Ÿä¸ºä»€ä¹ˆå®ƒåœ¨ç¤¾äº¤ç½‘ç»œä¸­å¾ˆé‡è¦ï¼Ÿ

**ç­”æ¡ˆ:**
ç¤¾åŒºå‘ç°ï¼ˆCommunity Detectionï¼‰æ˜¯è¯†åˆ«ç½‘ç»œä¸­èŠ‚ç‚¹èšç±»çš„è¿‡ç¨‹ï¼Œä½¿å¾—èšç±»å†…éƒ¨çš„è¿æ¥å¯†åº¦æ˜¾è‘—é«˜äºèšç±»ä¹‹é—´çš„è¿æ¥å¯†åº¦ã€‚

**é‡è¦æ€§:**
1. **ç†è§£ç½‘ç»œç»“æ„**: æ­ç¤ºç½‘ç»œçš„ç»„ç»‡æ¨¡å¼å’Œå±‚æ¬¡ç»“æ„
2. **æ¨èç³»ç»Ÿ**: åŸºäºç¤¾åŒºçš„æ¨èæ›´åŠ ç²¾å‡†
3. **è¥é”€ç­–ç•¥**: é’ˆå¯¹ä¸åŒç¤¾åŒºåˆ¶å®šå·®å¼‚åŒ–ç­–ç•¥
4. **å¼‚å¸¸æ£€æµ‹**: è¯†åˆ«ä¸ç¬¦åˆç¤¾åŒºæ¨¡å¼çš„å¼‚å¸¸è¡Œä¸º
5. **å½±å“åŠ›ä¼ æ’­**: è¯†åˆ«ç¤¾åŒºå†…çš„å…³é”®å½±å“è€…

## ğŸ§® ç®—æ³•åŸç†ç±»

### Q4: è¯¦ç»†è§£é‡ŠPageRankç®—æ³•çš„åŸç†å’Œå®ç°æ­¥éª¤ã€‚

**ç­”æ¡ˆ:**
PageRankæ˜¯Googleç”¨æ¥è¡¡é‡ç½‘é¡µé‡è¦æ€§çš„ç®—æ³•ï¼ŒåŸºäºéšæœºæ¸¸èµ°æ¨¡å‹ã€‚

**æ ¸å¿ƒåŸç†:**
1. **éšæœºæ¸¸èµ°å‡è®¾**: ç”¨æˆ·éšæœºç‚¹å‡»é“¾æ¥ï¼Œæœ‰æ¦‚ç‡éšæœºè·³è½¬åˆ°ä»»æ„é¡µé¢
2. **é‡è¦æ€§ä¼ æ’­**: é‡è¦é¡µé¢é“¾æ¥çš„é¡µé¢ä¹Ÿé‡è¦
3. **æ”¶æ•›æ€§**: ç»è¿‡è¶³å¤Ÿå¤šæ¬¡è¿­ä»£ï¼ŒPageRankå€¼ä¼šæ”¶æ•›

**æ•°å­¦å…¬å¼:**
```
PR(p) = (1-d)/n + d * Î£(PR(i)/C(i))
```

å…¶ä¸­:
- PR(p): é¡µé¢pçš„PageRankå€¼
- d: é˜»å°¼å› å­ï¼ˆé€šå¸¸0.85ï¼‰
- n: æ€»é¡µé¢æ•°
- PR(i): é“¾æ¥åˆ°pçš„é¡µé¢içš„PageRankå€¼
- C(i): é¡µé¢içš„å‡ºé“¾æ•°é‡

**å®ç°æ­¥éª¤:**
1. **åˆå§‹åŒ–**: æ‰€æœ‰é¡µé¢PageRankå€¼è®¾ä¸º1/n
2. **æ„å»ºè½¬ç§»çŸ©é˜µ**: æ ¹æ®é“¾æ¥å…³ç³»æ„å»ºæ¦‚ç‡è½¬ç§»çŸ©é˜µ
3. **è¿­ä»£è®¡ç®—**: ä½¿ç”¨çŸ©é˜µä¹˜æ³•è¿­ä»£æ›´æ–°PageRankå€¼
4. **æ”¶æ•›åˆ¤æ–­**: å½“å˜åŒ–å°äºé˜ˆå€¼æ—¶åœæ­¢

**ä»£ç å®ç°:**
```python
def calculate_pagerank(self, graph, damping_factor=0.85, max_iterations=100, tolerance=1e-6):
    n = len(graph.nodes)
    if n == 0:
        return {}

    # æ„å»ºè½¬ç§»çŸ©é˜µ
    transition_matrix = self._build_transition_matrix(graph)

    # åˆå§‹åŒ–PageRankå‘é‡
    pagerank = np.ones(n) / n

    # è¿­ä»£è®¡ç®—
    for iteration in range(max_iterations):
        old_pagerank = pagerank.copy()

        # PageRankè¿­ä»£å…¬å¼
        pagerank = damping_factor * transition_matrix @ pagerank + (1 - damping_factor) / n

        # æ£€æŸ¥æ”¶æ•›
        if np.linalg.norm(pagerank - old_pagerank, 1) < tolerance:
            break

    return dict(zip(graph.nodes, pagerank))
```

### Q5: Louvainç¤¾åŒºå‘ç°ç®—æ³•çš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿæœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿ

**ç­”æ¡ˆ:**
Louvainç®—æ³•æ˜¯åŸºäºæ¨¡å—åº¦ä¼˜åŒ–çš„è´ªå¿ƒç®—æ³•ï¼Œç”¨äºå‘ç°ç½‘ç»œä¸­çš„ç¤¾åŒºç»“æ„ã€‚

**å·¥ä½œåŸç†:**
1. **åˆå§‹åŒ–**: æ¯ä¸ªèŠ‚ç‚¹ä½œä¸ºç‹¬ç«‹ç¤¾åŒº
2. **å±€éƒ¨ç§»åŠ¨é˜¶æ®µ**:
   - éå†æ¯ä¸ªèŠ‚ç‚¹ï¼Œå°è¯•å°†å…¶ç§»åŠ¨åˆ°ç›¸é‚»ç¤¾åŒº
   - è®¡ç®—ç§»åŠ¨åçš„æ¨¡å—åº¦å¢ç›Š
   - é€‰æ‹©ä½¿æ¨¡å—åº¦å¢ç›Šæœ€å¤§çš„ç§»åŠ¨
   - é‡å¤ç›´åˆ°æ— æ³•æ”¹è¿›
3. **ç¤¾åŒºèšåˆé˜¶æ®µ**:
   - å°†æ¯ä¸ªç¤¾åŒºèšåˆä¸ºè¶…èŠ‚ç‚¹
   - ç¤¾åŒºé—´çš„è¾¹æƒé‡ä¸ºåŸè¾¹æƒé‡ä¹‹å’Œ
   - æ„å»ºæ–°çš„ç½‘ç»œ
4. **é‡å¤è¿­ä»£**: åœ¨æ–°ç½‘ç»œä¸Šé‡å¤ä¸Šè¿°è¿‡ç¨‹

**æ¨¡å—åº¦è®¡ç®—:**
```
Î”Q = [Î£_in + k_i,in / (2m) - (Î£_tot + k_i)Â² / (4mÂ²)]
      - [Î£_in / (2m) - Î£_totÂ² / (4mÂ²) - k_i,outÂ² / (4mÂ²)]
```

**ä¼˜ç‚¹:**
- æ—¶é—´å¤æ‚åº¦ä½: O(n log n)
- å¯æ‰©å±•æ€§å¥½: é€‚åˆå¤§è§„æ¨¡ç½‘ç»œ
- ç»“æœè´¨é‡é«˜: é€šå¸¸èƒ½äº§ç”Ÿè¾ƒå¥½çš„ç¤¾åŒºåˆ’åˆ†
- å®ç°ç®€å•: ç®—æ³•é€»è¾‘æ¸…æ™°

**ç¼ºç‚¹:**
- åˆ†è¾¨ç‡é™åˆ¶: å¯èƒ½æ— æ³•å‘ç°å°ç¤¾åŒº
- å±€éƒ¨æœ€ä¼˜: å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜è§£
- ç¡®å®šæ€§å·®: ä¸åŒè¿è¡Œå¯èƒ½äº§ç”Ÿä¸åŒç»“æœ
- å±‚æ¬¡ç»“æ„: ä¸ä¿ç•™ç¤¾åŒºå‘ç°çš„å±‚æ¬¡ä¿¡æ¯

### Q6: Dijkstraç®—æ³•å’ŒBFSç®—æ³•åœ¨å¯»æ‰¾æœ€çŸ­è·¯å¾„æ—¶æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿåˆ†åˆ«é€‚ç”¨äºä»€ä¹ˆåœºæ™¯ï¼Ÿ

**ç­”æ¡ˆ:**

**Dijkstraç®—æ³•:**
- **é€‚ç”¨åœºæ™¯**: å¸¦éè´Ÿæƒé‡çš„å›¾
- **æ—¶é—´å¤æ‚åº¦**: O(E + V log V)ï¼ˆä½¿ç”¨ä¼˜å…ˆé˜Ÿåˆ—ï¼‰
- **ç©ºé—´å¤æ‚åº¦**: O(V)
- **åŸç†**: è´ªå¿ƒç®—æ³•ï¼Œæ¯æ¬¡é€‰æ‹©è·ç¦»èµ·ç‚¹æœ€è¿‘çš„æœªè®¿é—®èŠ‚ç‚¹
- **ç‰¹ç‚¹**: ä¿è¯æ‰¾åˆ°æœ€çŸ­è·¯å¾„ï¼Œæ”¯æŒæƒé‡

**BFSç®—æ³•:**
- **é€‚ç”¨åœºæ™¯**: æ— æƒé‡å›¾æˆ–æ‰€æœ‰è¾¹æƒé‡ç›¸ç­‰çš„å›¾
- **æ—¶é—´å¤æ‚åº¦**: O(V + E)
- **ç©ºé—´å¤æ‚åº¦**: O(V)
- **åŸç†**: å±‚æ¬¡éå†ï¼Œé€å±‚æ‰©å±•
- **ç‰¹ç‚¹**: ç®€å•é«˜æ•ˆï¼Œå¤©ç„¶æ‰¾åˆ°æœ€çŸ­è·¯å¾„

**åœºæ™¯é€‰æ‹©:**
- **ç¤¾äº¤ç½‘ç»œå…³ç³»åˆ†æ**: ä½¿ç”¨BFSï¼ˆå…³ç³»å¼ºåº¦ç›¸åŒï¼‰
- **äº¤é€šç½‘ç»œå¯¼èˆª**: ä½¿ç”¨Dijkstraï¼ˆè·ç¦»/æ—¶é—´æƒé‡ï¼‰
- **è®¡ç®—æœºç½‘ç»œè·¯ç”±**: ä½¿ç”¨Dijkstraï¼ˆå¸¦å®½/å»¶è¿Ÿæƒé‡ï¼‰
- **æ¸¸æˆå¯»è·¯**: æ ¹æ®æ˜¯å¦è€ƒè™‘åœ°å½¢æˆæœ¬é€‰æ‹©

**ä»£ç å¯¹æ¯”:**
```python
# Dijkstraç®—æ³•
def dijkstra(self, graph, start, end):
    distances = {node: float('inf') for node in graph.nodes}
    distances[start] = 0
    visited = set()
    pq = [(0, start)]

    while pq:
        current_distance, current = heapq.heappop(pq)

        if current in visited:
            continue

        visited.add(current)

        for neighbor, weight in graph.neighbors(current):
            distance = current_distance + weight
            if distance < distances[neighbor]:
                distances[neighbor] = distance
                heapq.heappush(pq, (distance, neighbor))

    return distances.get(end, float('inf'))

# BFSç®—æ³•
def bfs(self, graph, start, end):
    queue = [(start, [start])]
    visited = {start}

    while queue:
        current, path = queue.pop(0)

        if current == end:
            return path

        for neighbor in graph.neighbors(current):
            if neighbor not in visited:
                visited.add(neighbor)
                queue.append((neighbor, path + [neighbor]))

    return None
```

## ğŸ’» ç¼–ç¨‹å®ç°ç±»

### Q7: å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œæ£€æµ‹å›¾ä¸­çš„æ¡¥ï¼ˆBridgeï¼‰ã€‚

**ç­”æ¡ˆ:**
æ¡¥æ˜¯å›¾ä¸­çš„è¾¹ï¼Œåˆ é™¤åä¼šå¢åŠ è¿é€šåˆ†é‡çš„æ•°é‡ã€‚

```python
def find_bridges(self, graph: SocialNetworkGraph) -> List[Tuple[int, int]]:
    """
    ä½¿ç”¨Tarjanç®—æ³•æ‰¾å›¾ä¸­çš„æ¡¥

    æ—¶é—´å¤æ‚åº¦: O(V + E)
    ç©ºé—´å¤æ‚åº¦: O(V)
    """
    def dfs(u, parent, time, disc, low, visited, bridges):
        visited[u] = True
        disc[u] = low[u] = time
        time += 1

        for v in graph.get_agent_friends(u):
            if not visited[v]:
                dfs(v, u, time, disc, low, visited, bridges)

                # æ›´æ–°lowå€¼
                low[u] = min(low[u], low[v])

                # æ£€æŸ¥æ˜¯å¦ä¸ºæ¡¥
                if low[v] > disc[u]:
                    bridges.append((min(u, v), max(u, v)))

            elif v != parent:  # æ›´æ–°lowå€¼ï¼Œå¿½ç•¥å›è¾¹
                low[u] = min(low[u], disc[v])

    n = graph.get_agent_count()
    if n == 0:
        return []

    disc = [0] * (max(graph.agents.keys()) + 1)
    low = [0] * (max(graph.agents.keys()) + 1)
    visited = [False] * (max(graph.agents.keys()) + 1)
    bridges = []
    time = 1

    # å¤„ç†æ‰€æœ‰è¿é€šåˆ†é‡
    for node in graph.agents:
        if not visited[node]:
            dfs(node, -1, time, disc, low, visited, bridges)

    return bridges
```

### Q8: å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œè®¡ç®—ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„æ‰€æœ‰æœ€çŸ­è·¯å¾„ã€‚

**ç­”æ¡ˆ:**
```python
def find_all_shortest_paths(self, graph: SocialNetworkGraph,
                           start: int, end: int) -> List[List[int]]:
    """
    æ‰¾åˆ°ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„æ‰€æœ‰æœ€çŸ­è·¯å¾„

    ä½¿ç”¨æ”¹è¿›çš„BFSç®—æ³•ï¼Œè®°å½•æ‰€æœ‰æœ€çŸ­è·¯å¾„
    """
    if start == end:
        return [[start]]

    if not graph.has_agent(start) or not graph.has_agent(end):
        return []

    from collections import defaultdict, deque

    # BFSæ‰¾åˆ°æœ€çŸ­è·ç¦»
    queue = deque([start])
    distances = {start: 0}
    parents = defaultdict(set)

    while queue:
        current = queue.popleft()

        for neighbor in graph.get_agent_friends(current):
            if neighbor not in distances:
                distances[neighbor] = distances[current] + 1
                parents[neighbor].add(current)
                queue.append(neighbor)
            elif distances[neighbor] == distances[current] + 1:
                # æ‰¾åˆ°å¦ä¸€æ¡æœ€çŸ­è·¯å¾„
                parents[neighbor].add(current)

    if end not in distances:
        return []

    # å›æº¯æ„å»ºæ‰€æœ‰è·¯å¾„
    def build_paths(node, path, paths):
        if node == start:
            paths.append(path[::-1])
            return

        for parent in parents[node]:
            build_paths(parent, [parent] + path, paths)

    all_paths = []
    build_paths(end, [end], all_paths)

    return all_paths
```

### Q9: å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œæ£€æµ‹å›¾ä¸­çš„å¼ºè¿é€šåˆ†é‡ã€‚

**ç­”æ¡ˆ:**
```python
def find_strongly_connected_components(self, directed_graph) -> List[Set[int]]:
    """
    ä½¿ç”¨Kosarajuç®—æ³•æ‰¾å¼ºè¿é€šåˆ†é‡

    æ—¶é—´å¤æ‚åº¦: O(V + E)
    """
    def dfs_first_pass(u, visited, stack):
        visited.add(u)
        for v in directed_graph.get_neighbors(u):
            if v not in visited:
                dfs_first_pass(v, visited, stack)
        stack.append(u)

    def dfs_second_pass(u, visited, component):
        visited.add(u)
        component.add(u)
        for v in directed_graph.get_reverse_neighbors(u):
            if v not in visited:
                dfs_second_pass(v, visited, component)

    # ç¬¬ä¸€æ¬¡DFSï¼ŒæŒ‰å®Œæˆæ—¶é—´æ’åº
    visited = set()
    stack = []

    for node in directed_graph.get_all_nodes():
        if node not in visited:
            dfs_first_pass(node, visited, stack)

    # ç¬¬äºŒæ¬¡DFSï¼Œåœ¨åå‘å›¾ä¸ŠæŒ‰é€†åºå¤„ç†
    visited.clear()
    sccs = []

    while stack:
        node = stack.pop()
        if node not in visited:
            component = set()
            dfs_second_pass(node, visited, component)
            sccs.append(component)

    return sccs
```

## ğŸ—ï¸ ç³»ç»Ÿè®¾è®¡ç±»

### Q10: è®¾è®¡ä¸€ä¸ªå¥½å‹æ¨èç³»ç»Ÿï¼Œä½ ä¼šè€ƒè™‘å“ªäº›å› ç´ ï¼Ÿå¦‚ä½•å®ç°ï¼Ÿ

**ç­”æ¡ˆ:**
å¥½å‹æ¨èç³»ç»Ÿåº”è¯¥è€ƒè™‘å¤šç§å› ç´ æ¥æé«˜æ¨èè´¨é‡ï¼š

**æ ¸å¿ƒå› ç´ :**
1. **å…±åŒå¥½å‹æ•°é‡**: æœ€ç›´æ¥çš„æ¨èä¾æ®
2. **ç¤¾äº¤è·ç¦»**: äºŒåº¦äººè„‰ã€ä¸‰åº¦äººè„‰
3. **å…´è¶£ç›¸ä¼¼åº¦**: åŸºäºç”¨æˆ·ç”»åƒå’Œå…´è¶£æ ‡ç­¾
4. **åœ°ç†ä½ç½®**: é™„è¿‘ç”¨æˆ·æ¨è
5. **æ´»è·ƒåº¦**: æ¨èæ´»è·ƒç”¨æˆ·å¢åŠ äº’åŠ¨æ¦‚ç‡
6. **äº’åŠ¨å†å²**: ä¹‹å‰çš„ç‚¹èµã€è¯„è®ºç­‰äº’åŠ¨

**å®ç°æ¶æ„:**
```python
class FriendRecommendationSystem:
    def __init__(self, graph: SocialNetworkGraph, user_profiles: Dict):
        self.graph = graph
        self.profiles = user_profiles
        self.weights = {
            'common_friends': 0.4,
            'social_distance': 0.2,
            'interest_similarity': 0.2,
            'location': 0.1,
            'activity': 0.1
        }

    def recommend_friends(self, user_id: int, top_k: int = 10) -> List[Tuple[int, float]]:
        """ä¸ºç”¨æˆ·æ¨èå¥½å‹"""
        if not self.graph.has_agent(user_id):
            return []

        candidates = {}
        current_friends = set(self.graph.get_agent_friends(user_id))

        # 1. åŸºäºå…±åŒå¥½å‹
        common_friends_score = self._score_by_common_friends(user_id, current_friends)

        # 2. åŸºäºç¤¾äº¤è·ç¦»
        distance_score = self._score_by_social_distance(user_id, current_friends)

        # 3. åŸºäºå…´è¶£ç›¸ä¼¼åº¦
        interest_score = self._score_by_interest_similarity(user_id)

        # 4. åŸºäºåœ°ç†ä½ç½®
        location_score = self._score_by_location(user_id)

        # 5. åŸºäºæ´»è·ƒåº¦
        activity_score = self._score_by_activity()

        # ç»¼åˆè¯„åˆ†
        for candidate in set(common_friends_score.keys()) | \
                        set(distance_score.keys()) | \
                        set(interest_score.keys()):

            if candidate not in current_friends and candidate != user_id:
                total_score = (
                    self.weights['common_friends'] * common_friends_score.get(candidate, 0) +
                    self.weights['social_distance'] * distance_score.get(candidate, 0) +
                    self.weights['interest_similarity'] * interest_score.get(candidate, 0) +
                    self.weights['location'] * location_score.get(candidate, 0) +
                    self.weights['activity'] * activity_score.get(candidate, 0)
                )
                candidates[candidate] = total_score

        # æ’åºå¹¶è¿”å›top-k
        sorted_candidates = sorted(candidates.items(), key=lambda x: x[1], reverse=True)
        return sorted_candidates[:top_k]

    def _score_by_common_friends(self, user_id: int, current_friends: Set[int]) -> Dict[int, float]:
        """åŸºäºå…±åŒå¥½å‹çš„è¯„åˆ†"""
        scores = {}

        for friend in current_friends:
            for friend_of_friend in self.graph.get_agent_friends(friend):
                if friend_of_friend not in current_friends and friend_of_friend != user_id:
                    common_friends = len(current_friends & set(self.graph.get_agent_friends(friend_of_friend)))
                    scores[friend_of_friend] = scores.get(friend_of_friend, 0) + common_friends

        # å½’ä¸€åŒ–
        if scores:
            max_score = max(scores.values())
            scores = {k: v/max_score for k, v in scores.items()}

        return scores

    def _score_by_social_distance(self, user_id: int, current_friends: Set[int]) -> Dict[int, float]:
        """åŸºäºç¤¾äº¤è·ç¦»çš„è¯„åˆ†"""
        scores = {}
        calculator = ShortestPathCalculator()

        for other_user in self.graph.agents:
            if other_user != user_id and other_user not in current_friends:
                path = calculator.calculate_shortest_path(self.graph, user_id, other_user)
                if path:
                    distance = len(path) - 1
                    # è·ç¦»è¶Šè¿‘ï¼Œåˆ†æ•°è¶Šé«˜
                    scores[other_user] = 1.0 / distance if distance > 0 else 1.0

        return scores
```

### Q11: å¦‚ä½•è®¾è®¡ä¸€ä¸ªå®æ—¶çš„ç¤¾äº¤ç½‘ç»œåˆ†æç³»ç»Ÿï¼Ÿ

**ç­”æ¡ˆ:**
å®æ—¶ç¤¾äº¤ç½‘ç»œåˆ†æç³»ç»Ÿéœ€è¦å¤„ç†å¤§è§„æ¨¡æ•°æ®æµå¹¶æä¾›å®æ—¶åˆ†æç»“æœï¼š

**ç³»ç»Ÿæ¶æ„:**
```
æ•°æ®é‡‡é›†å±‚ â†’ æ¶ˆæ¯é˜Ÿåˆ— â†’ æµå¤„ç†å¼•æ“ â†’ åˆ†æå¼•æ“ â†’ å­˜å‚¨å±‚ â†’ APIå±‚
```

**æ ¸å¿ƒç»„ä»¶:**

1. **æ•°æ®é‡‡é›†å±‚**:
   - ç”¨æˆ·è¡Œä¸ºæ—¥å¿—ï¼ˆç‚¹èµã€è¯„è®ºã€åˆ†äº«ï¼‰
   - å…³ç³»å˜åŒ–ï¼ˆåŠ å¥½å‹ã€å–æ¶ˆå…³æ³¨ï¼‰
   - å†…å®¹ç”Ÿæˆï¼ˆå‘å¸–ã€å›¾ç‰‡ã€è§†é¢‘ï¼‰

2. **æ¶ˆæ¯é˜Ÿåˆ—**:
   - Kafka/RabbitMQå¤„ç†é«˜å¹¶å‘æ¶ˆæ¯
   - æ¶ˆæ¯åˆ†åŒºå’Œè´Ÿè½½å‡è¡¡
   - æŒä¹…åŒ–å’Œé‡è¯•æœºåˆ¶

3. **æµå¤„ç†å¼•æ“**:
   - Apache Flink/Spark Streaming
   - å®æ—¶è®¡ç®—ç”¨æˆ·æ´»è·ƒåº¦
   - æ£€æµ‹å¼‚å¸¸è¡Œä¸ºå’Œçƒ­ç‚¹äº‹ä»¶

4. **åˆ†æå¼•æ“**:
   - å›¾æ•°æ®åº“ï¼ˆNeo4j, JanusGraphï¼‰
   - å®æ—¶PageRankè®¡ç®—
   - ç¤¾åŒºå‘ç°å’Œä¼ æ’­åˆ†æ

5. **å­˜å‚¨å±‚**:
   - æ—¶åºæ•°æ®åº“ï¼ˆInfluxDBï¼‰å­˜å‚¨æŒ‡æ ‡
   - ç¼“å­˜å±‚ï¼ˆRedisï¼‰æä¾›å¿«é€ŸæŸ¥è¯¢
   - æ•°æ®ä»“åº“ï¼ˆHiveï¼‰ç”¨äºç¦»çº¿åˆ†æ

**å®ç°ç¤ºä¾‹:**
```python
class RealTimeAnalyzer:
    def __init__(self):
        self.graph = SocialNetworkGraph()
        self.redis_client = redis.Redis()
        self.message_queue = KafkaConsumer(['social_events'])

    def process_events(self):
        """å¤„ç†å®æ—¶äº‹ä»¶æµ"""
        for event in self.message_queue:
            event_type = event['type']

            if event_type == 'friendship':
                self._handle_friendship_event(event)
            elif event_type == 'interaction':
                self._handle_interaction_event(event)
            elif event_type == 'content':
                self._handle_content_event(event)

    def _handle_friendship_event(self, event):
        """å¤„ç†å¥½å‹å…³ç³»äº‹ä»¶"""
        user_id = event['user_id']
        friend_id = event['friend_id']
        action = event['action']  # 'add' or 'remove'

        if action == 'add':
            self.graph.add_friendship(user_id, friend_id)
            # æ›´æ–°å®æ—¶æŒ‡æ ‡
            self._update_friendship_metrics(user_id, friend_id)
        else:
            self.graph.remove_friendship(user_id, friend_id)

    def get_real_time_metrics(self, user_id: int) -> Dict[str, any]:
        """è·å–ç”¨æˆ·çš„å®æ—¶æŒ‡æ ‡"""
        # ä»ç¼“å­˜è·å–
        cached_metrics = self.redis_client.hgetall(f"user_metrics:{user_id}")

        if cached_metrics:
            return {
                'friend_count': int(cached_metrics.get('friend_count', 0)),
                'interaction_count': int(cached_metrics.get('interaction_count', 0)),
                'influence_score': float(cached_metrics.get('influence_score', 0)),
                'last_updated': cached_metrics.get('last_updated')
            }

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œå®æ—¶è®¡ç®—
        return self._calculate_real_time_metrics(user_id)
```

### Q12: å¦‚ä½•å¤„ç†ç™¾ä¸‡çº§èŠ‚ç‚¹çš„ç¤¾äº¤ç½‘ç»œå›¾è®¡ç®—ï¼Ÿ

**ç­”æ¡ˆ:**
å¤„ç†å¤§è§„æ¨¡å›¾è®¡ç®—éœ€è¦ç»“åˆåˆ†å¸ƒå¼è®¡ç®—å’Œç®—æ³•ä¼˜åŒ–ï¼š

**ç­–ç•¥1: åˆ†å¸ƒå¼å›¾è®¡ç®—**
```python
# ä½¿ç”¨GraphXï¼ˆSparkï¼‰è¿›è¡Œåˆ†å¸ƒå¼PageRankè®¡ç®—
from pyspark import SparkContext
from graphframes import GraphFrame

def distributed_pagerank(vertices_df, edges_df):
    """åˆ†å¸ƒå¼PageRankè®¡ç®—"""
    sc = SparkContext()

    # åˆ›å»ºGraphFrame
    graph = GraphFrame(vertices_df, edges_df)

    # è¿è¡ŒPageRankç®—æ³•
    results = graph.pageRank(resetProbability=0.15, maxIter=10)

    return results.vertices
```

**ç­–ç•¥2: é‡‡æ ·å’Œè¿‘ä¼¼ç®—æ³•**
```python
class ScalableAnalyzer:
    def __init__(self, sample_ratio=0.1):
        self.sample_ratio = sample_ratio

    def analyze_large_graph(self, graph: SocialNetworkGraph):
        """åˆ†æå¤§è§„æ¨¡å›¾"""
        if graph.get_agent_count() > 100000:
            # ä½¿ç”¨é‡‡æ ·
            sampled_graph = self._sample_graph(graph)
            return self._analyze_sampled(sampled_graph, graph)
        else:
            return self._analyze_full(graph)

    def _sample_graph(self, graph):
        """å›¾é‡‡æ ·"""
        # åŸºäºåº¦æ•°çš„é‡‡æ ·
        high_degree_nodes = self._get_high_degree_nodes(graph)
        random_nodes = self._get_random_nodes(graph)

        sampled_nodes = high_degree_nodes | random_nodes
        return self._extract_subgraph(graph, sampled_nodes)
```

**ç­–ç•¥3: å¢é‡è®¡ç®—**
```python
class IncrementalAnalyzer:
    def __init__(self):
        self.cached_results = {}
        self.last_update = {}

    def incremental_pagerank(self, graph, changed_nodes):
        """å¢é‡PageRankè®¡ç®—"""
        if not changed_nodes:
            return self.cached_results.get('pagerank', {})

        # åªé‡æ–°è®¡ç®—å—å½±å“çš„éƒ¨åˆ†
        affected_nodes = self._get_affected_nodes(changed_nodes)

        # å±€éƒ¨æ›´æ–°
        local_pagerank = self._compute_local_pagerank(graph, affected_nodes)

        # æ›´æ–°ç¼“å­˜
        self._update_cache(local_pagerank)

        return self.cached_results['pagerank']
```

## ğŸ¯ é«˜çº§ç®—æ³•ç±»

### Q13: å¦‚ä½•æ£€æµ‹ç¤¾äº¤ç½‘ç»œä¸­çš„å¼‚å¸¸è¡Œä¸ºï¼Ÿ

**ç­”æ¡ˆ:**
å¼‚å¸¸è¡Œä¸ºæ£€æµ‹æ˜¯ç¤¾äº¤ç½‘ç»œå®‰å…¨çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼š

**å¼‚å¸¸ç±»å‹:**
1. **è¿æ¥å¼‚å¸¸**: çŸ­æ—¶é—´å†…å¤§é‡æ·»åŠ /åˆ é™¤å¥½å‹
2. **å†…å®¹å¼‚å¸¸**: å‘å¸ƒå¼‚å¸¸å†…å®¹ã€åƒåœ¾ä¿¡æ¯
3. **è¡Œä¸ºå¼‚å¸¸**: å¼‚å¸¸æ´»è·ƒæ¨¡å¼ã€æœºå™¨äººè¡Œä¸º
4. **ç½‘ç»œå¼‚å¸¸**: å¼‚å¸¸çš„è¿æ¥æ¨¡å¼

**æ£€æµ‹æ–¹æ³•:**
```python
class AnomalyDetector:
    def __init__(self, graph: SocialNetworkGraph):
        self.graph = graph
        self.baseline_metrics = self._establish_baseline()

    def detect_connection_anomalies(self, user_id: int,
                                  recent_connections: List[int]) -> Dict[str, any]:
        """æ£€æµ‹è¿æ¥å¼‚å¸¸"""
        current_degree = self.graph.get_agent_degree(user_id)
        historical_avg = self.baseline_metrics[user_id]['avg_degree']

        anomalies = {}

        # å¼‚å¸¸1: è¿æ¥æ•°é‡æ¿€å¢
        if len(recent_connections) > historical_avg * 5:
            anomalies['sudden_growth'] = {
                'severity': 'high',
                'current': len(recent_connections),
                'baseline': historical_avg
            }

        # å¼‚å¸¸2: è¿æ¥æ¨¡å¼å¼‚å¸¸
        new_neighbors = set(recent_connections)
        existing_friends = set(self.graph.get_agent_friends(user_id))

        # æ£€æŸ¥æ–°è¿æ¥çš„ç¤¾åŒºåˆ†å¸ƒ
        community_diversity = self._calculate_community_diversity(new_neighbors)
        if community_diversity < 0.1:  # æ–°è¿æ¥éƒ½åœ¨åŒä¸€ç¤¾åŒº
            anomalies['community_concentration'] = {
                'severity': 'medium',
                'diversity': community_diversity
            }

        return anomalies

    def detect_behavior_anomalies(self, user_id: int,
                                activity_log: List[Dict]) -> Dict[str, any]:
        """æ£€æµ‹è¡Œä¸ºå¼‚å¸¸"""
        anomalies = {}

        # æ—¶é—´æ¨¡å¼å¼‚å¸¸
        activity_times = [log['timestamp'] for log in activity_log]
        time_pattern = self._analyze_time_pattern(activity_times)

        if self._is_bot_like_pattern(time_pattern):
            anomalies['bot_behavior'] = {
                'severity': 'high',
                'pattern': 'regular_intervals'
            }

        # å†…å®¹å¼‚å¸¸
        content_types = [log['content_type'] for log in activity_log]
        content_similarity = self._calculate_content_similarity(content_types)

        if content_similarity > 0.9:  # å†…å®¹é«˜åº¦ç›¸ä¼¼
            anomalies['content_repetition'] = {
                'severity': 'medium',
                'similarity': content_similarity
            }

        return anomalies
```

### Q14: å®ç°ä¸€ä¸ªä¸ªæ€§åŒ–PageRankç®—æ³•

**ç­”æ¡ˆ:**
ä¸ªæ€§åŒ–PageRankè€ƒè™‘ç”¨æˆ·çš„ä¸ªäººåå¥½ï¼Œä¸ºä¸åŒç”¨æˆ·ç”Ÿæˆä¸åŒçš„æ’åï¼š

```python
class PersonalizedPageRank:
    def __init__(self, damping_factor=0.85):
        self.damping_factor = damping_factor

    def calculate_personalized_pagerank(self, graph: SocialNetworkGraph,
                                      user_preferences: Dict[int, float],
                                      max_iterations=100,
                                      tolerance=1e-6) -> Dict[int, float]:
        """
        è®¡ç®—ä¸ªæ€§åŒ–PageRank

        Args:
            graph: ç¤¾äº¤ç½‘ç»œå›¾
            user_preferences: ç”¨æˆ·åå¥½å­—å…¸ {node_id: preference_score}
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            tolerance: æ”¶æ•›é˜ˆå€¼

        Returns:
            ä¸ªæ€§åŒ–PageRankåˆ†æ•°å­—å…¸
        """
        n = graph.get_agent_count()
        if n == 0:
            return {}

        nodes = list(graph.agents.keys())
        node_index = {node: i for i, node in enumerate(nodes)}

        # æ„å»ºè½¬ç§»çŸ©é˜µ
        transition_matrix = self._build_transition_matrix(graph, nodes, node_index)

        # æ„å»ºä¸ªæ€§åŒ–é‡å¯å‘é‡
        restart_vector = self._build_restart_vector(user_preferences, nodes, node_index)

        # åˆå§‹åŒ–PageRankå‘é‡
        pagerank = np.ones(n) / n

        # è¿­ä»£è®¡ç®—
        for iteration in range(max_iterations):
            old_pagerank = pagerank.copy()

            # ä¸ªæ€§åŒ–PageRankè¿­ä»£å…¬å¼
            pagerank = (self.damping_factor * transition_matrix @ pagerank +
                       (1 - self.damping_factor) * restart_vector)

            # æ£€æŸ¥æ”¶æ•›
            if np.linalg.norm(pagerank - old_pagerank, 1) < tolerance:
                break

        return {nodes[i]: pagerank[i] for i in range(n)}

    def _build_restart_vector(self, user_preferences: Dict[int, float],
                             nodes: List[int], node_index: Dict[int, int]) -> np.ndarray:
        """æ„å»ºä¸ªæ€§åŒ–é‡å¯å‘é‡"""
        n = len(nodes)
        restart_vector = np.zeros(n)

        total_preference = sum(user_preferences.values())
        if total_preference == 0:
            # æ²¡æœ‰åå¥½æ—¶ä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
            restart_vector = np.ones(n) / n
        else:
            # æ ¹æ®ç”¨æˆ·åå¥½æ„å»ºé‡å¯å‘é‡
            for node in nodes:
                if node in user_preferences:
                    idx = node_index[node]
                    restart_vector[idx] = user_preferences[node] / total_preference

            # æœªæŒ‡å®šçš„èŠ‚ç‚¹å¹³å‡åˆ†é…å‰©ä½™æƒé‡
            unspecified_weight = 1.0 - sum(restart_vector)
            unspecified_count = np.sum(restart_vector == 0)
            if unspecified_count > 0:
                restart_vector[restart_vector == 0] = unspecified_weight / unspecified_count

        return restart_vector

    def recommend_by_personalized_pagerank(self, graph: SocialNetworkGraph,
                                          user_id: int,
                                          user_interests: List[str],
                                          top_k: int = 10) -> List[Tuple[int, float]]:
        """åŸºäºä¸ªæ€§åŒ–PageRankçš„æ¨è"""
        # æ„å»ºç”¨æˆ·åå¥½
        user_preferences = self._build_user_preferences(graph, user_id, user_interests)

        # è®¡ç®—ä¸ªæ€§åŒ–PageRank
        personalized_scores = self.calculate_personalized_pagerank(graph, user_preferences)

        # è¿‡æ»¤æ‰å·²ç»æ˜¯å¥½å‹çš„èŠ‚ç‚¹
        current_friends = set(graph.get_agent_friends(user_id))
        current_friends.add(user_id)

        recommendations = [(node, score) for node, score in personalized_scores.items()
                         if node not in current_friends]

        # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›top-k
        recommendations.sort(key=lambda x: x[1], reverse=True)
        return recommendations[:top_k]
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–ç±»

### Q15: å¦‚ä½•ä¼˜åŒ–å¤§è§„æ¨¡ç¤¾äº¤ç½‘ç»œå›¾çš„æŸ¥è¯¢æ€§èƒ½ï¼Ÿ

**ç­”æ¡ˆ:**
ä¼˜åŒ–å¤§è§„æ¨¡å›¾æŸ¥è¯¢éœ€è¦å¤šå±‚æ¬¡çš„ä¼˜åŒ–ç­–ç•¥ï¼š

**1. ç´¢å¼•ä¼˜åŒ–**
```python
class OptimizedGraph:
    def __init__(self):
        self.graph = nx.Graph()
        self.adjacency_index = {}  # é‚»æ¥ç´¢å¼•
        self.degree_index = {}     # åº¦æ•°ç´¢å¼•
        self.community_index = {}  # ç¤¾åŒºç´¢å¼•

    def build_indexes(self):
        """æ„å»ºå„ç§ç´¢å¼•"""
        # é‚»æ¥ç´¢å¼•ï¼šå¿«é€ŸæŸ¥æ‰¾é‚»å±…
        for node in self.graph.nodes():
            self.adjacency_index[node] = set(self.graph.neighbors(node))

        # åº¦æ•°ç´¢å¼•ï¼šæŒ‰åº¦æ•°æ’åºçš„èŠ‚ç‚¹åˆ—è¡¨
        degree_dict = dict(self.graph.degree())
        self.degree_index = defaultdict(list)
        for node, degree in degree_dict.items():
            self.degree_index[degree].append(node)

    def fast_neighbors_query(self, node: int) -> Set[int]:
        """å¿«é€Ÿé‚»å±…æŸ¥è¯¢"""
        return self.adjacency_index.get(node, set())

    def range_degree_query(self, min_degree: int, max_degree: int) -> List[int]:
        """åº¦æ•°èŒƒå›´æŸ¥è¯¢"""
        result = []
        for degree in range(min_degree, max_degree + 1):
            result.extend(self.degree_index.get(degree, []))
        return result
```

**2. ç¼“å­˜ç­–ç•¥**
```python
class CachedGraphOperations:
    def __init__(self, graph: SocialNetworkGraph, cache_size=1000):
        self.graph = graph
        self.cache_size = cache_size
        self.path_cache = {}
        self.pagerank_cache = {}
        self.community_cache = {}

    def get_shortest_path_cached(self, start: int, end: int) -> Optional[List[int]]:
        """ç¼“å­˜çš„æœ€çŸ­è·¯å¾„æŸ¥è¯¢"""
        cache_key = (start, end)

        if cache_key in self.path_cache:
            return self.path_cache[cache_key]

        # è®¡ç®—è·¯å¾„
        calculator = ShortestPathCalculator()
        path = calculator.calculate_shortest_path(self.graph, start, end)

        # æ›´æ–°ç¼“å­˜
        self._update_cache(self.path_cache, cache_key, path)

        return path

    def _update_cache(self, cache, key, value):
        """LRUç¼“å­˜æ›´æ–°"""
        if len(cache) >= self.cache_size:
            # ç§»é™¤æœ€æ—§çš„æ¡ç›®
            oldest_key = next(iter(cache))
            del cache[oldest_key]

        cache[key] = value
```

**3. é¢„è®¡ç®—**
```python
class PrecomputedGraph:
    def __init__(self, graph: SocialNetworkGraph):
        self.graph = graph
        self.all_pairs_shortest_paths = {}
        self.pagerank_scores = {}
        self.community_assignments = {}

    def precompute_all_shortest_paths(self):
        """é¢„è®¡ç®—æ‰€æœ‰æœ€çŸ­è·¯å¾„"""
        calculator = ShortestPathCalculator()
        self.all_pairs_shortest_paths = calculator.get_all_shortest_paths(self.graph)

    def precompute_pagerank(self):
        """é¢„è®¡ç®—PageRankåˆ†æ•°"""
        calculator = PageRankCalculator()
        self.pagerank_scores = calculator.calculate_pagerank(self.graph)

    def get_shortest_path_instant(self, start: int, end: int) -> Optional[List[int]]:
        """å³æ—¶è·å–æœ€çŸ­è·¯å¾„"""
        return self.all_pairs_shortest_paths.get((start, end))
```

**4. åˆ†åŒºå¤„ç†**
```python
class PartitionedGraph:
    def __init__(self, graph: SocialNetworkGraph, partition_size=1000):
        self.graph = graph
        self.partition_size = partition_size
        self.partitions = self._create_partitions()

    def _create_partitions(self):
        """åˆ›å»ºå›¾åˆ†åŒº"""
        nodes = list(self.graph.agents.keys())
        partitions = []

        for i in range(0, len(nodes), self.partition_size):
            partition_nodes = nodes[i:i + self.partition_size]
            subgraph = self._extract_subgraph(partition_nodes)
            partitions.append(subgraph)

        return partitions

    def query_within_partition(self, query_func, partition_id: int):
        """åœ¨ç‰¹å®šåˆ†åŒºå†…æ‰§è¡ŒæŸ¥è¯¢"""
        if partition_id < len(self.partitions):
            return query_func(self.partitions[partition_id])
        return None
```

## ğŸª æ€»ç»“ç±»

### Q16: åœ¨å¼€å‘ç¤¾äº¤ç½‘ç»œç®—æ³•æ—¶ï¼Œä½ é‡åˆ°çš„æœ€å¤§æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•è§£å†³çš„ï¼Ÿ

**ç­”æ¡ˆ:**
åœ¨å¼€å‘ç¤¾äº¤ç½‘ç»œç®—æ³•è¿‡ç¨‹ä¸­ï¼Œæˆ‘é‡åˆ°äº†å‡ ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š

**æŒ‘æˆ˜1: å¤§è§„æ¨¡æ•°æ®å¤„ç†**
- **é—®é¢˜**: ç™¾ä¸‡çº§èŠ‚ç‚¹çš„å›¾ç®—æ³•è®¡ç®—æ—¶é—´è¿‡é•¿
- **è§£å†³æ–¹æ¡ˆ**:
  - å®ç°åˆ†å¸ƒå¼è®¡ç®—ç‰ˆæœ¬
  - ä½¿ç”¨é‡‡æ ·å’Œè¿‘ä¼¼ç®—æ³•
  - ä¼˜åŒ–æ•°æ®ç»“æ„å’Œç®—æ³•å¤æ‚åº¦

**æŒ‘æˆ˜2: å®æ—¶æ€§è¦æ±‚**
- **é—®é¢˜**: ç”¨æˆ·æœŸæœ›å®æ—¶å¾—åˆ°åˆ†æç»“æœ
- **è§£å†³æ–¹æ¡ˆ**:
  - å®ç°å¢é‡è®¡ç®—
  - ä½¿ç”¨ç¼“å­˜å’Œé¢„è®¡ç®—
  - å¼‚æ­¥å¤„ç†å’Œæµå¼è®¡ç®—

**æŒ‘æˆ˜3: ç®—æ³•å‡†ç¡®æ€§**
- **é—®é¢˜**: ç¡®ä¿ç®—æ³•ç»“æœçš„æ­£ç¡®æ€§å’Œç¨³å®šæ€§
- **è§£å†³æ–¹æ¡ˆ**:
  - ä¸¥æ ¼çš„TDDå¼€å‘æµç¨‹
  - å¤§é‡è¾¹ç•Œæƒ…å†µæµ‹è¯•
  - ä¸å·²çŸ¥ç»“æœå¯¹æ¯”éªŒè¯

**æŒ‘æˆ˜4: å¤šç®—æ³•é›†æˆ**
- **é—®é¢˜**: ä¸åŒç®—æ³•ä¹‹é—´çš„æ¥å£å’Œæ•°æ®æ ¼å¼ç»Ÿä¸€
- **è§£å†³æ–¹æ¡ˆ**:
  - è®¾è®¡ç»Ÿä¸€çš„ç®—æ³•æ¥å£
  - å®ç°é€‚é…å™¨æ¨¡å¼
  - å»ºç«‹æ ‡å‡†åŒ–çš„æ•°æ®æ ¼å¼

è¿™äº›æŒ‘æˆ˜çš„è§£å†³è¿‡ç¨‹è®©æˆ‘æ·±å…¥ç†è§£äº†å¤§è§„æ¨¡ç³»ç»Ÿè®¾è®¡çš„å¤æ‚æ€§ï¼Œä¹Ÿæå‡äº†é—®é¢˜è§£å†³èƒ½åŠ›ã€‚

---

## ğŸ“š å­¦ä¹ èµ„æºæ¨è

### ç»å…¸ä¹¦ç±
1. "Networks, Crowds, and Markets" - ç¤¾ä¼šç½‘ç»œåˆ†æåŸºç¡€
2. "Graph Algorithms" - å›¾ç®—æ³•å®ç”¨æŒ‡å—
3. "Test-Driven Development" - TDDæ–¹æ³•è®º
4. "Designing Data-Intensive Applications" - å¤§è§„æ¨¡ç³»ç»Ÿè®¾è®¡

### åœ¨çº¿èµ„æº
1. **Stanford Network Analysis Platform** (SNAP)
2. **NetworkX Documentation**
3. **Apache Spark GraphX Guide**
4. **Neo4j Graph Database**

### å®è·µé¡¹ç›®
1. æ„å»ºå°å‹ç¤¾äº¤ç½‘ç»œåˆ†æå¹³å°
2. å®ç°æ¨èç³»ç»Ÿç®—æ³•
3. å¼€å‘å›¾å¯è§†åŒ–å·¥å…·
4. å‚ä¸å¼€æºå›¾ç®—æ³•é¡¹ç›®

è¿™äº›é¢è¯•é¢˜æ¶µç›–äº†ç¤¾äº¤ç½‘ç»œç®—æ³•çš„æ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼Œå‡†å¤‡é¢è¯•æ—¶å»ºè®®ç»“åˆå®é™…é¡¹ç›®ç»éªŒæ¥å›ç­”ï¼Œå±•ç¤ºè‡ªå·±çš„ç†è§£å’Œå®è·µèƒ½åŠ›ã€‚

---

## ğŸ·ï¸ æ ‡ç­¾

`#é¢è¯•é¢˜` `#ç¤¾äº¤ç½‘ç»œ` `#å›¾ç®—æ³•` `#PageRank` `#ç¤¾åŒºå‘ç°` `#ç³»ç»Ÿè®¾è®¡` `#ç®—æ³•å®ç°` `#æŠ€æœ¯é¢è¯•`