"""
åˆ†å¸ƒå¼ç¼“å­˜å’Œä¸€è‡´æ€§ç³»ç»Ÿæµ‹è¯•
"""

import pytest
import asyncio
import time
import json
import threading
from unittest.mock import Mock, patch, AsyncMock
from src.distributed.distributed_cache import (
    DistributedCache, CacheNode, CacheEntry, CacheConsistency,
    ConsistencyLevel, CacheOperation, ConflictResolution,
    CacheCluster, CacheReplication, CachePartitioning
)


class TestCacheEntry:
    """æµ‹è¯•ç¼“å­˜æ¡ç›®"""

    def test_cache_entry_creation(self):
        """æµ‹è¯•ç¼“å­˜æ¡ç›®åˆ›å»º"""
        entry = CacheEntry(
            key="test_key",
            value="test_value",
            ttl_seconds=60
        )

        assert entry.key == "test_key"
        assert entry.value == "test_value"
        assert entry.ttl_seconds == 60
        assert entry.version == 1
        assert entry.created_at > 0
        assert entry.expires_at is not None
        assert entry.access_count == 0
        assert entry.last_accessed is None

    def test_cache_entry_ttl_calculation(self):
        """æµ‹è¯•TTLè®¡ç®—"""
        entry = CacheEntry(
            key="test_key",
            value="test_value",
            ttl_seconds=60
        )

        # åº”è¯¥åœ¨æœªæ¥60ç§’è¿‡æœŸ
        expected_expiry = entry.created_at + 60
        assert abs(entry.expires_at - expected_expiry) < 1

    def test_cache_entry_access(self):
        """æµ‹è¯•è®¿é—®ç¼“å­˜æ¡ç›®"""
        entry = CacheEntry(
            key="test_key",
            value="test_value",
            ttl_seconds=60
        )

        initial_count = entry.access_count
        time.sleep(0.01)
        entry.access()

        assert entry.access_count == initial_count + 1
        assert entry.last_accessed > entry.created_at

    def test_cache_entry_is_expired(self):
        """æµ‹è¯•ç¼“å­˜æ¡ç›®è¿‡æœŸæ£€æŸ¥"""
        entry = CacheEntry(
            key="test_key",
            value="test_value",
            ttl_seconds=1
        )

        # æœªè¿‡æœŸ
        assert not entry.is_expired()

        # æ¨¡æ‹Ÿè¿‡æœŸ
        entry.expires_at = time.time() - 1
        assert entry.is_expired()

    def test_cache_entry_update_value(self):
        """æµ‹è¯•æ›´æ–°ç¼“å­˜æ¡ç›®å€¼"""
        entry = CacheEntry(
            key="test_key",
            value="test_value",
            ttl_seconds=60
        )

        entry.update_value("new_value")
        assert entry.value == "new_value"
        assert entry.version == 2

    def test_cache_entry_serialization(self):
        """æµ‹è¯•ç¼“å­˜æ¡ç›®åºåˆ—åŒ–"""
        entry = CacheEntry(
            key="test_key",
            value={"nested": "data"},
            ttl_seconds=60
        )

        serialized = entry.to_dict()
        assert serialized["key"] == "test_key"
        assert serialized["value"] == {"nested": "data"}
        assert serialized["version"] == 1

        # æµ‹è¯•ååºåˆ—åŒ–
        restored = CacheEntry.from_dict(serialized)
        assert restored.key == entry.key
        assert restored.value == entry.value
        assert restored.version == entry.version


class TestCacheNode:
    """æµ‹è¯•ç¼“å­˜èŠ‚ç‚¹"""

    def test_cache_node_initialization(self):
        """æµ‹è¯•ç¼“å­˜èŠ‚ç‚¹åˆå§‹åŒ–"""
        node = CacheNode(
            node_id="node_1",
            address="localhost:8001",
            max_memory_mb=100
        )

        assert node.node_id == "node_1"
        assert node.address == "localhost:8001"
        assert node.max_memory_mb == 100
        assert len(node.cache) == 0
        assert node.current_memory_mb == 0
        assert node.is_active is True

    def test_cache_node_set_and_get(self):
        """æµ‹è¯•ç¼“å­˜èŠ‚ç‚¹è®¾ç½®å’Œè·å–"""
        node = CacheNode("node_1", "localhost:8001")

        # è®¾ç½®ç¼“å­˜
        result = node.set("key1", "value1", ttl_seconds=60)
        assert result is True

        # è·å–ç¼“å­˜
        value = node.get("key1")
        assert value == "value1"

        # è·å–ä¸å­˜åœ¨çš„é”®
        value = node.get("nonexistent")
        assert value is None

    def test_cache_node_set_with_ttl(self):
        """æµ‹è¯•å¸¦TTLçš„ç¼“å­˜è®¾ç½®"""
        node = CacheNode("node_1", "localhost:8001")

        node.set("key1", "value1", ttl_seconds=1)
        value = node.get("key1")
        assert value == "value1"

        # ç­‰å¾…è¿‡æœŸ
        time.sleep(1.1)
        value = node.get("key1")
        assert value is None

    def test_cache_node_delete(self):
        """æµ‹è¯•åˆ é™¤ç¼“å­˜"""
        node = CacheNode("node_1", "localhost:8001")

        node.set("key1", "value1")
        assert node.get("key1") == "value1"

        result = node.delete("key1")
        assert result is True
        assert node.get("key1") is None

        # åˆ é™¤ä¸å­˜åœ¨çš„é”®
        result = node.delete("nonexistent")
        assert result is False

    def test_cache_node_clear(self):
        """æµ‹è¯•æ¸…ç©ºç¼“å­˜"""
        node = CacheNode("node_1", "localhost:8001")

        node.set("key1", "value1")
        node.set("key2", "value2")
        assert len(node.cache) == 2

        node.clear()
        assert len(node.cache) == 0

    def test_cache_node_size_limit(self):
        """æµ‹è¯•ç¼“å­˜å¤§å°é™åˆ¶"""
        node = CacheNode("node_1", "localhost:8001", max_entries=2)

        node.set("key1", "value1")
        node.set("key2", "value2")
        assert len(node.cache) == 2

        # æ·»åŠ ç¬¬ä¸‰ä¸ªæ¡ç›®åº”è¯¥è§¦å‘LRUæ·˜æ±°
        node.set("key3", "value3")
        assert len(node.cache) == 2
        assert "key3" in node.cache
        # key1åº”è¯¥è¢«æ·˜æ±°ï¼ˆæœ€ä¹…æœªè®¿é—®ï¼‰

    def test_cache_node_memory_limit(self):
        """æµ‹è¯•å†…å­˜é™åˆ¶"""
        node = CacheNode("node_1", "localhost:8001", max_memory_mb=1)

        # æ·»åŠ å¤§å€¼åº”è¯¥è§¦å‘æ¸…ç†
        large_value = "x" * (1024 * 1024)  # 1MB
        node.set("large_key", large_value)

        # å†…å­˜ä½¿ç”¨åº”è¯¥æ¥è¿‘é™åˆ¶
        assert node.current_memory_mb <= node.max_memory_mb * 1.1  # å…è®¸10%æº¢å‡º

    def test_cache_node_statistics(self):
        """æµ‹è¯•ç¼“å­˜èŠ‚ç‚¹ç»Ÿè®¡"""
        node = CacheNode("node_1", "localhost:8001")

        stats = node.get_statistics()
        assert stats['total_entries'] == 0
        assert stats['hit_count'] == 0
        assert stats['miss_count'] == 0
        assert stats['hit_rate'] == 0.0

        # æ·»åŠ ä¸€äº›ç¼“å­˜
        node.set("key1", "value1")
        node.get("key1")  # hit
        node.get("nonexistent")  # miss

        stats = node.get_statistics()
        assert stats['total_entries'] == 1
        assert stats['hit_count'] == 1
        assert stats['miss_count'] == 1
        assert stats['hit_rate'] == 0.5

    def test_cache_node_cleanup_expired(self):
        """æµ‹è¯•æ¸…ç†è¿‡æœŸç¼“å­˜"""
        node = CacheNode("node_1", "localhost:8001")

        node.set("key1", "value1", ttl_seconds=1)
        node.set("key2", "value2", ttl_seconds=60)

        time.sleep(1.1)
        cleaned_count = node.cleanup_expired()

        assert cleaned_count == 1
        assert node.get("key1") is None
        assert node.get("key2") == "value2"


class TestCacheConsistency:
    """æµ‹è¯•ç¼“å­˜ä¸€è‡´æ€§"""

    def test_consistency_level_validation(self):
        """æµ‹è¯•ä¸€è‡´æ€§çº§åˆ«éªŒè¯"""
        for level in ConsistencyLevel:
            assert isinstance(level.value, str)
            assert len(level.value) > 0

    def test_cache_operation_creation(self):
        """æµ‹è¯•ç¼“å­˜æ“ä½œåˆ›å»º"""
        operation = CacheOperation(
            operation_type="SET",
            key="test_key",
            value="test_value",
            node_id="node_1",
            timestamp=time.time()
        )

        assert operation.operation_type == "SET"
        assert operation.key == "test_key"
        assert operation.value == "test_value"
        assert operation.node_id == "node_1"
        assert operation.timestamp > 0

    def test_conflict_resolution_last_write_wins(self):
        """æµ‹è¯•æœ€åå†™å…¥è·èƒœçš„å†²çªè§£å†³"""
        consistency = CacheConsistency()

        entry1 = CacheEntry("key", "value1", ttl_seconds=60)
        entry2 = CacheEntry("key", "value2", ttl_seconds=60)
        entry2.created_at = entry1.created_at + 1  # æ›´æ–°çš„æ—¶é—´æˆ³

        winner = consistency.resolve_conflict(entry1, entry2, ConflictResolution.LAST_WRITE_WINS)
        assert winner.value == "value2"

    def test_conflict_resolution_version_wins(self):
        """æµ‹è¯•ç‰ˆæœ¬å·è·èƒœçš„å†²çªè§£å†³"""
        consistency = CacheConsistency()

        entry1 = CacheEntry("key", "value1", ttl_seconds=60)
        entry2 = CacheEntry("key", "value2", ttl_seconds=60)
        entry2.version = 2  # æ›´é«˜çš„ç‰ˆæœ¬å·

        winner = consistency.resolve_conflict(entry1, entry2, ConflictResolution.VERSION_WINS)
        assert winner.value == "value2"

    def test_conflict_resolution_custom_merge(self):
        """æµ‹è¯•è‡ªå®šä¹‰åˆå¹¶å†²çªè§£å†³"""
        consistency = CacheConsistency()

        def merge_func(entry1, entry2):
            return CacheEntry(
                key=entry1.key,
                value=f"{entry1.value}+{entry2.value}",
                ttl_seconds=entry1.ttl_seconds
            )

        entry1 = CacheEntry("key", "value1", ttl_seconds=60)
        entry2 = CacheEntry("key", "value2", ttl_seconds=60)

        winner = consistency.resolve_conflict(entry1, entry2, ConflictResolution.CUSTOM_MERGE, merge_func)
        assert winner.value == "value1+value2"

    @pytest.mark.asyncio
    async def test_propagate_operation_to_nodes(self):
        """æµ‹è¯•å‘èŠ‚ç‚¹ä¼ æ’­æ“ä½œ"""
        consistency = CacheConsistency()

        node1 = Mock()
        node2 = Mock()
        nodes = {"node1": node1, "node2": node2}

        operation = CacheOperation(
            operation_type="SET",
            key="test_key",
            value="test_value",
            node_id="source_node",
            timestamp=time.time()
        )

        await consistency.propagate_operation(operation, nodes, exclude_nodes=["source_node"])

        # åº”è¯¥ä¼ æ’­åˆ°å…¶ä»–èŠ‚ç‚¹
        node1.apply_operation.assert_called_once_with(operation)
        node2.apply_operation.assert_called_once_with(operation)

    @pytest.mark.asyncio
    async def test_sync_nodes_consistency(self):
        """æµ‹è¯•èŠ‚ç‚¹ä¸€è‡´æ€§åŒæ­¥"""
        consistency = CacheConsistency()

        node1 = Mock()
        node1.get_all_entries.return_value = {
            "key1": CacheEntry("key1", "value1", ttl_seconds=60),
            "key2": CacheEntry("key2", "old_value", ttl_seconds=60)
        }

        node2 = Mock()
        node2.get_all_entries.return_value = {
            "key1": CacheEntry("key1", "new_value", ttl_seconds=60),
            "key3": CacheEntry("key3", "value3", ttl_seconds=60)
        }

        nodes = {"node1": node1, "node2": node2}

        sync_result = await consistency.sync_nodes(nodes)

        assert len(sync_result) > 0
        # åº”è¯¥æ£€æµ‹åˆ°key1çš„å†²çª


class TestCacheReplication:
    """æµ‹è¯•ç¼“å­˜å¤åˆ¶"""

    def test_replication_initialization(self):
        """æµ‹è¯•å¤åˆ¶åˆå§‹åŒ–"""
        replication = CacheReplication(
            replication_factor=3,
            consistency_level=ConsistencyLevel.EVENTUAL
        )

        assert replication.replication_factor == 3
        assert replication.consistency_level == ConsistencyLevel.EVENTUAL

    def test_select_replication_nodes(self):
        """æµ‹è¯•é€‰æ‹©å¤åˆ¶èŠ‚ç‚¹"""
        replication = CacheReplication(replication_factor=2)

        available_nodes = ["node1", "node2", "node3", "node4"]
        source_node = "node1"

        selected = replication.select_replication_nodes(
            key="test_key",
            available_nodes=available_nodes,
            source_node=source_node
        )

        assert len(selected) == 2
        assert source_node not in selected  # ä¸åº”è¯¥åŒ…å«æºèŠ‚ç‚¹
        assert all(node in available_nodes for node in selected)

    def test_replication_factor_validation(self):
        """æµ‹è¯•å¤åˆ¶å› å­éªŒè¯"""
        # å¤åˆ¶å› å­åº”è¯¥å°äºç­‰äºå¯ç”¨èŠ‚ç‚¹æ•°
        replication = CacheReplication(replication_factor=5)
        available_nodes = ["node1", "node2", "node3"]

        selected = replication.select_replication_nodes(
            key="test_key",
            available_nodes=available_nodes,
            source_node="node1"
        )

        # æœ€å¤šé€‰æ‹©å¯ç”¨èŠ‚ç‚¹æ•°-1ï¼ˆæ’é™¤æºèŠ‚ç‚¹ï¼‰
        assert len(selected) <= len(available_nodes) - 1

    @pytest.mark.asyncio
    async def test_replicate_write_operation(self):
        """æµ‹è¯•å¤åˆ¶å†™æ“ä½œ"""
        replication = CacheReplication(replication_factor=2)

        node1 = Mock()
        node2 = Mock()
        nodes = {"node1": node1, "node2": node2}

        operation = CacheOperation(
            operation_type="SET",
            key="test_key",
            value="test_value",
            node_id="source_node",
            timestamp=time.time()
        )

        result = await replication.replicate_write(
            operation=operation,
            nodes=nodes,
            replication_nodes=["node1", "node2"]
        )

        assert result.success_count == 2
        assert result.failure_count == 0
        node1.apply_operation.assert_called_once_with(operation)
        node2.apply_operation.assert_called_once_with(operation)

    @pytest.mark.asyncio
    async def test_replicate_read_operation(self):
        """æµ‹è¯•å¤åˆ¶è¯»æ“ä½œ"""
        replication = CacheReplication()

        node1 = Mock()
        node1.get.return_value = "value1"
        node2 = Mock()
        node2.get.return_value = "value2"
        nodes = {"node1": node1, "node2": node2}

        values = await replication.replicate_read(
            key="test_key",
            nodes=nodes,
            consistency_level=ConsistencyLevel.EVENTUAL
        )

        assert len(values) >= 1  # è‡³å°‘åº”è¯¥æœ‰ä¸€ä¸ªè¿”å›å€¼


class TestCachePartitioning:
    """æµ‹è¯•ç¼“å­˜åˆ†åŒº"""

    def test_partitioning_initialization(self):
        """æµ‹è¯•åˆ†åŒºåˆå§‹åŒ–"""
        partitioning = CachePartitioning(num_partitions=16)

        assert partitioning.num_partitions == 16

    def test_hash_based_partitioning(self):
        """æµ‹è¯•åŸºäºå“ˆå¸Œçš„åˆ†åŒº"""
        partitioning = CachePartitioning(num_partitions=4)

        # ç›¸åŒçš„é”®åº”è¯¥æ€»æ˜¯æ˜ å°„åˆ°ç›¸åŒçš„åˆ†åŒº
        partition1 = partitioning.get_partition("test_key")
        partition2 = partitioning.get_partition("test_key")
        assert partition1 == partition2

        # ä¸åŒçš„é”®å¯èƒ½æ˜ å°„åˆ°ä¸åŒçš„åˆ†åŒº
        partition3 = partitioning.get_partition("different_key")
        # æ³¨æ„ï¼šä¸æ˜¯ä¸€å®šä¸åŒï¼Œå› ä¸ºå“ˆå¸Œå¯èƒ½ç¢°æ’

    def test_range_based_partitioning(self):
        """æµ‹è¯•åŸºäºèŒƒå›´çš„åˆ†åŒº"""
        partitioning = CachePartitioning(num_partitions=4)

        # ä½¿ç”¨æ•°å­—é”®è¿›è¡ŒèŒƒå›´åˆ†åŒº
        partition1 = partitioning.get_partition("user:100")
        partition2 = partitioning.get_partition("user:200")

        # éªŒè¯åˆ†åŒºç»“æœåœ¨æœ‰æ•ˆèŒƒå›´å†…
        assert 0 <= partition1 < partitioning.num_partitions
        assert 0 <= partition2 < partitioning.num_partitions

    def test_consistent_hashing(self):
        """æµ‹è¯•ä¸€è‡´æ€§å“ˆå¸Œ"""
        partitioning = CachePartitioning(num_partitions=100)
        partitioning.set_partitioning_strategy("consistent_hash")

        # ä¸€è‡´æ€§å“ˆå¸Œåº”è¯¥å‡åŒ€åˆ†å¸ƒé”®
        partitions = {}
        for i in range(1000):
            key = f"key_{i}"
            partition = partitioning.get_partition(key)
            partitions[partition] = partitions.get(partition, 0) + 1

        # éªŒè¯åˆ†å¸ƒç›¸å¯¹å‡åŒ€
        avg_per_partition = 1000 / partitioning.num_partitions
        for count in partitions.values():
            # å…è®¸ä¸€å®šçš„åå·® - ä½¿ç”¨æ›´å®½æ¾çš„å®¹å¿åº¦
            assert abs(count - avg_per_partition) <= avg_per_partition * 1.2

    def test_rebalance_partitions(self):
        """æµ‹è¯•é‡æ–°å¹³è¡¡åˆ†åŒº"""
        partitioning = CachePartitioning(num_partitions=4)

        old_nodes = ["node1", "node2"]
        new_nodes = ["node1", "node2", "node3"]

        migration_plan = partitioning.rebalance(old_nodes, new_nodes)

        # åº”è¯¥æœ‰è¿ç§»è®¡åˆ’
        assert len(migration_plan) > 0

        # éªŒè¯è¿ç§»è®¡åˆ’åŒ…å«æœ‰æ•ˆçš„åˆ†åŒº
        for partition, target_node in migration_plan.items():
            assert 0 <= partition < partitioning.num_partitions
            assert target_node in new_nodes


class TestCacheCluster:
    """æµ‹è¯•ç¼“å­˜é›†ç¾¤"""

    def test_cluster_initialization(self):
        """æµ‹è¯•é›†ç¾¤åˆå§‹åŒ–"""
        cluster = CacheCluster(
            cluster_id="cluster_1",
            replication_factor=2,
            consistency_level=ConsistencyLevel.EVENTUAL
        )

        assert cluster.cluster_id == "cluster_1"
        assert cluster.replication_factor == 2
        assert cluster.consistency_level == ConsistencyLevel.EVENTUAL
        assert len(cluster.nodes) == 0

    def test_add_remove_nodes(self):
        """æµ‹è¯•æ·»åŠ å’Œç§»é™¤èŠ‚ç‚¹"""
        cluster = CacheCluster("cluster_1")

        # æ·»åŠ èŠ‚ç‚¹
        node1 = CacheNode("node1", "localhost:8001")
        result = cluster.add_node(node1)
        assert result is True
        assert len(cluster.nodes) == 1

        # æ·»åŠ é‡å¤èŠ‚ç‚¹
        result = cluster.add_node(node1)
        assert result is False

        # ç§»é™¤èŠ‚ç‚¹
        result = cluster.remove_node("node1")
        assert result is True
        assert len(cluster.nodes) == 0

        # ç§»é™¤ä¸å­˜åœ¨çš„èŠ‚ç‚¹
        result = cluster.remove_node("nonexistent")
        assert result is False

    def test_cluster_set_and_get(self):
        """æµ‹è¯•é›†ç¾¤è®¾ç½®å’Œè·å–"""
        cluster = CacheCluster("cluster_1")

        node1 = CacheNode("node1", "localhost:8001")
        node2 = CacheNode("node2", "localhost:8002")
        cluster.add_node(node1)
        cluster.add_node(node2)

        # è®¾ç½®å€¼
        result = cluster.set("key1", "value1")
        assert result is True

        # è·å–å€¼
        value = cluster.get("key1")
        assert value == "value1"

    def test_cluster_consistency_levels(self):
        """æµ‹è¯•é›†ç¾¤ä¸€è‡´æ€§çº§åˆ«"""
        # å¼ºä¸€è‡´æ€§
        cluster_strong = CacheCluster(
            "cluster_1",
            consistency_level=ConsistencyLevel.STRONG
        )

        # æœ€ç»ˆä¸€è‡´æ€§
        cluster_eventual = CacheCluster(
            "cluster_2",
            consistency_level=ConsistencyLevel.EVENTUAL
        )

        assert cluster_strong.consistency_level == ConsistencyLevel.STRONG
        assert cluster_eventual.consistency_level == ConsistencyLevel.EVENTUAL

    @pytest.mark.asyncio
    async def test_cluster_failover(self):
        """æµ‹è¯•é›†ç¾¤æ•…éšœè½¬ç§»"""
        cluster = CacheCluster("cluster_1")

        node1 = CacheNode("node1", "localhost:8001")
        node2 = CacheNode("node2", "localhost:8002")
        cluster.add_node(node1)
        cluster.add_node(node2)

        # è®¾ç½®å€¼
        await cluster.set_async("key1", "value1")
        value = await cluster.get_async("key1")
        assert value == "value1"

        # æ¨¡æ‹ŸèŠ‚ç‚¹æ•…éšœ
        node1.is_active = False
        cluster.handle_node_failure("node1")

        # åº”è¯¥ä»ç„¶èƒ½ä»å…¶ä»–èŠ‚ç‚¹è·å–å€¼
        value = await cluster.get_async("key1")
        assert value == "value1"

    def test_cluster_statistics(self):
        """æµ‹è¯•é›†ç¾¤ç»Ÿè®¡"""
        cluster = CacheCluster("cluster_1")

        node1 = CacheNode("node1", "localhost:8001")
        node2 = CacheNode("node2", "localhost:8002")
        cluster.add_node(node1)
        cluster.add_node(node2)

        stats = cluster.get_cluster_statistics()
        assert stats['total_nodes'] == 2
        assert stats['active_nodes'] == 2
        assert 'cache_statistics' in stats

    @pytest.mark.asyncio
    async def test_cluster_rebalance(self):
        """æµ‹è¯•é›†ç¾¤é‡æ–°å¹³è¡¡"""
        cluster = CacheCluster("cluster_1")

        node1 = CacheNode("node1", "localhost:8001")
        node2 = CacheNode("node2", "localhost:8002")
        cluster.add_node(node1)
        cluster.add_node(node2)

        # è®¾ç½®ä¸€äº›æ•°æ®
        await cluster.set_async("key1", "value1")
        await cluster.set_async("key2", "value2")

        # æ·»åŠ æ–°èŠ‚ç‚¹
        node3 = CacheNode("node3", "localhost:8003")
        cluster.add_node(node3)

        # è§¦å‘é‡æ–°å¹³è¡¡
        rebalance_result = await cluster.rebalance()
        assert rebalance_result.success
        assert rebalance_result.migrated_keys > 0


class TestDistributedCache:
    """æµ‹è¯•åˆ†å¸ƒå¼ç¼“å­˜ä¸»ç±»"""

    def test_distributed_cache_initialization(self):
        """æµ‹è¯•åˆ†å¸ƒå¼ç¼“å­˜åˆå§‹åŒ–"""
        cache = DistributedCache(
            cluster_id="cache_cluster",
            node_id="node_1",
            max_memory_mb=100,
            replication_factor=2
        )

        assert cache.cluster.cluster_id == "cache_cluster"
        assert cache.local_node.node_id == "node_1"
        assert cache.replication_factor == 2

    def test_distributed_cache_basic_operations(self):
        """æµ‹è¯•åˆ†å¸ƒå¼ç¼“å­˜åŸºæœ¬æ“ä½œ"""
        cache = DistributedCache("cluster", "node1")

        # è®¾ç½®å’Œè·å–
        result = cache.set("key", "value")
        assert result is True

        value = cache.get("key")
        assert value == "value"

        # åˆ é™¤
        result = cache.delete("key")
        assert result is True

        value = cache.get("key")
        assert value is None

    @pytest.mark.asyncio
    async def test_distributed_cache_async_operations(self):
        """æµ‹è¯•åˆ†å¸ƒå¼ç¼“å­˜å¼‚æ­¥æ“ä½œ"""
        cache = DistributedCache("cluster", "node1")

        # å¼‚æ­¥è®¾ç½®å’Œè·å–
        result = await cache.set_async("async_key", "async_value")
        assert result is True

        value = await cache.get_async("async_key")
        assert value == "async_value"

    def test_distributed_cache_batch_operations(self):
        """æµ‹è¯•åˆ†å¸ƒå¼ç¼“å­˜æ‰¹é‡æ“ä½œ"""
        cache = DistributedCache("cluster", "node1")

        # æ‰¹é‡è®¾ç½®
        data = {"key1": "value1", "key2": "value2", "key3": "value3"}
        results = cache.set_batch(data)
        assert all(results.values())

        # æ‰¹é‡è·å–
        keys = ["key1", "key2", "key3", "nonexistent"]
        values = cache.get_batch(keys)
        assert values["key1"] == "value1"
        assert values["key2"] == "value2"
        assert values["key3"] == "value3"
        assert values["nonexistent"] is None

    @pytest.mark.asyncio
    async def test_distributed_cache_consistency_strong(self):
        """æµ‹è¯•å¼ºä¸€è‡´æ€§"""
        cache = DistributedCache(
            "cluster", "node1",
            consistency_level=ConsistencyLevel.STRONG
        )

        result = await cache.set_async("key", "value")
        assert result is True

        # å¼ºä¸€è‡´æ€§ä¸‹åº”è¯¥èƒ½ç«‹å³è¯»åˆ°æœ€æ–°å€¼
        value = await cache.get_async("key")
        assert value == "value"

    @pytest.mark.asyncio
    async def test_distributed_cache_consistency_eventual(self):
        """æµ‹è¯•æœ€ç»ˆä¸€è‡´æ€§"""
        cache = DistributedCache(
            "cluster", "node1",
            consistency_level=ConsistencyLevel.EVENTUAL
        )

        result = await cache.set_async("key", "value")
        assert result is True

        # æœ€ç»ˆä¸€è‡´æ€§ä¸‹å¯èƒ½éœ€è¦ç­‰å¾…æ‰èƒ½è¯»åˆ°æœ€æ–°å€¼
        value = await cache.get_async("key")
        assert value == "value"  # åœ¨å•èŠ‚ç‚¹æµ‹è¯•ä¸­åº”è¯¥èƒ½ç«‹å³è¯»åˆ°

    def test_distributed_cache_persistence(self):
        """æµ‹è¯•åˆ†å¸ƒå¼ç¼“å­˜æŒä¹…åŒ–"""
        cache = DistributedCache("cluster", "node1", enable_persistence=True)

        cache.set("persistent_key", "persistent_value")

        # æ¨¡æ‹Ÿé‡å¯
        cache_data = cache.save_to_dict()
        new_cache = DistributedCache.load_from_dict(cache_data)

        value = new_cache.get("persistent_key")
        assert value == "persistent_value"

    def test_distributed_cache_monitoring(self):
        """æµ‹è¯•åˆ†å¸ƒå¼ç¼“å­˜ç›‘æ§"""
        cache = DistributedCache("cluster", "node1")

        cache.set("monitor_key", "monitor_value")
        cache.get("monitor_key")
        cache.get("nonexistent_key")

        metrics = cache.get_metrics()
        assert metrics['hit_count'] >= 1
        assert metrics['miss_count'] >= 1
        assert metrics['total_operations'] >= 2
        assert 'hit_rate' in metrics

    def test_distributed_cache_node_health_check(self):
        """æµ‹è¯•èŠ‚ç‚¹å¥åº·æ£€æŸ¥"""
        cache = DistributedCache("cluster", "node1")

        health = cache.check_node_health()
        assert health['is_healthy'] is True
        assert 'memory_usage' in health
        assert 'cache_size' in health
        assert 'uptime' in health

    @pytest.mark.asyncio
    async def test_distributed_cache_full_lifecycle(self):
        """æµ‹è¯•åˆ†å¸ƒå¼ç¼“å­˜å®Œæ•´ç”Ÿå‘½å‘¨æœŸ"""
        cache = DistributedCache("test_cluster", "test_node")

        # å¯åŠ¨
        await cache.start()
        assert cache.is_running is True

        # åŸºæœ¬æ“ä½œ
        await cache.set_async("lifecycle_key", "lifecycle_value")
        value = await cache.get_async("lifecycle_key")
        assert value == "lifecycle_value"

        # æ‰¹é‡æ“ä½œ
        batch_data = {"batch1": "value1", "batch2": "value2"}
        await cache.set_batch_async(batch_data)
        batch_values = await cache.get_batch_async(["batch1", "batch2"])
        assert batch_values["batch1"] == "value1"
        assert batch_values["batch2"] == "value2"

        # åœæ­¢
        await cache.stop()
        assert cache.is_running is False

    def test_distributed_cache_performance_monitoring(self):
        """æµ‹è¯•åˆ†å¸ƒå¼ç¼“å­˜æ€§èƒ½ç›‘æ§"""
        cache = DistributedCache("cluster", "node1")

        # æ‰§è¡Œä¸€äº›æ“ä½œ
        for i in range(100):
            cache.set(f"perf_key_{i}", f"perf_value_{i}")

        for i in range(100):
            cache.get(f"perf_key_{i}")

        performance = cache.get_performance_metrics()
        assert performance['total_operations'] >= 200
        assert 'operations_per_second' in performance
        assert 'average_response_time' in performance
        assert 'memory_efficiency' in performance


class TestCacheEntryAdvanced:
    """æµ‹è¯•ç¼“å­˜æ¡ç›®çš„é«˜çº§åŠŸèƒ½"""

    def test_cache_entry_update_ttl(self):
        """æµ‹è¯•æ›´æ–°TTLåŠŸèƒ½"""
        entry = CacheEntry(
            key="test_key",
            value="test_value",
            ttl_seconds=60
        )

        original_expiry = entry.expires_at
        time.sleep(0.1)  # ç¡®ä¿æ—¶é—´å·®

        entry.update_ttl(120)
        assert entry.ttl_seconds == 120
        assert entry.expires_at > original_expiry

    def test_cache_entry_increment_version(self):
        """æµ‹è¯•ç‰ˆæœ¬å·é€’å¢"""
        entry = CacheEntry(
            key="test_key",
            value="test_value"
        )

        original_version = entry.version
        new_version = entry.increment_version()
        assert new_version == original_version + 1
        assert entry.version == new_version

    def test_cache_entry_from_dict(self):
        """æµ‹è¯•ä»å­—å…¸åˆ›å»ºç¼“å­˜æ¡ç›®"""
        data = {
            "key": "test_key",
            "value": {"nested": "data"},
            "ttl_seconds": 60,
            "version": 2,
            "created_at": time.time(),
            "expires_at": time.time() + 60,
            "access_count": 5,
            "last_accessed": time.time(),
            "metadata": {"custom": "field"}
        }

        entry = CacheEntry.from_dict(data)
        assert entry.key == "test_key"
        assert entry.value == {"nested": "data"}
        assert entry.ttl_seconds == 60
        assert entry.version == 2
        assert entry.access_count == 5
        assert entry.metadata == {"custom": "field"}


class TestCacheNodeAdvanced:
    """æµ‹è¯•ç¼“å­˜èŠ‚ç‚¹çš„é«˜çº§åŠŸèƒ½"""

    def test_cache_node_memory_pressure_eviction(self):
        """æµ‹è¯•å†…å­˜å‹åŠ›ä¸‹çš„LRUæ·˜æ±°"""
        # åˆ›å»ºä¸€ä¸ªå†…å­˜é™åˆ¶å¾ˆå°çš„èŠ‚ç‚¹
        node = CacheNode(
            id="test_node",
            max_memory=1024,  # 1KB
            max_entries=5
        )

        # æ·»åŠ å¤§é‡æ¡ç›®è§¦å‘æ·˜æ±°
        large_entries = []
        for i in range(10):
            # åˆ›å»ºè¶³å¤Ÿå¤§çš„æ¡ç›®æ¥è§¦å‘å†…å­˜é™åˆ¶
            large_value = "x" * 200  # 200å­—ç¬¦
            entry = CacheEntry(
                key=f"key_{i}",
                value=large_value,
                ttl_seconds=300
            )
            large_entries.append(entry)
            node.put(f"key_{i}", entry)

        # éªŒè¯æœ€æ—§çš„æ¡ç›®è¢«æ·˜æ±°
        assert len(node.storage) <= 5
        assert "key_0" not in node.storage  # æœ€æ—§çš„åº”è¯¥è¢«æ·˜æ±°
        assert "key_9" in node.storage  # æœ€æ–°çš„åº”è¯¥ä¿ç•™

    def test_cache_node_concurrent_access(self):
        """æµ‹è¯•å¹¶å‘è®¿é—®"""
        node = CacheNode(id="test_node")

        def worker(worker_id):
            for i in range(10):
                key = f"worker_{worker_id}_key_{i}"
                value = f"worker_{worker_id}_value_{i}"
                entry = CacheEntry(key=key, value=value)
                node.put(key, entry)
                retrieved = node.get(key)
                assert retrieved == value

        # åˆ›å»ºå¤šä¸ªçº¿ç¨‹å¹¶å‘è®¿é—®
        threads = []
        for i in range(5):
            thread = threading.Thread(target=worker, args=(i,))
            threads.append(thread)
            thread.start()

        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()

        # éªŒè¯æ‰€æœ‰æ•°æ®éƒ½æ­£ç¡®å­˜å‚¨
        assert len(node.storage) == 50  # 5 workers * 10 entries each

    def test_cache_node_edge_cases(self):
        """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
        node = CacheNode(id="test_node")

        # æµ‹è¯•ç©ºå€¼å¤„ç†
        entry = CacheEntry(key="null_key", value=None)
        node.put("null_key", entry)
        assert node.get("null_key") is None

        # æµ‹è¯•ç©ºå­—ç¬¦ä¸²é”®
        entry = CacheEntry(key="", value="empty_key_value")
        node.put("", entry)
        assert node.get("") == "empty_key_value"

        # æµ‹è¯•ç‰¹æ®Šå­—ç¬¦é”®
        special_key = "key_with_special_chars_!@#$%^&*()"
        entry = CacheEntry(key=special_key, value="special_value")
        node.put(special_key, entry)
        assert node.get(special_key) == "special_value"


class TestCacheConsistencyAdvanced:
    """æµ‹è¯•ç¼“å­˜ä¸€è‡´æ€§çš„é«˜çº§åŠŸèƒ½"""

    def test_conflict_resolution_version_wins_complex(self):
        """æµ‹è¯•ç‰ˆæœ¬å†²çªè§£å†³çš„å¤æ‚æƒ…å†µ"""
        consistency = CacheConsistency(ConsistencyLevel.STRONG)
        consistency.set_conflict_resolver(ConflictResolution.VERSION_WINS)

        # åˆ›å»ºå¤šä¸ªä¸åŒç‰ˆæœ¬çš„æ¡ç›®
        entries = [
            CacheEntry(key="test_key", value="v1", version=1),
            CacheEntry(key="test_key", value="v2", version=3),
            CacheEntry(key="test_key", value="v3", version=2),
        ]

        resolved = consistency.resolve_conflict(*entries)
        assert resolved.value == "v2"  # ç‰ˆæœ¬å·æœ€é«˜çš„åº”è¯¥è·èƒœ
        assert resolved.version == 3

    def test_conflict_resolution_custom_merge(self):
        """æµ‹è¯•è‡ªå®šä¹‰åˆå¹¶ç­–ç•¥"""
        consistency = CacheConsistency()
        consistency.conflict_resolver = ConflictResolution.CUSTOM_MERGE

        # åˆ›å»ºå¤šä¸ªæ¡ç›®
        entries = [
            CacheEntry(key="test_key", value={"count": 1}),
            CacheEntry(key="test_key", value={"count": 2}),
        ]

        resolved = consistency.resolve_conflict(*entries)
        # è‡ªå®šä¹‰åˆå¹¶åº”è¯¥è¿”å›ç¬¬ä¸€ä¸ªæ¡ç›®ï¼ˆé»˜è®¤å®ç°ï¼‰
        assert resolved.value["count"] == 1

    def test_operation_propagation_with_failures(self):
        """æµ‹è¯•æ“ä½œä¼ æ’­æ—¶çš„æ•…éšœå¤„ç†"""
        consistency = CacheConsistency()

        # åˆ›å»ºæ¨¡æ‹ŸèŠ‚ç‚¹ï¼Œå…¶ä¸­ä¸€ä¸ªä¼šå¤±è´¥
        node1 = Mock()
        node1.id = "node1"
        node1.apply_operation = AsyncMock(return_value=True)

        node2 = Mock()
        node2.id = "node2"
        node2.apply_operation = AsyncMock(side_effect=Exception("Node failure"))

        node3 = Mock()
        node3.id = "node3"
        node3.apply_operation = AsyncMock(return_value=True)

        operation = CacheOperation(
            operation_type="SET",
            key="test_key",
            value="test_value",
            node_id="source_node"
        )

        # ä¼ æ’­æ“ä½œåº”è¯¥å¤„ç†éƒ¨åˆ†å¤±è´¥
        results = asyncio.run(consistency.propagate_operation_to_nodes(
            operation, [node1, node2, node3]
        ))

        # åº”è¯¥æœ‰æˆåŠŸå’Œå¤±è´¥çš„ç»“æœ
        assert len(results) == 3
        assert results[0] is True  # node1 æˆåŠŸ
        assert results[1] is False  # node2 å¤±è´¥
        assert results[2] is True  # node3 æˆåŠŸ


class TestCacheReplicationAdvanced:
    """æµ‹è¯•ç¼“å­˜å¤åˆ¶çš„é«˜çº§åŠŸèƒ½"""

    def test_replication_with_unhealthy_nodes(self):
        """æµ‹è¯•åŒ…å«ä¸å¥åº·èŠ‚ç‚¹çš„å¤åˆ¶"""
        replication = CacheReplication(replication_factor=2)

        # åˆ›å»ºèŠ‚ç‚¹ï¼Œå…¶ä¸­ä¸€äº›æ˜¯ä¸å¥åº·çš„
        healthy_nodes = []
        for i in range(5):
            node = Mock()
            node.id = f"node_{i}"
            node.status = "active" if i < 3 else "inactive"
            node.apply_operation = Mock(return_value=True)
            healthy_nodes.append(node)

        # é€‰æ‹©å¤åˆ¶èŠ‚ç‚¹åº”è¯¥åªé€‰æ‹©å¥åº·èŠ‚ç‚¹
        selected = replication.select_replication_nodes(
            "test_key", healthy_nodes[:3], "source_node"
        )

        # åº”è¯¥é€‰æ‹©è¶³å¤Ÿçš„å¥åº·èŠ‚ç‚¹
        assert len(selected) <= min(2, 2)  # æœ€å¤š2ä¸ªï¼Œä¸”ä¸è¶…è¿‡å¥åº·èŠ‚ç‚¹æ•°

    def test_replication_factor_edge_cases(self):
        """æµ‹è¯•å¤åˆ¶å› å­çš„è¾¹ç•Œæƒ…å†µ"""
        # æµ‹è¯•å¤åˆ¶å› å­ä¸º0ï¼ˆä¼šè¢«è°ƒæ•´ä¸º1ï¼‰
        replication = CacheReplication(replication_factor=0)
        assert replication.replication_factor == 1  # æœ€å°å€¼ä¸º1

        # æµ‹è¯•å¤åˆ¶å› å­å¤§äºå¯ç”¨èŠ‚ç‚¹æ•°
        replication = CacheReplication(replication_factor=10)
        node_ids = [f"node_{i}" for i in range(3)]

        selected = replication.select_replication_nodes(
            "test_key", node_ids, "node_0"  # source_nodeæ˜¯ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
        )
        # æœ€å¤šé€‰æ‹©å¯ç”¨èŠ‚ç‚¹æ•°ï¼ˆæ’é™¤æºèŠ‚ç‚¹åå‰©ä¸‹2ä¸ªï¼‰
        assert len(selected) <= 2


class TestCachePartitioningAdvanced:
    """æµ‹è¯•ç¼“å­˜åˆ†åŒºçš„é«˜çº§åŠŸèƒ½"""

    def test_partition_rebalance_with_failed_nodes(self):
        """æµ‹è¯•åŒ…å«æ•…éšœèŠ‚ç‚¹çš„é‡æ–°å¹³è¡¡"""
        partitioning = CachePartitioning(partition_count=16)

        # æ·»åŠ åˆå§‹èŠ‚ç‚¹å¹¶åˆ†é…åˆ†åŒº
        initial_nodes = [f"node_{i}" for i in range(4)]
        for i, node_id in enumerate(initial_nodes):
            for partition in range(partitioning.partition_count):
                if partition % len(initial_nodes) == i:
                    partitioning.assign_node_to_partition(node_id, partition)

        # æ¨¡æ‹Ÿä¸€äº›èŠ‚ç‚¹æ•…éšœ
        failed_nodes = ["node_1", "node_3"]
        active_nodes = ["node_0", "node_2"]

        # é‡æ–°å¹³è¡¡
        migration_plan = partitioning.rebalance(failed_nodes, active_nodes)

        # åº”è¯¥æœ‰è¿ç§»è®¡åˆ’
        assert isinstance(migration_plan, dict)
        # åªåº”è¯¥åŒ…å«æ´»è·ƒèŠ‚ç‚¹
        for partition, node_id in migration_plan.items():
            assert node_id in active_nodes

    def test_consistent_hashing_distribution(self):
        """æµ‹è¯•ä¸€è‡´æ€§å“ˆå¸Œçš„åˆ†å¸ƒå‡åŒ€æ€§"""
        partitioning = CachePartitioning(partition_count=100)
        partitioning.partition_strategy = "consistent_hash"

        # æ·»åŠ å¤šä¸ªèŠ‚ç‚¹å¹¶åˆ†é…åˆ†åŒº
        nodes = [f"node_{i}" for i in range(10)]
        for i, node_id in enumerate(nodes):
            for partition in range(partitioning.partition_count):
                if partition % len(nodes) == i:
                    partitioning.assign_node_to_partition(node_id, partition)

        # æµ‹è¯•å¤§é‡é”®çš„åˆ†å¸ƒ
        key_distribution = {node_id: 0 for node_id in nodes}

        empty_count = 0
        for i in range(1000):
            key = f"test_key_{i}"
            nodes = partitioning.get_nodes_for_key(key)
            if not nodes:
                empty_count += 1
                continue
            primary_node = list(nodes)[0] if nodes else None
            if primary_node in key_distribution:
                key_distribution[primary_node] += 1

        # éªŒè¯åˆ†å¸ƒç›¸å¯¹å‡åŒ€ï¼ˆå…è®¸ä¸€å®šåå·®ï¼‰
        total_keys = sum(key_distribution.values())
        actual_nodes = [node_id for node_id, count in key_distribution.items() if count > 0]
        expected_per_node = total_keys / len(actual_nodes)

        for node_id, count in key_distribution.items():
            # å…è®¸50%çš„åå·®ï¼ˆå› ä¸ºåˆ†åŒºå¯èƒ½åˆ†å¸ƒä¸å‡ï¼‰
            assert count >= expected_per_node * 0.5
            assert count <= expected_per_node * 1.5


class TestDistributedCacheEdgeCases:
    """æµ‹è¯•åˆ†å¸ƒå¼ç¼“å­˜çš„è¾¹ç•Œæƒ…å†µ"""

    def test_cache_with_extreme_values(self):
        """æµ‹è¯•æç«¯å€¼çš„ç¼“å­˜å¤„ç†"""
        cache = DistributedCache("cluster", "test_node")

        # æµ‹è¯•éå¸¸å¤§çš„å€¼
        large_value = "x" * 1000000  # 1MBçš„å­—ç¬¦ä¸²
        result = cache.set("large_key", large_value)
        assert result is True

        retrieved = cache.get("large_key")
        assert retrieved == large_value

        # æµ‹è¯•Unicodeå­—ç¬¦
        unicode_value = "æµ‹è¯•ğŸš€Unicodeå†…å®¹"
        result = cache.set("unicode_key", unicode_value)
        assert result is True

        retrieved = cache.get("unicode_key")
        assert retrieved == unicode_value

    def test_cache_with_complex_data_structures(self):
        """æµ‹è¯•å¤æ‚æ•°æ®ç»“æ„çš„ç¼“å­˜"""
        cache = DistributedCache("cluster", "test_node")

        # æµ‹è¯•åµŒå¥—å­—å…¸
        complex_dict = {
            "level1": {
                "level2": {
                    "level3": [1, 2, 3, {"nested": "value"}]
                }
            },
            "array": [{"item": i} for i in range(10)]
        }

        cache.set("complex_dict", complex_dict)
        retrieved = cache.get("complex_dict")
        assert retrieved == complex_dict

        # æµ‹è¯•è‡ªå®šä¹‰å¯¹è±¡
        class CustomObject:
            def __init__(self, value):
                self.value = value
                self.timestamp = time.time()

            def __eq__(self, other):
                return isinstance(other, CustomObject) and self.value == other.value

        custom_obj = CustomObject("test_value")
        cache.set("custom_obj", custom_obj)
        retrieved = cache.get("custom_obj")
        assert retrieved.value == custom_obj.value

    def test_cache_concurrent_stress_test(self):
        """æµ‹è¯•é«˜å¹¶å‘å‹åŠ›"""
        cache = DistributedCache("cluster", "test_node")

        def stress_worker(worker_id):
            results = []
            for i in range(100):
                key = f"worker_{worker_id}_key_{i}"
                value = f"worker_{worker_id}_value_{i}"

                # å†™å…¥
                set_result = cache.set(key, value)
                results.append(('set', key, set_result))

                # è¯»å–
                get_result = cache.get(key)
                results.append(('get', key, get_result == value))

            return results

        # å¯åŠ¨å¤šä¸ªå¹¶å‘å·¥ä½œçº¿ç¨‹
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(stress_worker, i) for i in range(10)]
            all_results = []
            for future in concurrent.futures.as_completed(futures):
                all_results.extend(future.result())

        # éªŒè¯æ‰€æœ‰æ“ä½œéƒ½æˆåŠŸ
        set_operations = [r for r in all_results if r[0] == 'set']
        get_operations = [r for r in all_results if r[0] == 'get']

        assert all(r[2] for r in set_operations)  # æ‰€æœ‰setæ“ä½œæˆåŠŸ
        assert all(r[2] for r in get_operations)  # æ‰€æœ‰getæ“ä½œæˆåŠŸ

        # éªŒè¯æ•°æ®ä¸€è‡´æ€§
        final_stats = cache.get_metrics()
        assert final_stats['hit_count'] >= 900  # è‡³å°‘å¤§éƒ¨åˆ†getå‘½ä¸­
        assert final_stats['total_operations'] >= 2000  # è‡³å°‘æ‰§è¡Œäº†2000ä¸ªæ“ä½œ

    def test_cache_persistence_edge_cases(self):
        """æµ‹è¯•æŒä¹…åŒ–çš„è¾¹ç•Œæƒ…å†µ"""
        cache = DistributedCache("cluster", "test_node", enable_persistence=True)

        # æµ‹è¯•ç©ºç¼“å­˜çš„æŒä¹…åŒ–
        empty_data = cache.save_to_dict()
        assert isinstance(empty_data, dict)

        # æµ‹è¯•å¤§é‡æ•°æ®çš„æŒä¹…åŒ–
        for i in range(1000):
            cache.set(f"persist_key_{i}", f"persist_value_{i}")

        data = cache.save_to_dict()
        assert 'storage' in data
        assert len(data['storage']) == 1000

        # æµ‹è¯•ä»ä¿å­˜çš„æ•°æ®åŠ è½½
        new_cache = DistributedCache.load_from_dict(data)
        assert new_cache.get("persist_key_0") == "persist_value_0"
        assert new_cache.get("persist_key_999") == "persist_value_999"
        assert new_cache.get("nonexistent_key") is None

    def test_cache_health_monitoring_under_stress(self):
        """æµ‹è¯•å‹åŠ›ä¸‹çš„å¥åº·ç›‘æ§"""
        cache = DistributedCache("cluster", "test_node")

        # æ‰§è¡Œå¤§é‡æ“ä½œ
        for i in range(500):
            cache.set(f"stress_key_{i}", f"stress_value_{i}")
            if i % 2 == 0:
                cache.get(f"stress_key_{i}")

        # æ£€æŸ¥å¥åº·çŠ¶æ€
        health = cache.check_node_health()
        assert health['is_healthy'] is True
        assert health['cache_size'] == 500
        assert health['hit_rate'] >= 0.5  # è‡³å°‘50%å‘½ä¸­ç‡

        # æ£€æŸ¥æ€§èƒ½æŒ‡æ ‡
        performance = cache.get_performance_metrics()
        assert performance['total_operations'] >= 750  # 500 sets + 250 gets
        assert performance['operations_per_second'] > 0
        assert performance['memory_efficiency'] > 0