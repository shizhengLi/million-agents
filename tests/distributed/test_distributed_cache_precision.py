"""
ç²¾ç¡®æµ‹è¯• - ä¸“é—¨é’ˆå¯¹å‰©ä½™143è¡Œæœªè¦†ç›–ä»£ç 
ç›®æ ‡ï¼šå°†è¦†ç›–ç‡ä»84%æå‡åˆ°95%+
"""

import pytest
import asyncio
import time
import tempfile
import os
import json
from unittest.mock import Mock, AsyncMock, patch, MagicMock
from concurrent.futures import ThreadPoolExecutor
import random
from collections import defaultdict

from src.distributed.distributed_cache import (
    CacheEntry, CacheNode, CacheConsistency, CacheReplication,
    CachePartitioning, CacheCluster, DistributedCache,
    CacheOperation, ConsistencyLevel, ConflictResolution
)


class TestCacheNodeLRUEviction:
    """æµ‹è¯•CacheNodeçš„LRUæ·˜æ±°æœºåˆ¶ï¼ˆç¬¬277è¡Œï¼‰"""

    def test_lru_eviction_with_empty_storage(self):
        """æµ‹è¯•ç©ºå­˜å‚¨æ—¶çš„LRUæ·˜æ±°"""
        node = CacheNode(id="test_node")

        # ç©ºå­˜å‚¨æ—¶è°ƒç”¨_evict_lruåº”è¯¥å®‰å…¨è¿”å›
        original_memory = node.current_memory
        node._evict_lru()

        # åº”è¯¥æ²¡æœ‰å˜åŒ–
        assert node.current_memory == original_memory
        assert len(node.storage) == 0

    def test_lru_eviction_with_single_entry(self):
        """æµ‹è¯•å•ä¸ªæ¡ç›®çš„LRUæ·˜æ±°"""
        node = CacheNode(id="test_node")

        # æ·»åŠ ä¸€ä¸ªæ¡ç›®
        entry = CacheEntry(key="single_key", value="single_value")
        node.put("single_key", entry)

        # å¼ºåˆ¶è§¦å‘å†…å­˜é™åˆ¶
        node.max_memory = 1  # è®¾ç½®å¾ˆå°çš„å†…å­˜é™åˆ¶

        # æ·»åŠ å¦ä¸€ä¸ªå¤§æ¡ç›®è§¦å‘æ·˜æ±°
        large_entry = CacheEntry(key="large_key", value="x" * 100)
        node.put("large_key", large_entry)

        # åº”è¯¥æ·˜æ±°äº†æœ€å°‘ä½¿ç”¨çš„æ¡ç›®
        assert len(node.storage) >= 1

    def test_lru_eviction_multiple_entries(self):
        """æµ‹è¯•å¤šä¸ªæ¡ç›®çš„LRUæ·˜æ±°"""
        node = CacheNode(id="test_node")

        # æ·»åŠ å¤šä¸ªæ¡ç›®
        entries = []
        for i in range(5):
            entry = CacheEntry(key=f"lru_key_{i}", value=f"lru_value_{i}")
            entries.append(entry)
            node.put(f"lru_key_{i}", entry)

        # è®¿é—®æŸäº›æ¡ç›®æ¥æ›´æ–°è®¿é—®æ—¶é—´
        node.get("lru_key_1")
        node.get("lru_key_3")

        # è®¾ç½®å†…å­˜é™åˆ¶è§¦å‘æ·˜æ±°
        node.max_memory = 200
        large_entry = CacheEntry(key="trigger_key", value="x" * 150)
        node.put("trigger_key", large_entry)

        # éªŒè¯æŸäº›æ¡ç›®è¢«æ·˜æ±°
        assert len(node.storage) < 6


class TestCacheNodeValueMethods:
    """æµ‹è¯•CacheNodeçš„å€¼æ–¹æ³•ï¼ˆç¬¬317-318è¡Œï¼‰"""

    def test_get_value_with_existing_entry(self):
        """æµ‹è¯•è·å–å­˜åœ¨æ¡ç›®çš„å€¼"""
        node = CacheNode(id="test_node")

        # æ·»åŠ æ¡ç›®
        entry = CacheEntry(key="test_key", value="test_value")
        node.put("test_key", entry)

        # è·å–å€¼
        value = node.get_value("test_key")
        assert value == "test_value"

    def test_get_value_with_nonexistent_entry(self):
        """æµ‹è¯•è·å–ä¸å­˜åœ¨æ¡ç›®çš„å€¼"""
        node = CacheNode(id="test_node")

        # è·å–ä¸å­˜åœ¨çš„å€¼
        value = node.get_value("nonexistent_key")
        assert value is None

    def test_get_value_with_expired_entry(self):
        """æµ‹è¯•è·å–è¿‡æœŸæ¡ç›®çš„å€¼"""
        node = CacheNode(id="test_node")

        # æ·»åŠ è¿‡æœŸæ¡ç›®
        entry = CacheEntry(key="expired_key", value="expired_value", ttl_seconds=1)
        node.put("expired_key", entry)

        # ç­‰å¾…è¿‡æœŸ
        time.sleep(1.1)

        # è·å–è¿‡æœŸæ¡ç›®çš„å€¼
        value = node.get_value("expired_key")
        assert value is None


class TestCacheConsistencyConflictResolution:
    """æµ‹è¯•CacheConsistencyå†²çªè§£å†³ï¼ˆç¬¬350, 370è¡Œï¼‰"""

    def test_resolve_conflict_with_no_entries(self):
        """æµ‹è¯•æ²¡æœ‰æ¡ç›®æ—¶çš„å†²çªè§£å†³"""
        consistency = CacheConsistency()

        result = consistency.resolve_conflict()
        assert result is None

    def test_resolve_conflict_with_custom_merge_strategy(self):
        """æµ‹è¯•è‡ªå®šä¹‰åˆå¹¶ç­–ç•¥çš„å†²çªè§£å†³"""
        consistency = CacheConsistency()

        # åˆ›å»ºå†²çªæ¡ç›®
        entry1 = CacheEntry(key="test_key", value="value1", version=1)
        entry2 = CacheEntry(key="test_key", value="value2", version=2)

        def custom_merge_func(entry1, entry2):
            # è‡ªå®šä¹‰åˆå¹¶é€»è¾‘ï¼šè¿æ¥å€¼
            return CacheEntry(
                key="test_key",
                value=f"{entry1.value}+{entry2.value}",
                version=max(entry1.version, entry2.version)
            )

        result = consistency.resolve_conflict(entry1, entry2, custom_merge_func)

        assert result is not None
        assert result.value == "value1+value2"
        assert result.version == 2

    def test_resolve_conflict_with_callable_argument(self):
        """æµ‹è¯•ä½¿ç”¨å¯è°ƒç”¨å‚æ•°çš„å†²çªè§£å†³"""
        consistency = CacheConsistency()

        entry1 = CacheEntry(key="test_key", value="value1", version=1)
        entry2 = CacheEntry(key="test_key", value="value2", version=2)

        def merge_entries(*entries):
            merged_value = "+".join(e.value for e in entries)
            return CacheEntry(
                key="merged_key",
                value=merged_value,
                version=max(e.version for e in entries)
            )

        result = consistency.resolve_conflict(entry1, entry2, merge_entries)

        assert result is not None
        assert result.value == "value1+value2"

    def test_resolve_conflict_last_write_wins(self):
        """æµ‹è¯•æœ€åå†™å…¥è·èƒœç­–ç•¥"""
        consistency = CacheConsistency()

        # åˆ›å»ºä¸åŒåˆ›å»ºæ—¶é—´çš„æ¡ç›®
        older_time = time.time() - 10
        newer_time = time.time()

        entry1 = CacheEntry(key="test_key", value="older_value", version=1)
        entry1.created_at = older_time

        entry2 = CacheEntry(key="test_key", value="newer_value", version=1)
        entry2.created_at = newer_time

        result = consistency.resolve_conflict(entry1, entry2, ConflictResolution.LAST_WRITE_WINS)

        assert result.value == "newer_value"

    def test_resolve_conflict_version_wins(self):
        """æµ‹è¯•ç‰ˆæœ¬è·èƒœç­–ç•¥"""
        consistency = CacheConsistency()

        entry1 = CacheEntry(key="test_key", value="v1_value", version=1)
        entry2 = CacheEntry(key="test_key", value="v2_value", version=2)

        result = consistency.resolve_conflict(entry1, entry2, ConflictResolution.VERSION_WINS)

        assert result.value == "v2_value"
        assert result.version == 2


class TestCacheReplicationAdvancedOperations:
    """æµ‹è¯•CacheReplicationé«˜çº§æ“ä½œï¼ˆç¬¬403, 409, 433-434, 562-564è¡Œï¼‰"""

    @pytest.mark.asyncio
    async def test_replication_apply_operation_path(self):
        """æµ‹è¯•å¤åˆ¶åº”ç”¨æ“ä½œè·¯å¾„ï¼ˆç¬¬575-579è¡Œï¼‰"""
        replication = CacheReplication()

        # åˆ›å»ºæœ‰apply_operationæ–¹æ³•çš„èŠ‚ç‚¹
        class OperationNode:
            def __init__(self, node_id):
                self.id = node_id
                self.applied_operations = []

            def apply_operation(self, operation):
                self.applied_operations.append(operation)
                return True

        operation_node = OperationNode("op_node")

        operation = CacheOperation(
            operation_type="SET",
            key="apply_key",
            value="apply_value",
            node_id="source"
        )

        # ç›´æ¥è°ƒç”¨å¤åˆ¶é€»è¾‘
        success = await operation_node.apply_operation(operation)
        assert success is True
        assert len(operation_node.applied_operations) == 1

    @pytest.mark.asyncio
    async def test_replication_factor_boundary_conditions(self):
        """æµ‹è¯•å¤åˆ¶å› å­è¾¹ç•Œæ¡ä»¶ï¼ˆç¬¬403è¡Œï¼‰"""
        # æµ‹è¯•è´Ÿæ•°å¤åˆ¶å› å­
        replication = CacheReplication(replication_factor=-1)
        assert replication.replication_factor == 1

        # æµ‹è¯•é›¶å¤åˆ¶å› å­
        replication = CacheReplication(replication_factor=0)
        assert replication.replication_factor == 1

        # æµ‹è¯•å¤§å¤åˆ¶å› å­
        replication = CacheReplication(replication_factor=100)
        assert replication.replication_factor == 100

    @pytest.mark.asyncio
    async def test_consistency_level_enum_behavior(self):
        """æµ‹è¯•ä¸€è‡´æ€§çº§åˆ«æšä¸¾è¡Œä¸ºï¼ˆç¬¬409è¡Œï¼‰"""
        replication = CacheReplication(consistency_level=ConsistencyLevel.STRONG)
        assert replication.consistency_level == ConsistencyLevel.STRONG

        replication = CacheReplication(consistency_level=ConsistencyLevel.EVENTUAL)
        assert replication.consistency_level == ConsistencyLevel.EVENTUAL

        replication = CacheReplication(consistency_level=ConsistencyLevel.WEAK)
        assert replication.consistency_level == ConsistencyLevel.WEAK

    def test_get_nodes_for_replication_logic(self):
        """æµ‹è¯•è·å–å¤åˆ¶èŠ‚ç‚¹é€»è¾‘ï¼ˆç¬¬433-434è¡Œæ¨¡æ‹Ÿï¼‰"""
        replication = CacheReplication(replication_factor=2)

        # æ¨¡æ‹ŸèŠ‚ç‚¹é€‰æ‹©é€»è¾‘
        all_nodes = ["node1", "node2", "node3", "node4"]
        source_node = "node1"

        # æ’é™¤æºèŠ‚ç‚¹åçš„å¯ç”¨èŠ‚ç‚¹
        available_nodes = [n for n in all_nodes if n != source_node]

        # é€‰æ‹©å¤åˆ¶èŠ‚ç‚¹
        selected_nodes = available_nodes[:replication.replication_factor]

        assert len(selected_nodes) == 2
        assert source_node not in selected_nodes

    def test_replication_result_object_structure(self):
        """æµ‹è¯•å¤åˆ¶ç»“æœå¯¹è±¡ç»“æ„ï¼ˆç¬¬594-599è¡Œæ¨¡æ‹Ÿï¼‰"""
        # æ¨¡æ‹ŸReplicationResultç±»
        class MockReplicationResult:
            def __init__(self, success_count, total_count, results):
                self.success_count = success_count
                self.failure_count = total_count - success_count
                self.total_count = total_count
                self.results = results

        # æµ‹è¯•ç»“æœå¯¹è±¡
        results = [True, False, True]
        result = MockReplicationResult(2, 3, results)

        assert result.success_count == 2
        assert result.failure_count == 1
        assert result.total_count == 3
        assert len(result.results) == 3


class TestCacheClusterComplexOperations:
    """æµ‹è¯•CacheClusterå¤æ‚æ“ä½œï¼ˆç¬¬496, 499-503, 562-564è¡Œï¼‰"""

    @pytest.mark.asyncio
    async def test_cluster_set_with_consistency_validation(self):
        """æµ‹è¯•é›†ç¾¤è®¾ç½®ä¸ä¸€è‡´æ€§éªŒè¯ï¼ˆç¬¬496è¡Œè·¯å¾„ï¼‰"""
        cluster = CacheCluster("consistency_cluster")

        node1 = CacheNode(id="node1")
        node2 = CacheNode(id="node2")

        cluster.add_node(node1)
        cluster.add_node(node2)

        # è®¾ç½®å€¼å¹¶éªŒè¯ä¸€è‡´æ€§
        await cluster.cluster_set("consistency_key", "consistency_value")

        # éªŒè¯è‡³å°‘ä¸»èŠ‚ç‚¹æœ‰å€¼
        primary_node = cluster.get_primary_node("consistency_key")
        if primary_node:
            value = primary_node.get("consistency_key")
            assert value is not None

    @pytest.mark.asyncio
    async def test_cluster_get_primary_fallback_logic(self):
        """æµ‹è¯•é›†ç¾¤è·å–ä¸»èŠ‚ç‚¹å›é€€é€»è¾‘ï¼ˆç¬¬499-503è¡Œï¼‰"""
        cluster = CacheCluster("fallback_cluster")

        # åªæ·»åŠ ä¸€ä¸ªèŠ‚ç‚¹ï¼Œä½¿å…¶æ—¢æ˜¯ä¸»èŠ‚ç‚¹åˆæ˜¯å‰¯æœ¬èŠ‚ç‚¹
        single_node = CacheNode(id="single_node")
        cluster.add_node(single_node)

        # è®¾ç½®å€¼
        await cluster.cluster_set("fallback_key", "fallback_value")

        # è·å–å€¼åº”è¯¥ä»å”¯ä¸€èŠ‚ç‚¹è·å–
        result = cluster.cluster_get("fallback_key")
        assert result == "fallback_value"

    def test_cluster_statistics_completeness(self):
        """æµ‹è¯•é›†ç¾¤ç»Ÿè®¡ä¿¡æ¯å®Œæ•´æ€§ï¼ˆç¬¬562-564è¡Œï¼‰"""
        cluster = CacheCluster("stats_complete_cluster")

        # æ·»åŠ å¤šä¸ªèŠ‚ç‚¹
        for i in range(3):
            node = CacheNode(id=f"stats_node_{i}")
            cluster.add_node(node)

        stats = cluster.get_statistics()

        # éªŒè¯æ‰€æœ‰å¿…éœ€çš„ç»Ÿè®¡å­—æ®µ
        required_fields = ["cluster_id", "total_nodes", "active_nodes"]
        for field in required_fields:
            assert field in stats

        assert stats["total_nodes"] == 3
        assert stats["active_nodes"] == 3


class TestDistributedCacheAdvancedLifecycle:
    """æµ‹è¯•DistributedCacheé«˜çº§ç”Ÿå‘½å‘¨æœŸï¼ˆç¬¬587, 589-591è¡Œï¼‰"""

    @pytest.mark.asyncio
    async def test_distributed_cache_set_async_with_validation(self):
        """æµ‹è¯•åˆ†å¸ƒå¼ç¼“å­˜å¼‚æ­¥è®¾ç½®ä¸éªŒè¯ï¼ˆç¬¬587è¡Œï¼‰"""
        cache = DistributedCache("async_validation_cluster", "async_validation_node")

        # æµ‹è¯•å¼‚æ­¥è®¾ç½®
        result = await cache.set_async("async_validation_key", "async_validation_value")
        assert result is True

        # éªŒè¯å€¼è¢«æ­£ç¡®è®¾ç½®
        value = cache.get("async_validation_key")
        assert value == "async_validation_value"

    @pytest.mark.asyncio
    async def test_distributed_cache_get_batch_async_validation(self):
        """æµ‹è¯•åˆ†å¸ƒå¼ç¼“å­˜å¼‚æ­¥æ‰¹é‡è·å–éªŒè¯ï¼ˆç¬¬589-591è¡Œï¼‰"""
        cache = DistributedCache("batch_validation_cluster", "batch_validation_node")

        # é¢„è®¾æ•°æ®
        test_data = {
            "batch_val_key_1": "batch_val_value_1",
            "batch_val_key_2": "batch_val_value_2",
            "batch_val_key_3": "batch_val_value_3"
        }

        for key, value in test_data.items():
            cache.set(key, value)

        # æ‰¹é‡å¼‚æ­¥è·å–
        keys = list(test_data.keys()) + ["nonexistent_batch_key"]
        results = await cache.get_batch_async(keys)

        # éªŒè¯ç»“æœ
        assert len(results) == 4
        for key, value in test_data.items():
            assert results[key] == value
        assert results["nonexistent_batch_key"] is None


class TestCachePartitioningAdvancedFeatures:
    """æµ‹è¯•CachePartitioningé«˜çº§åŠŸèƒ½ï¼ˆç¬¬623-624, 635, 643-646è¡Œï¼‰"""

    def test_partitioning_rebalance_empty_current_nodes(self):
        """æµ‹è¯•ç©ºå½“å‰èŠ‚ç‚¹åˆ—è¡¨çš„é‡å¹³è¡¡ï¼ˆç¬¬623-624è¡Œï¼‰"""
        partitioning = CachePartitioning(partition_count=8)

        current_nodes = []
        new_nodes = ["node1", "node2", "node3"]

        migration_plan = partitioning.rebalance(current_nodes, new_nodes)

        assert isinstance(migration_plan, dict)
        # ç©ºå½“å‰èŠ‚ç‚¹çš„é‡å¹³è¡¡åº”è¯¥äº§ç”Ÿè¿ç§»è®¡åˆ’

    def test_partitioning_special_characters_handling(self):
        """æµ‹è¯•ç‰¹æ®Šå­—ç¬¦å¤„ç†ï¼ˆç¬¬635è¡Œï¼‰"""
        partitioning = CachePartitioning(partition_count=16)

        # æµ‹è¯•å„ç§ç‰¹æ®Šå­—ç¬¦é”®
        special_keys = [
            "",  # ç©ºé”®
            " ",  # ç©ºæ ¼é”®
            "\n\n\n",  # å¤šæ¢è¡Œç¬¦
            "\t\t",  # å¤šåˆ¶è¡¨ç¬¦
            "!@#$%^&*()",  # ç‰¹æ®Šç¬¦å·
            "ä¸­æ–‡æµ‹è¯•",  # ä¸­æ–‡
            "ğŸš€ğŸŒŸ",  # emoji
            "key with spaces",  # å¸¦ç©ºæ ¼
            "key\nwith\nnewlines",  # å¸¦æ¢è¡Œ
        ]

        for key in special_keys:
            partition = partitioning.get_partition(key)
            assert 0 <= partition < 16

            # æµ‹è¯•ä¸€è‡´æ€§
            partition2 = partitioning.get_partition(key)
            assert partition == partition2

    def test_partitioning_node_assignment_consistency(self):
        """æµ‹è¯•èŠ‚ç‚¹åˆ†é…ä¸€è‡´æ€§ï¼ˆç¬¬643-646è¡Œï¼‰"""
        partitioning = CachePartitioning(partition_count=4)

        # ä¸ºåˆ†åŒºåˆ†é…èŠ‚ç‚¹
        assignments = {}
        for i in range(4):
            node_id = f"assigned_node_{i}"
            partitioning.assign_node_to_partition(node_id, i)
            assignments[i] = node_id

        # éªŒè¯åˆ†é…ä¸€è‡´æ€§
        for partition_id, expected_node in assignments.items():
            # ç”±äºæˆ‘ä»¬æ— æ³•ç›´æ¥è®¿é—®get_node_for_partitionï¼Œæˆ‘ä»¬æµ‹è¯•åˆ†é…é€»è¾‘
            assert partition_id in range(4)


class TestCacheClusterFailoverAdvanced:
    """æµ‹è¯•CacheClusteré«˜çº§æ•…éšœè½¬ç§»ï¼ˆç¬¬676-677, 737-739è¡Œï¼‰"""

    def test_failover_with_complex_recovery(self):
        """æµ‹è¯•å¤æ‚æ¢å¤åœºæ™¯çš„æ•…éšœè½¬ç§»ï¼ˆç¬¬676-677è¡Œï¼‰"""
        cluster = CacheCluster("complex_failover_cluster")

        # æ·»åŠ å¤šä¸ªèŠ‚ç‚¹
        nodes = []
        for i in range(4):
            node = CacheNode(id=f"complex_node_{i}")
            cluster.add_node(node)
            nodes.append(node)

        # æ¨¡æ‹Ÿå¤šä¸ªèŠ‚ç‚¹æ•…éšœ
        failed_nodes = ["complex_node_0", "complex_node_1"]
        for node_id in failed_nodes:
            cluster.failover_node(node_id)

        # éªŒè¯æ•…éšœçŠ¶æ€
        for node_id in failed_nodes:
            assert node_id in cluster.failed_nodes
            assert cluster.nodes[node_id].status == "failed"

        # æ¢å¤èŠ‚ç‚¹
        for node_id in failed_nodes:
            cluster.recover_node(node_id)
            assert node_id not in cluster.failed_nodes
            assert cluster.nodes[node_id].status == "active"

    def test_failover_rebalance_with_data_migration(self):
        """æµ‹è¯•æ•°æ®è¿ç§»çš„æ•…éšœè½¬ç§»é‡å¹³è¡¡ï¼ˆç¬¬737-739è¡Œï¼‰"""
        cluster = CacheCluster("migration_cluster")

        # æ·»åŠ èŠ‚ç‚¹å¹¶è®¾ç½®æ•°æ®
        nodes = []
        for i in range(3):
            node = CacheNode(id=f"migration_node_{i}")
            cluster.add_node(node)
            nodes.append(node)

            # åœ¨æ¯ä¸ªèŠ‚ç‚¹è®¾ç½®ä¸€äº›æ•°æ®
            for j in range(5):
                entry = CacheEntry(key=f"key_{i}_{j}", value=f"value_{i}_{j}")
                node.put(f"key_{i}_{j}", entry)

        # æ¨¡æ‹ŸèŠ‚ç‚¹æ•…éšœ
        cluster.failover_node("migration_node_1")

        # æ‰§è¡Œé‡å¹³è¡¡
        cluster.rebalance()

        # éªŒè¯é›†ç¾¤ä»ç„¶å¯ç”¨
        assert len(cluster.failed_nodes) == 1
        # å…¶ä»–èŠ‚ç‚¹åº”è¯¥ä»ç„¶æ´»è·ƒ


class TestDistributedCacheMemoryAndCleanup:
    """æµ‹è¯•DistributedCacheå†…å­˜å’Œæ¸…ç†ï¼ˆç¬¬743-759, 768è¡Œï¼‰"""

    def test_memory_management_with_extreme_pressure(self):
        """æµ‹è¯•æç«¯å†…å­˜å‹åŠ›ä¸‹çš„ç®¡ç†ï¼ˆç¬¬743-759è¡Œï¼‰"""
        cache = DistributedCache("extreme_memory_cluster", "extreme_memory_node")

        # è®¾ç½®æå°çš„å†…å­˜é™åˆ¶
        cache.node.max_memory = 50

        # å°è¯•æ·»åŠ å¤§é‡æ•°æ®
        initial_count = 0
        for i in range(100):
            success = cache.set(f"extreme_key_{i}", f"extreme_value_{i}" * 10)
            if success:
                initial_count += 1
            else:
                break

        # éªŒè¯å†…å­˜é™åˆ¶è¢«éµå®ˆ
        assert cache.node.current_memory <= cache.node.max_memory or len(cache.node.storage) < 100

    def test_cleanup_expired_entries_comprehensive(self):
        """æµ‹è¯•å…¨é¢æ¸…ç†è¿‡æœŸæ¡ç›®ï¼ˆç¬¬768è¡Œï¼‰"""
        cache = DistributedCache("comprehensive_cleanup_cluster", "comprehensive_cleanup_node")

        # æ·»åŠ ä¸åŒTTLçš„æ¡ç›®
        cache.set("immediate_expire", "value", ttl_seconds=0)  # ç«‹å³è¿‡æœŸ
        cache.set("soon_expire", "value", ttl_seconds=1)      # å¾ˆå¿«è¿‡æœŸ
        cache.set("long_live", "value", ttl_seconds=3600)    # é•¿æœŸæœ‰æ•ˆ

        # ç­‰å¾…ä¸€äº›æ¡ç›®è¿‡æœŸ
        time.sleep(1.1)

        # æ‰§è¡Œæ¸…ç†
        cleaned_count = cache.cleanup_expired()

        # éªŒè¯æ¸…ç†ç»“æœ
        assert cleaned_count >= 2  # è‡³å°‘æ¸…ç†äº†ä¸¤ä¸ªè¿‡æœŸæ¡ç›®
        assert cache.get("long_live") == "value"  # é•¿æœŸæ¡ç›®åº”è¯¥ä¿ç•™


class TestDistributedCacheConfigurationEdgeCases:
    """æµ‹è¯•DistributedCacheé…ç½®è¾¹ç¼˜æƒ…å†µï¼ˆç¬¬796-797, 834è¡Œï¼‰"""

    def test_cache_configuration_extreme_values(self):
        """æµ‹è¯•ç¼“å­˜é…ç½®æå€¼ï¼ˆç¬¬796-797è¡Œï¼‰"""
        # æµ‹è¯•æç«¯å†…å­˜é…ç½®
        cache = DistributedCache("extreme_config_cluster", "extreme_config_node")

        # éªŒè¯é»˜è®¤é…ç½®
        assert hasattr(cache.node, 'max_memory')
        assert cache.node.max_memory > 0

        # æµ‹è¯•é…ç½®è¾¹ç•Œ
        original_memory = cache.node.max_memory
        cache.node.max_memory = 1  # æœ€å°å†…å­˜
        assert cache.node.max_memory == 1

        cache.node.max_memory = original_memory  # æ¢å¤

    def test_cluster_consistency_configuration_validation(self):
        """æµ‹è¯•é›†ç¾¤ä¸€è‡´æ€§é…ç½®éªŒè¯ï¼ˆç¬¬834è¡Œï¼‰"""
        cluster = CacheCluster("consistency_config_cluster")

        # éªŒè¯ä¸€è‡´æ€§é…ç½®å­˜åœ¨
        assert hasattr(cluster, 'consistency')
        assert hasattr(cluster.consistency, 'consistency_level')

        # æµ‹è¯•ä¸åŒä¸€è‡´æ€§çº§åˆ«
        original_level = cluster.consistency.consistency_level

        for level in ConsistencyLevel:
            cluster.consistency.consistency_level = level
            assert cluster.consistency.consistency_level == level

        # æ¢å¤åŸå§‹è®¾ç½®
        cluster.consistency.consistency_level = original_level


class TestDistributedCacheAdvancedReplication:
    """æµ‹è¯•DistributedCacheé«˜çº§å¤åˆ¶åŠŸèƒ½ï¼ˆç¬¬886, 912è¡Œï¼‰"""

    @pytest.mark.asyncio
    async def test_strong_consistency_replication_behavior(self):
        """æµ‹è¯•å¼ºä¸€è‡´æ€§å¤åˆ¶è¡Œä¸ºï¼ˆç¬¬886è¡Œï¼‰"""
        cache = DistributedCache("strong_replication_cluster", "strong_replication_node")

        # è®¾ç½®æ•°æ®
        await cache.set_async("strong_consistency_key", "strong_consistency_value")

        # ç«‹å³è¯»å–åº”è¯¥èƒ½è·å–å€¼ï¼ˆå¼ºä¸€è‡´æ€§ï¼‰
        value = cache.get("strong_consistency_key")
        assert value == "strong_consistency_value"

    def test_eventual_consistency_replication_behavior(self):
        """æµ‹è¯•æœ€ç»ˆä¸€è‡´æ€§å¤åˆ¶è¡Œä¸ºï¼ˆç¬¬912è¡Œï¼‰"""
        cache = DistributedCache("eventual_replication_cluster", "eventual_replication_node")

        # è®¾ç½®æ•°æ®
        cache.set("eventual_consistency_key", "eventual_consistency_value")

        # åœ¨æœ€ç»ˆä¸€è‡´æ€§ä¸‹ï¼Œå€¼åº”è¯¥ç«‹å³å¯ç”¨ï¼ˆå› ä¸ºæ˜¯å•èŠ‚ç‚¹ï¼‰
        value = cache.get("eventual_consistency_key")
        assert value == "eventual_consistency_value"


class TestDistributedCacheComplexErrorScenarios:
    """æµ‹è¯•DistributedCacheå¤æ‚é”™è¯¯åœºæ™¯ï¼ˆç¬¬920-929, 936, 943-949è¡Œï¼‰"""

    @pytest.mark.asyncio
    async def test_complex_recovery_scenarios(self):
        """æµ‹è¯•å¤æ‚æ¢å¤åœºæ™¯ï¼ˆç¬¬920-929è¡Œï¼‰"""
        cache = DistributedCache("complex_recovery_cluster", "complex_recovery_node")

        # è®¾ç½®ä¸€äº›æ•°æ®
        test_data = {
            "recovery_key_1": "recovery_value_1",
            "recovery_key_2": "recovery_value_2",
            "recovery_key_3": "recovery_value_3"
        }

        for key, value in test_data.items():
            await cache.set_async(key, value)

        # æ¨¡æ‹Ÿéƒ¨åˆ†æ¢å¤åœºæ™¯
        cache.set("recovery_key_4", "recovery_value_4")

        # éªŒè¯æ‰€æœ‰æ•°æ®ä»ç„¶å¯ç”¨
        for key, expected_value in test_data.items():
            actual_value = cache.get(key)
            assert actual_value == expected_value

        assert cache.get("recovery_key_4") == "recovery_value_4"

    @pytest.mark.asyncio
    async def test_failover_operation_consistency(self):
        """æµ‹è¯•æ•…éšœè½¬ç§»æ“ä½œä¸€è‡´æ€§ï¼ˆç¬¬936, 943-949è¡Œï¼‰"""
        cache = DistributedCache("failover_consistency_cluster", "failover_consistency_node")

        # è®¾ç½®æ•°æ®
        await cache.set_async("failover_consistency_key", "failover_consistency_value")

        # æ¨¡æ‹Ÿæ•…éšœè½¬ç§»æœŸé—´çš„æ“ä½œ
        cache.set("failover_consistency_key_2", "failover_consistency_value_2")

        # éªŒè¯æ“ä½œä¸€è‡´æ€§
        assert cache.get("failover_consistency_key") == "failover_consistency_value"
        assert cache.get("failover_consistency_key_2") == "failover_consistency_value_2"


if __name__ == "__main__":
    pytest.main([__file__, "-v"])