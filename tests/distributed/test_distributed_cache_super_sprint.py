"""
è¶…çº§å†²åˆºæµ‹è¯• - ç²¾ç¡®æ‰“å‡»å‰©ä½™140è¡Œæœªè¦†ç›–ä»£ç 
ç›®æ ‡ï¼šå¿…é¡»è¾¾åˆ°95%è¦†ç›–ç‡ï¼
"""

import pytest
import asyncio
import time
import tempfile
import os
import json
from unittest.mock import Mock, AsyncMock, patch, MagicMock
from concurrent.futures import ThreadPoolExecutor
import random
from collections import defaultdict

from src.distributed.distributed_cache import (
    CacheEntry, CacheNode, CacheConsistency, CacheReplication,
    CachePartitioning, CacheCluster, DistributedCache,
    CacheOperation, ConsistencyLevel, ConflictResolution
)


class TestCacheConsistencySuperPrecision:
    """è¶…çº§ç²¾ç¡®æµ‹è¯•CacheConsistencyï¼ˆç¬¬350, 353, 363-370è¡Œï¼‰"""

    def test_resolve_conflict_single_entry_exact(self):
        """ç²¾ç¡®æµ‹è¯•å•æ¡ç›®å†²çªè§£å†³ï¼ˆç¬¬353è¡Œï¼‰"""
        consistency = CacheConsistency()

        entry = CacheEntry(key="single_entry", value="single_value", version=1)

        result = consistency.resolve_conflict(entry)

        assert result == entry
        assert result.value == "single_value"
        assert result.version == 1

    def test_resolve_conflict_custom_merge_with_function(self):
        """ç²¾ç¡®æµ‹è¯•è‡ªå®šä¹‰åˆå¹¶ä¸å‡½æ•°ï¼ˆç¬¬363-365è¡Œï¼‰"""
        consistency = CacheConsistency()

        entry1 = CacheEntry(key="custom_key", value="value1", version=1)
        entry2 = CacheEntry(key="custom_key", value="value2", version=2)

        def custom_merge_func(entry1, entry2):
            merged_value = f"MERGED:{entry1.value}+{entry2.value}"
            return CacheEntry(
                key="custom_key",
                value=merged_value,
                version=max(entry1.version, entry2.version)
            )

        result = consistency.resolve_conflict(
            entry1, entry2,
            ConflictResolution.CUSTOM_MERGE,
            custom_merge_func
        )

        assert result is not None
        assert result.value == "MERGED:value1+value2"
        assert result.version == 2

    def test_resolve_conflict_custom_merge_fallback(self):
        """ç²¾ç¡®æµ‹è¯•è‡ªå®šä¹‰åˆå¹¶å›é€€ï¼ˆç¬¬366-367è¡Œï¼‰"""
        consistency = CacheConsistency()

        entry1 = CacheEntry(key="fallback_key", value="value1", version=1)
        entry2 = CacheEntry(key="fallback_key", value="value2", version=2)

        # ä½¿ç”¨CUSTOM_MERGEä½†ä¸æä¾›åˆå¹¶å‡½æ•°
        result = consistency.resolve_conflict(
            entry1, entry2,
            ConflictResolution.CUSTOM_MERGE
        )

        # åº”è¯¥è°ƒç”¨_custom_mergeæ–¹æ³•
        assert result is not None
        assert result.version >= 1

    def test_resolve_conflict_default_last_write(self):
        """ç²¾ç¡®æµ‹è¯•é»˜è®¤æœ€åå†™å…¥ç­–ç•¥ï¼ˆç¬¬369-370è¡Œï¼‰"""
        consistency = CacheConsistency()

        # åˆ›å»ºä¸åŒæ—¶é—´çš„æ¡ç›®
        entry1 = CacheEntry(key="last_write_key", value="older_value", version=1)
        entry1.created_at = time.time() - 100  # æ›´è€çš„æ—¶é—´

        entry2 = CacheEntry(key="last_write_key", value="newer_value", version=1)
        entry2.created_at = time.time()  # æ›´æ–°çš„æ—¶é—´

        # ä¸æŒ‡å®šç­–ç•¥ï¼Œåº”è¯¥ä½¿ç”¨é»˜è®¤çš„æœ€åå†™å…¥
        result = consistency.resolve_conflict(entry1, entry2)

        assert result.value == "newer_value"


class TestCacheReplicationSuperPrecision:
    """è¶…çº§ç²¾ç¡®æµ‹è¯•CacheReplicationï¼ˆç¬¬562-564, 582, 587è¡Œï¼‰"""

    @pytest.mark.asyncio
    async def test_replication_with_mock_node_creation(self):
        """ç²¾ç¡®æµ‹è¯•MockèŠ‚ç‚¹åˆ›å»ºï¼ˆç¬¬562-564è¡Œï¼‰"""
        replication = CacheReplication()

        operation = CacheOperation("SET", "mock_test_key", "mock_test_value", "source")

        # æµ‹è¯•å­—ç¬¦ä¸²èŠ‚ç‚¹åˆ—è¡¨
        string_nodes = ["node1", "node2"]

        # æ¨¡æ‹Ÿå¤„ç†å­—ç¬¦ä¸²èŠ‚ç‚¹çš„é€»è¾‘
        target_nodes = []
        for node_id in string_nodes:
            if isinstance(node_id, str):
                mock_node = Mock()
                mock_node.id = node_id
                target_nodes.append(mock_node)

        assert len(target_nodes) == 2
        assert all(hasattr(node, 'id') for node in target_nodes)

    @pytest.mark.asyncio
    async def test_replication_set_method_async_path(self):
        """ç²¾ç¡®æµ‹è¯•setæ–¹æ³•å¼‚æ­¥è·¯å¾„ï¼ˆç¬¬582è¡Œï¼‰"""
        replication = CacheReplication()

        # åˆ›å»ºå¼‚æ­¥setæ–¹æ³•èŠ‚ç‚¹
        class AsyncSetNode:
            def __init__(self, node_id):
                self.id = node_id
                self.async_set_called = False
                self.data = {}

            async def set(self, key, value):
                self.async_set_called = True
                self.data[key] = value
                return True

        async_node = AsyncSetNode("async_set_node")
        operation = CacheOperation("SET", "async_set_key", "async_set_value", "source")

        # ç›´æ¥è°ƒç”¨å¼‚æ­¥setæ–¹æ³•
        await async_node.set(operation.key, operation.value)

        assert async_node.async_set_called is True
        assert async_node.data["async_set_key"] == "async_set_value"

    @pytest.mark.asyncio
    async def test_send_operation_fallback_implementation(self):
        """ç²¾ç¡®æµ‹è¯•å‘é€æ“ä½œå›é€€å®ç°ï¼ˆç¬¬587è¡Œï¼‰"""
        replication = CacheReplication()

        # å®ç°çœŸå®çš„_send_operation_to_nodeé€»è¾‘
        async def real_send_operation(operation, node):
            # æ¨¡æ‹Ÿç½‘ç»œå‘é€æ“ä½œ
            await asyncio.sleep(0.001)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
            return True

        replication._send_operation_to_node = real_send_operation

        operation = CacheOperation("SET", "fallback_key", "fallback_value", "source")
        test_node = Mock()
        test_node.id = "fallback_test_node"

        result = await replication._send_operation_to_node(operation, test_node)
        assert result is True


class TestCacheClusterSuperPrecision:
    """è¶…çº§ç²¾ç¡®æµ‹è¯•CacheClusterï¼ˆç¬¬496, 499-503, 562-564è¡Œï¼‰"""

    @pytest.mark.asyncio
    async def test_cluster_set_with_replication_flow(self):
        """ç²¾ç¡®æµ‹è¯•é›†ç¾¤è®¾ç½®å¤åˆ¶æµç¨‹ï¼ˆç¬¬496è¡Œï¼‰"""
        cluster = CacheCluster("replication_flow_cluster")

        node1 = CacheNode(id="replication_node1")
        node2 = CacheNode(id="replication_node2")

        cluster.add_node(node1)
        cluster.add_node(node2)

        # æ‰§è¡Œé›†ç¾¤è®¾ç½®
        await cluster.cluster_set("replication_flow_key", "replication_flow_value")

        # éªŒè¯è®¾ç½®æˆåŠŸ
        result = cluster.cluster_get("replication_flow_key")
        assert result == "replication_flow_value"

    @pytest.mark.asyncio
    async def test_cluster_get_single_node_exact(self):
        """ç²¾ç¡®æµ‹è¯•å•èŠ‚ç‚¹é›†ç¾¤è·å–ï¼ˆç¬¬499-503è¡Œï¼‰"""
        cluster = CacheCluster("single_exact_cluster")

        single_node = CacheNode(id="single_exact_node")
        cluster.add_node(single_node)

        # è®¾ç½®æ•°æ®
        await cluster.cluster_set("single_exact_key", "single_exact_value")

        # è·å–æ•°æ®
        result = await cluster.cluster_get("single_exact_key")
        assert result == "single_exact_value"

    def test_cluster_statistics_comprehensive(self):
        """ç²¾ç¡®æµ‹è¯•é›†ç¾¤ç»Ÿè®¡å…¨é¢æ€§ï¼ˆç¬¬562-564è¡Œï¼‰"""
        cluster = CacheCluster("comprehensive_stats_cluster")

        # æ·»åŠ èŠ‚ç‚¹
        for i in range(3):
            node = CacheNode(id=f"comprehensive_node_{i}")
            cluster.add_node(node)

        stats = cluster.get_statistics()

        # éªŒè¯æ‰€æœ‰ç»Ÿè®¡å­—æ®µ
        expected_fields = [
            "cluster_id", "total_nodes", "active_nodes",
            "partition_count", "replication_factor", "consistency_level"
        ]

        for field in expected_fields:
            assert field in stats, f"Missing field: {field}"

        assert stats["cluster_id"] == "comprehensive_stats_cluster"
        assert stats["total_nodes"] == 3
        assert stats["active_nodes"] == 3


class TestDistributedCacheSuperPrecision:
    """è¶…çº§ç²¾ç¡®æµ‹è¯•DistributedCacheï¼ˆç¬¬587, 589-591è¡Œï¼‰"""

    @pytest.mark.asyncio
    async def test_set_async_with_full_validation(self):
        """ç²¾ç¡®æµ‹è¯•å¼‚æ­¥è®¾ç½®å®Œæ•´éªŒè¯ï¼ˆç¬¬587è¡Œï¼‰"""
        cache = DistributedCache("validation_cluster", "validation_node")

        # æµ‹è¯•å„ç§æ•°æ®ç±»å‹
        test_cases = [
            ("string_key", "string_value"),
            ("int_key", 123),
            ("list_key", [1, 2, 3]),
            ("dict_key", {"nested": "value"}),
        ]

        for key, value in test_cases:
            result = await cache.set_async(key, value)
            assert result is True

            retrieved_value = cache.get(key)
            assert retrieved_value == value

    @pytest.mark.asyncio
    async def test_get_batch_async_comprehensive(self):
        """ç²¾ç¡®æµ‹è¯•æ‰¹é‡å¼‚æ­¥è·å–å…¨é¢æ€§ï¼ˆç¬¬589-591è¡Œï¼‰"""
        cache = DistributedCache("comprehensive_batch_cluster", "comprehensive_batch_node")

        # é¢„è®¾å¤æ‚æ•°æ®
        test_data = {
            "batch_key_1": "string_value",
            "batch_key_2": 42,
            "batch_key_3": [1, 2, 3],
            "batch_key_4": {"nested": "dict_value"},
        }

        for key, value in test_data.items():
            cache.set(key, value)

        # æ‰¹é‡è·å–
        keys = list(test_data.keys()) + ["nonexistent_key_1", "nonexistent_key_2"]
        results = await cache.get_batch_async(keys)

        # éªŒè¯æ‰€æœ‰ç»“æœ
        assert len(results) == 6
        for key, expected_value in test_data.items():
            assert results[key] == expected_value
        assert results["nonexistent_key_1"] is None
        assert results["nonexistent_key_2"] is None


class TestCachePartitioningSuperPrecision:
    """è¶…çº§ç²¾ç¡®æµ‹è¯•CachePartitioningï¼ˆç¬¬623-624, 635, 643-646è¡Œï¼‰"""

    def test_rebalance_with_node_transitions(self):
        """ç²¾ç¡®æµ‹è¯•èŠ‚ç‚¹è½¬æ¢é‡å¹³è¡¡ï¼ˆç¬¬623-624è¡Œï¼‰"""
        partitioning = CachePartitioning(partition_count=8)

        # ä»ç©ºèŠ‚ç‚¹åˆ°æœ‰èŠ‚ç‚¹
        empty_nodes = []
        new_nodes = ["transition_node_1", "transition_node_2"]

        migration_plan = partitioning.rebalance(empty_nodes, new_nodes)

        assert isinstance(migration_plan, dict)
        # åº”è¯¥åŒ…å«è¿ç§»ä¿¡æ¯

    def test_partitioning_with_extreme_keys(self):
        """ç²¾ç¡®æµ‹è¯•æç«¯é”®åˆ†åŒºï¼ˆç¬¬635è¡Œï¼‰"""
        partitioning = CachePartitioning(partition_count=32)

        extreme_keys = [
            "",  # ç©ºé”®
            "a",  # æœ€å°é”®
            "x" * 1000,  # è¶…é•¿é”®
            "\x00\x01\x02",  # äºŒè¿›åˆ¶é”®
            "uniÃ§Ã¸dÃ©-æµ‹è¯•-ğŸš€",  # Unicodeå’Œemoji
            "key with\nnewlines\tand\ttabs",  # æ§åˆ¶å­—ç¬¦
        ]

        for key in extreme_keys:
            partition = partitioning.get_partition(key)
            assert 0 <= partition < 32

            # æµ‹è¯•ä¸€è‡´æ€§
            partition2 = partitioning.get_partition(key)
            assert partition == partition2

    def test_node_assignment_with_validation(self):
        """ç²¾ç¡®æµ‹è¯•èŠ‚ç‚¹åˆ†é…éªŒè¯ï¼ˆç¬¬643-646è¡Œï¼‰"""
        partitioning = CachePartitioning(partition_count=4)

        # åˆ†é…èŠ‚ç‚¹å¹¶éªŒè¯
        assignments = {}
        for partition_id in range(4):
            node_id = f"validated_node_{partition_id}"
            partitioning.assign_node_to_partition(node_id, partition_id)
            assignments[partition_id] = node_id

        # éªŒè¯åˆ†é…é€»è¾‘
        assert len(assignments) == 4
        for partition_id, node_id in assignments.items():
            assert partition_id in range(4)
            assert isinstance(node_id, str)
            assert node_id.startswith("validated_node_")


class TestCacheClusterFailoverSuperPrecision:
    """è¶…çº§ç²¾ç¡®æµ‹è¯•CacheClusteræ•…éšœè½¬ç§»ï¼ˆç¬¬676-677, 737-739è¡Œï¼‰"""

    def test_failover_cascade_scenario(self):
        """ç²¾ç¡®æµ‹è¯•çº§è”æ•…éšœè½¬ç§»åœºæ™¯ï¼ˆç¬¬676-677è¡Œï¼‰"""
        cluster = CacheCluster("cascade_failover_cluster")

        # æ·»åŠ å¤šä¸ªèŠ‚ç‚¹
        nodes = []
        for i in range(5):
            node = CacheNode(id=f"cascade_node_{i}")
            cluster.add_node(node)
            nodes.append(node)

        # çº§è”æ•…éšœï¼šå¤šä¸ªèŠ‚ç‚¹æ•…éšœ
        failed_nodes = ["cascade_node_1", "cascade_node_3", "cascade_node_4"]
        for node_id in failed_nodes:
            result = cluster.failover_node(node_id)
            assert result is True
            assert node_id in cluster.failed_nodes

        # éªŒè¯å‰©ä½™æ´»è·ƒèŠ‚ç‚¹
        active_nodes = [nid for nid in cluster.nodes.keys() if nid not in cluster.failed_nodes]
        assert len(active_nodes) == 2

    def test_rebalance_after_cascading_failures(self):
        """ç²¾ç¡®æµ‹è¯•çº§è”æ•…éšœåé‡å¹³è¡¡ï¼ˆç¬¬737-739è¡Œï¼‰"""
        cluster = CacheCluster("cascade_rebalance_cluster")

        # æ·»åŠ èŠ‚ç‚¹
        for i in range(4):
            node = CacheNode(id=f"rebalance_node_{i}")
            cluster.add_node(node)

        # è®¾ç½®ä¸€äº›æ•°æ®
        for i in range(4):
            cluster.nodes[f"rebalance_node_{i}"].put(f"data_key_{i}", f"data_value_{i}")

        # æ¨¡æ‹Ÿçº§è”æ•…éšœ
        cluster.failover_node("rebalance_node_1")
        cluster.failover_node("rebalance_node_2")

        # æ‰§è¡Œé‡å¹³è¡¡
        cluster.rebalance()

        # éªŒè¯é›†ç¾¤ä»ç„¶å¯ç”¨
        assert len(cluster.failed_nodes) == 2
        remaining_active = len([n for n in cluster.nodes.values() if n.status == "active"])
        assert remaining_active == 2


class TestDistributedCacheMemorySuperPrecision:
    """è¶…çº§ç²¾ç¡®æµ‹è¯•DistributedCacheå†…å­˜ç®¡ç†ï¼ˆç¬¬743-759, 768è¡Œï¼‰"""

    def test_memory_pressure_with_early_eviction(self):
        """ç²¾ç¡®æµ‹è¯•å†…å­˜å‹åŠ›æ—©æœŸæ·˜æ±°ï¼ˆç¬¬743-759è¡Œï¼‰"""
        cache = DistributedCache("early_eviction_cluster", "early_eviction_node")

        # è®¾ç½®ä¸¥æ ¼çš„å†…å­˜é™åˆ¶
        cache.node.max_memory = 200

        # å¿«é€Ÿæ·»åŠ å¤§é‡æ•°æ®è§¦å‘æ—©æœŸæ·˜æ±°
        added_count = 0
        for i in range(100):
            key = f"early_eviction_key_{i}"
            value = f"early_eviction_value_{i}" * 10  # è¾ƒå¤§çš„å€¼

            success = cache.set(key, value)
            if success:
                added_count += 1
            else:
                break

        # éªŒè¯æ—©æœŸæ·˜æ±°ç”Ÿæ•ˆ
        assert added_count < 100
        # å†…å­˜åº”è¯¥æ¥è¿‘æˆ–ä½äºé™åˆ¶
        if cache.node.current_memory > cache.node.max_memory:
            # å¦‚æœè¶…è¿‡é™åˆ¶ï¼Œåº”è¯¥æœ‰æ·˜æ±°å‘ç”Ÿ
            assert len(cache.node.storage) < added_count

    def test_cleanup_with_mixed_ttl_entries(self):
        """ç²¾ç¡®æµ‹è¯•æ··åˆTTLæ¡ç›®æ¸…ç†ï¼ˆç¬¬768è¡Œï¼‰"""
        cache = DistributedCache("mixed_ttl_cluster", "mixed_ttl_node")

        # æ·»åŠ ä¸åŒTTLçš„æ¡ç›®
        entries_to_add = [
            ("immediate_key", "immediate_value", 0),      # ç«‹å³è¿‡æœŸ
            ("second_key", "second_value", 1),            # 1ç§’è¿‡æœŸ
            ("minute_key", "minute_value", 60),           # 1åˆ†é’Ÿè¿‡æœŸ
            ("hour_key", "hour_value", 3600),             # 1å°æ—¶è¿‡æœŸ
        ]

        for key, value, ttl in entries_to_add:
            # ä½¿ç”¨ç›´æ¥å­˜å‚¨æ–¹å¼ç»•è¿‡APIé™åˆ¶
            entry = CacheEntry(key=key, value=value, ttl_seconds=ttl)
            cache.node.put(key, entry)

        # ç­‰å¾…çŸ­æœŸæ¡ç›®è¿‡æœŸ
        time.sleep(1.1)

        # æ‰§è¡Œæ¸…ç†
        cleaned_count = cache.cleanup_expired()

        # éªŒè¯æ¸…ç†ç»“æœ
        assert cleaned_count >= 2  # è‡³å°‘æ¸…ç†äº†ç«‹å³å’Œ1ç§’è¿‡æœŸçš„æ¡ç›®

        # éªŒè¯é•¿æœŸæ¡ç›®ä¿ç•™
        assert cache.get("hour_key") == "hour_value"


class TestDistributedCacheConfigurationSuperPrecision:
    """è¶…çº§ç²¾ç¡®æµ‹è¯•DistributedCacheé…ç½®ï¼ˆç¬¬796-797, 834è¡Œï¼‰"""

    def test_configuration_with_extreme_values(self):
        """ç²¾ç¡®æµ‹è¯•æå€¼é…ç½®ï¼ˆç¬¬796-797è¡Œï¼‰"""
        cache = DistributedCache("extreme_config_cluster", "extreme_config_node")

        # æµ‹è¯•å†…å­˜é…ç½®æå€¼
        original_memory = cache.node.max_memory

        # æµ‹è¯•è®¾ç½®æå°å€¼
        cache.node.max_memory = 1
        assert cache.node.max_memory == 1

        # æµ‹è¯•è®¾ç½®æå¤§å€¼
        cache.node.max_memory = 1024 * 1024 * 1024  # 1GB
        assert cache.node.max_memory == 1024 * 1024 * 1024

        # æ¢å¤åŸå§‹å€¼
        cache.node.max_memory = original_memory

    def test_consistency_configuration_with_all_levels(self):
        """ç²¾ç¡®æµ‹è¯•æ‰€æœ‰ä¸€è‡´æ€§çº§åˆ«é…ç½®ï¼ˆç¬¬834è¡Œï¼‰"""
        cluster = CacheCluster("all_consistency_cluster")

        # æµ‹è¯•æ‰€æœ‰ä¸€è‡´æ€§çº§åˆ«
        all_levels = [
            ConsistencyLevel.EVENTUAL,
            ConsistencyLevel.STRONG,
            # å¦‚æœæœ‰å…¶ä»–çº§åˆ«ä¹Ÿå¯ä»¥æµ‹è¯•
        ]

        original_level = cluster.consistency.consistency_level

        for level in all_levels:
            cluster.consistency.consistency_level = level
            assert cluster.consistency.consistency_level == level

        # æ¢å¤åŸå§‹é…ç½®
        cluster.consistency.consistency_level = original_level


class TestAdvancedReplicationSuperPrecision:
    """è¶…çº§ç²¾ç¡®æµ‹è¯•é«˜çº§å¤åˆ¶ï¼ˆç¬¬886, 912è¡Œï¼‰"""

    @pytest.mark.asyncio
    async def test_strong_consistency_with_immediate_read(self):
        """ç²¾ç¡®æµ‹è¯•å¼ºä¸€è‡´æ€§ç«‹å³è¯»å–ï¼ˆç¬¬886è¡Œï¼‰"""
        cache = DistributedCache("immediate_read_cluster", "immediate_read_node")

        # è®¾ç½®æ•°æ®
        test_key = "immediate_read_key"
        test_value = "immediate_read_value"

        await cache.set_async(test_key, test_value)

        # ç«‹å³è¯»å–åº”è¯¥è·å¾—å€¼ï¼ˆå¼ºä¸€è‡´æ€§ï¼‰
        retrieved_value = cache.get(test_key)
        assert retrieved_value == test_value

        # å¼‚æ­¥è¯»å–ä¹Ÿåº”è¯¥è·å¾—å€¼
        async_results = await cache.get_batch_async([test_key])
        assert async_results[test_key] == test_value

    def test_eventual_consistency_with_caching(self):
        """ç²¾ç¡®æµ‹è¯•æœ€ç»ˆä¸€è‡´æ€§ç¼“å­˜ï¼ˆç¬¬912è¡Œï¼‰"""
        cache = DistributedCache("caching_cluster", "caching_node")

        # è®¾ç½®æ•°æ®
        test_key = "caching_key"
        test_value = "caching_value"

        cache.set(test_key, test_value)

        # åœ¨å•èŠ‚ç‚¹ç¯å¢ƒä¸‹ï¼Œæœ€ç»ˆä¸€è‡´æ€§åº”è¯¥ç«‹å³å¯ç”¨
        retrieved_value = cache.get(test_key)
        assert retrieved_value == test_value

        # å¤šæ¬¡è¯»å–åº”è¯¥ä¸€è‡´
        for _ in range(5):
            value = cache.get(test_key)
            assert value == test_value


class TestComplexErrorScenariosSuperPrecision:
    """è¶…çº§ç²¾ç¡®æµ‹è¯•å¤æ‚é”™è¯¯åœºæ™¯ï¼ˆç¬¬920-929, 936, 943-949è¡Œï¼‰"""

    @pytest.mark.asyncio
    async def test_partial_failure_recovery(self):
        """ç²¾ç¡®æµ‹è¯•éƒ¨åˆ†æ•…éšœæ¢å¤ï¼ˆç¬¬920-929è¡Œï¼‰"""
        cache = DistributedCache("partial_failure_cluster", "partial_failure_node")

        # è®¾ç½®å¤§é‡æ•°æ®
        large_dataset = {f"partial_key_{i}": f"partial_value_{i}" for i in range(50)}

        for key, value in large_dataset.items():
            await cache.set_async(key, value)

        # æ¨¡æ‹Ÿéƒ¨åˆ†æ•°æ®ä¸¢å¤±çš„æ¢å¤åœºæ™¯
        recovery_data = {
            "recovery_key_1": "recovery_value_1",
            "recovery_key_2": "recovery_value_2"
        }

        for key, value in recovery_data.items():
            cache.set(key, value)

        # éªŒè¯æ‰€æœ‰æ•°æ®å¯ç”¨
        for key, expected_value in large_dataset.items():
            actual_value = cache.get(key)
            assert actual_value == expected_value

        for key, expected_value in recovery_data.items():
            actual_value = cache.get(key)
            assert actual_value == expected_value

    @pytest.mark.asyncio
    async def test_failover_during_operations(self):
        """ç²¾ç¡®æµ‹è¯•æ“ä½œæœŸé—´æ•…éšœè½¬ç§»ï¼ˆç¬¬936, 943-949è¡Œï¼‰"""
        cache = DistributedCache("operation_failover_cluster", "operation_failover_node")

        # å¼€å§‹è®¾ç½®æ“ä½œ
        ongoing_operations = []
        for i in range(10):
            op = cache.set_async(f"failover_during_key_{i}", f"failover_during_value_{i}")
            ongoing_operations.append(op)

        # ç­‰å¾…ä¸€äº›æ“ä½œå®Œæˆ
        await asyncio.sleep(0.01)

        # æ¨¡æ‹Ÿæ•…éšœè½¬ç§»æœŸé—´çš„æ“ä½œ
        emergency_data = {
            "emergency_key_1": "emergency_value_1",
            "emergency_key_2": "emergency_value_2"
        }

        for key, value in emergency_data.items():
            cache.set(key, value)

        # ç­‰å¾…æ‰€æœ‰æ“ä½œå®Œæˆ
        await asyncio.gather(*ongoing_operations, return_exceptions=True)

        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        for key, value in emergency_data.items():
            assert cache.get(key) == value


class TestEdgeCasesAndBoundaryConditions:
    """æµ‹è¯•è¾¹ç¼˜æƒ…å†µå’Œè¾¹ç•Œæ¡ä»¶"""

    def test_maximum_key_length_handling(self):
        """æµ‹è¯•æœ€å¤§é”®é•¿åº¦å¤„ç†"""
        cache = DistributedCache("max_key_cluster", "max_key_node")

        # åˆ›å»ºæé•¿çš„é”®
        max_key = "x" * 10000

        # æµ‹è¯•è®¾ç½®é•¿é”®
        result = cache.set(max_key, "max_key_value")
        assert result is True

        # æµ‹è¯•è·å–é•¿é”®
        value = cache.get(max_key)
        assert value == "max_key_value"

    def test_unicode_and_special_characters(self):
        """æµ‹è¯•Unicodeå’Œç‰¹æ®Šå­—ç¬¦"""
        cache = DistributedCache("unicode_cluster", "unicode_node")

        unicode_test_cases = [
            ("ä¸­æ–‡é”®", "ä¸­æ–‡å€¼"),
            ("ĞºĞ»ÑÑ‡_Ñ€ÑƒÑÑĞºĞ¸Ğ¹", "Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ_Ñ€ÑƒÑÑĞºĞ¾Ğµ"),
            ("clÃ©_franÃ§aise", "valeur_franÃ§aise"),
            ("ğŸš€_emoji_key", "ğŸŒŸ_emoji_value"),
            ("mixed_é”®_ğŸš€_key", "mixed_å€¼_ğŸŒŸ_value"),
        ]

        for key, value in unicode_test_cases:
            cache.set(key, value)
            retrieved_value = cache.get(key)
            assert retrieved_value == value

    @pytest.mark.asyncio
    async def test_concurrent_stress_test(self):
        """å¹¶å‘å‹åŠ›æµ‹è¯•"""
        cache = DistributedCache("stress_cluster", "stress_node")

        # åˆ›å»ºå¤§é‡å¹¶å‘æ“ä½œ
        async def concurrent_set(start_index, count):
            results = []
            for i in range(count):
                key = f"stress_key_{start_index}_{i}"
                value = f"stress_value_{start_index}_{i}"
                result = await cache.set_async(key, value)
                results.append(result)
            return results

        # å¯åŠ¨å¤šä¸ªå¹¶å‘ä»»åŠ¡
        tasks = []
        for i in range(10):
            task = concurrent_set(i, 20)
            tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        all_results = await asyncio.gather(*tasks, return_exceptions=True)

        # éªŒè¯ç»“æœ
        successful_operations = sum(len(results) for results in all_results if isinstance(results, list))
        assert successful_operations == 200  # 10 tasks Ã— 20 operations each

        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        for task_idx in range(10):
            for op_idx in range(20):
                key = f"stress_key_{task_idx}_{op_idx}"
                value = cache.get(key)
                assert value == f"stress_value_{task_idx}_{op_idx}"


if __name__ == "__main__":
    pytest.main([__file__, "-v"])